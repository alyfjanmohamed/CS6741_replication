{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "faea33c75fd249c2a08c728c95f2fd66": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b9b5361cbe984caa9c972edccbba3c2c",
              "IPY_MODEL_c6ffc89b9f944d5face06b010d03c342",
              "IPY_MODEL_4ef0fbe5baa849b59d81f1ce3d2d6460"
            ],
            "layout": "IPY_MODEL_6fbfd58481f941ef9e2cc6431fd481a5"
          }
        },
        "b9b5361cbe984caa9c972edccbba3c2c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7d3d515d41b740a78e0567a98e816d8d",
            "placeholder": "​",
            "style": "IPY_MODEL_361e127d50e44aa38c0527481e94fe20",
            "value": "pytorch_model.bin: 100%"
          }
        },
        "c6ffc89b9f944d5face06b010d03c342": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8b26d3d4762348b3a0203085fa34b894",
            "max": 62747391,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d88503c7ac6e4fd1b0f3f1eceb69c284",
            "value": 62747391
          }
        },
        "4ef0fbe5baa849b59d81f1ce3d2d6460": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f4e8a4c81b7a4544a6ee856909133b79",
            "placeholder": "​",
            "style": "IPY_MODEL_7f6f95ad4a0b42c081e88df65f520c8a",
            "value": " 62.7M/62.7M [00:00&lt;00:00, 152MB/s]"
          }
        },
        "6fbfd58481f941ef9e2cc6431fd481a5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7d3d515d41b740a78e0567a98e816d8d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "361e127d50e44aa38c0527481e94fe20": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8b26d3d4762348b3a0203085fa34b894": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d88503c7ac6e4fd1b0f3f1eceb69c284": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f4e8a4c81b7a4544a6ee856909133b79": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7f6f95ad4a0b42c081e88df65f520c8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "afb7c2eb611e469a9f2e1e6ce687be1b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c7a8973eaf1a4aaeab0d5e80617ec485",
              "IPY_MODEL_e5f5487c8a0e4b92b59ac25a5779c2ca",
              "IPY_MODEL_52b288a35a554e74932e3fa3b782b0d2"
            ],
            "layout": "IPY_MODEL_42d03ebe82ef46b6ba7848b0c204c36a"
          }
        },
        "c7a8973eaf1a4aaeab0d5e80617ec485": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_764656b7bb3a4057944e92258570833d",
            "placeholder": "​",
            "style": "IPY_MODEL_529ebdeb67a049eabf306f41a94fcb87",
            "value": "pytorch_model.bin: 100%"
          }
        },
        "e5f5487c8a0e4b92b59ac25a5779c2ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_954ee0c9d6bc448787837283d8e178c5",
            "max": 62747391,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b558357ddc2b4648abf5324888dd739f",
            "value": 62747391
          }
        },
        "52b288a35a554e74932e3fa3b782b0d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_468b2c6ae8de4022a700f5050e184b52",
            "placeholder": "​",
            "style": "IPY_MODEL_fdfef234ee474f009764e75376624947",
            "value": " 62.7M/62.7M [00:00&lt;00:00, 157MB/s]"
          }
        },
        "42d03ebe82ef46b6ba7848b0c204c36a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "764656b7bb3a4057944e92258570833d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "529ebdeb67a049eabf306f41a94fcb87": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "954ee0c9d6bc448787837283d8e178c5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b558357ddc2b4648abf5324888dd739f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "468b2c6ae8de4022a700f5050e184b52": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fdfef234ee474f009764e75376624947": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9d2779b010d24a9ea6353d0ca9707102": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7480d2b4ccf641c2bcac8d57f2c8e578",
              "IPY_MODEL_78f7eabe96794c6e9763662bbf1773c5",
              "IPY_MODEL_eb76ac75df6245098d4cc9456f65e57a"
            ],
            "layout": "IPY_MODEL_d16c9aaefc134859928298197da5b461"
          }
        },
        "7480d2b4ccf641c2bcac8d57f2c8e578": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3231b9f700004de681daf5cebc942481",
            "placeholder": "​",
            "style": "IPY_MODEL_b55e6dd67c2d44298a3c38dba1c6f9c3",
            "value": "config.json: 100%"
          }
        },
        "78f7eabe96794c6e9763662bbf1773c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f64e4d63bd1d4bd59af549d1c60ed533",
            "max": 409,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f394b1627d9049eeb59ebeeb5bb3cc4f",
            "value": 409
          }
        },
        "eb76ac75df6245098d4cc9456f65e57a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8609d8a6e86a461d93de072fd158ed5f",
            "placeholder": "​",
            "style": "IPY_MODEL_b0f782ba6bf04afa93113ce31e35f9c5",
            "value": " 409/409 [00:00&lt;00:00, 14.2kB/s]"
          }
        },
        "d16c9aaefc134859928298197da5b461": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3231b9f700004de681daf5cebc942481": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b55e6dd67c2d44298a3c38dba1c6f9c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f64e4d63bd1d4bd59af549d1c60ed533": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f394b1627d9049eeb59ebeeb5bb3cc4f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8609d8a6e86a461d93de072fd158ed5f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b0f782ba6bf04afa93113ce31e35f9c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c6b8a993658044e7be690e9aacbc6fe2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e053175f18c94a01b3e4bda59b14e0fa",
              "IPY_MODEL_de14010e542b49da8d113ea30773559b",
              "IPY_MODEL_c19c3d335a33498dad62d8a65f257d62"
            ],
            "layout": "IPY_MODEL_b0881173f1cd4eb9bc27a0a68de1a52f"
          }
        },
        "e053175f18c94a01b3e4bda59b14e0fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c17923d465dc4d3fa6abde9ffb3ca3ab",
            "placeholder": "​",
            "style": "IPY_MODEL_dea47177a0fd4868bb5ea82b8e25c045",
            "value": "vocab.txt: 100%"
          }
        },
        "de14010e542b49da8d113ea30773559b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_077a079546c5445181f612547f3ce593",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_dd1e82f4c50e4ba3b8ec53500aba56dc",
            "value": 231508
          }
        },
        "c19c3d335a33498dad62d8a65f257d62": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6876ace9912243309871e676cee18f01",
            "placeholder": "​",
            "style": "IPY_MODEL_dd4cbf43dd204b4fa9b5c4d345b64ddf",
            "value": " 232k/232k [00:00&lt;00:00, 6.20MB/s]"
          }
        },
        "b0881173f1cd4eb9bc27a0a68de1a52f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c17923d465dc4d3fa6abde9ffb3ca3ab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dea47177a0fd4868bb5ea82b8e25c045": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "077a079546c5445181f612547f3ce593": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dd1e82f4c50e4ba3b8ec53500aba56dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6876ace9912243309871e676cee18f01": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dd4cbf43dd204b4fa9b5c4d345b64ddf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VDMb3AvZF2FH",
        "outputId": "3af00781-1631-4fe3-a57d-0bfd45a2f407"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting accelerate\n",
            "  Downloading accelerate-0.29.2-py3-none-any.whl (297 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/297.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━\u001b[0m \u001b[32m204.8/297.4 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m297.4/297.4 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (24.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.2.1+cu121)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.20.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.13.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, accelerate\n",
            "Successfully installed accelerate-0.29.2 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.18.0-py3-none-any.whl (510 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.5/510.5 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.2)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.2.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.20.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.4->datasets) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: xxhash, dill, multiprocess, datasets\n",
            "Successfully installed datasets-2.18.0 dill-0.3.8 multiprocess-0.70.16 xxhash-3.4.1\n",
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.1-py3-none-any.whl (84 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from evaluate) (1.25.2)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from evaluate) (4.66.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.16)\n",
            "Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2023.6.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.20.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from evaluate) (24.0)\n",
            "Collecting responses<0.19 (from evaluate)\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.13.4)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (0.6)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (3.9.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.0.0->evaluate) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->evaluate) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n",
            "Installing collected packages: responses, evaluate\n",
            "Successfully installed evaluate-0.4.1 responses-0.18.0\n",
            "Collecting git+https://github.com/huggingface/transformers\n",
            "  Cloning https://github.com/huggingface/transformers to /tmp/pip-req-build-3g12470p\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers /tmp/pip-req-build-3g12470p\n",
            "  Resolved https://github.com/huggingface/transformers to commit 0bd58f1ce0573c0e3269de4215a17d318add49b9\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.40.0.dev0) (3.13.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers==4.40.0.dev0) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.40.0.dev0) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.40.0.dev0) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.40.0.dev0) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.40.0.dev0) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.40.0.dev0) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers==4.40.0.dev0) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.40.0.dev0) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.40.0.dev0) (4.66.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.40.0.dev0) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.40.0.dev0) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.40.0.dev0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.40.0.dev0) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.40.0.dev0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.40.0.dev0) (2024.2.2)\n",
            "Building wheels for collected packages: transformers\n",
            "  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-4.40.0.dev0-py3-none-any.whl size=8888190 sha256=394fd0af46197ae9975b9ee308a3ea99094519bc824b91561b4eeda07e752001\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-5h4od5xq/wheels/c0/14/d6/6c9a5582d2ac191ec0a483be151a4495fe1eb2a6706ca49f1b\n",
            "Successfully built transformers\n",
            "Installing collected packages: transformers\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.38.2\n",
            "    Uninstalling transformers-4.38.2:\n",
            "      Successfully uninstalled transformers-4.38.2\n",
            "Successfully installed transformers-4.40.0.dev0\n"
          ]
        }
      ],
      "source": [
        "!pip install accelerate\n",
        "!pip install datasets\n",
        "!pip install evaluate\n",
        "!pip install git+https://github.com/huggingface/transformers\n",
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import functools"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorrt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qZ_7DnK3OKeq",
        "outputId": "fda9b0fc-1d73-431f-c8d6-3c89571e884c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorrt\n",
            "  Downloading tensorrt-8.6.1.post1.tar.gz (18 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: tensorrt\n",
            "  Building wheel for tensorrt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tensorrt: filename=tensorrt-8.6.1.post1-py2.py3-none-any.whl size=17281 sha256=f90aa683caec3b87f9e54956d7d02757b42304470f82999a3bb332d3dcacc04f\n",
            "  Stored in directory: /root/.cache/pip/wheels/f4/c8/0e/b79b08e45752491b9acfdbd69e8a609e8b2ed7640dda5a3e59\n",
            "Successfully built tensorrt\n",
            "Installing collected packages: tensorrt\n",
            "Successfully installed tensorrt-8.6.1.post1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = torch.load('models/base.mod')\n",
        "model.save_pretrained(\"models/base_v3\")"
      ],
      "metadata": {
        "id": "eHFcBtZhShR7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "from transformers import BertModel, BertConfig, BertTokenizer\n",
        "\n",
        "model_name = \"huawei-noah/TinyBERT_General_4L_312D\"\n",
        "config = BertConfig.from_pretrained(model_name)#, output_hidden_states=True, output_attentions=True)\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "model = BertModel.from_pretrained(model_name, config=config)\n",
        "tokenizer.save_pretrained(\"models/basev2\")\n",
        "config.save_pretrained(\"models/basev2\")\n",
        "model.save_pretrained(\"models/basev2\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "afb7c2eb611e469a9f2e1e6ce687be1b",
            "c7a8973eaf1a4aaeab0d5e80617ec485",
            "e5f5487c8a0e4b92b59ac25a5779c2ca",
            "52b288a35a554e74932e3fa3b782b0d2",
            "42d03ebe82ef46b6ba7848b0c204c36a",
            "764656b7bb3a4057944e92258570833d",
            "529ebdeb67a049eabf306f41a94fcb87",
            "954ee0c9d6bc448787837283d8e178c5",
            "b558357ddc2b4648abf5324888dd739f",
            "468b2c6ae8de4022a700f5050e184b52",
            "fdfef234ee474f009764e75376624947"
          ]
        },
        "id": "Jz9VSOzdUy6f",
        "outputId": "2dec0731-3618-40e1-bddb-24d7a38379f0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/62.7M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "afb7c2eb611e469a9f2e1e6ce687be1b"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**COLA BENCHMARK**"
      ],
      "metadata": {
        "id": "RBNZlxvtP88Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code for running benchmark\n",
        "# !mkdir test_results_tinyBERT\n",
        "!python run_glue.py --model_name_or_path models/basev2 --task_name cola --do_train --do_eval --max_seq_length 128 --per_device_train_batch_size 32 --learning_rate 2e-5 --num_train_epochs 3 --output_dir test_results_tinyBERT"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UYJsMBD-F5FH",
        "outputId": "9eb641c4-d4c4-4870-9102-770caf81539c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-04-11 04:48:10.853339: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-04-11 04:48:10.853471: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-04-11 04:48:10.856884: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-04-11 04:48:13.374364: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "04/11/2024 04:48:20 - WARNING - __main__ - Process rank: 0, device: cpu, n_gpu: 0, distributed training: False, 16-bits training: False\n",
            "04/11/2024 04:48:20 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=0,\n",
            "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'gradient_accumulation_kwargs': None},\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_persistent_workers=False,\n",
            "dataloader_pin_memory=True,\n",
            "dataloader_prefetch_factor=None,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_num_input_tokens_seen=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=2e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=test_results_tinyBERT/runs/Apr11_04-48-20_3ec82996b54c,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "optim_target_modules=None,\n",
            "output_dir=test_results_tinyBERT,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=32,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=test_results_tinyBERT,\n",
            "save_on_each_node=False,\n",
            "save_only_model=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "split_batches=None,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "Overwrite dataset info from restored data version if exists.\n",
            "04/11/2024 04:48:23 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/nyu-mll___glue/cola/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c\n",
            "04/11/2024 04:48:23 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/nyu-mll___glue/cola/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c\n",
            "Found cached dataset glue (/root/.cache/huggingface/datasets/nyu-mll___glue/cola/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)\n",
            "04/11/2024 04:48:23 - INFO - datasets.builder - Found cached dataset glue (/root/.cache/huggingface/datasets/nyu-mll___glue/cola/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/nyu-mll___glue/cola/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c\n",
            "04/11/2024 04:48:23 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/nyu-mll___glue/cola/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c\n",
            "[INFO|configuration_utils.py:724] 2024-04-11 04:48:23,153 >> loading configuration file models/basev2/config.json\n",
            "[INFO|configuration_utils.py:789] 2024-04-11 04:48:23,158 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"models/basev2\",\n",
            "  \"architectures\": [\n",
            "    \"BertModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"cell\": {},\n",
            "  \"classifier_dropout\": null,\n",
            "  \"emb_size\": 312,\n",
            "  \"finetuning_task\": \"cola\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 312,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 1200,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 4,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"pre_trained\": \"\",\n",
            "  \"structure\": [],\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.40.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2085] 2024-04-11 04:48:23,159 >> loading file vocab.txt\n",
            "[INFO|tokenization_utils_base.py:2085] 2024-04-11 04:48:23,159 >> loading file tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2085] 2024-04-11 04:48:23,159 >> loading file added_tokens.json\n",
            "[INFO|tokenization_utils_base.py:2085] 2024-04-11 04:48:23,159 >> loading file special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2085] 2024-04-11 04:48:23,159 >> loading file tokenizer_config.json\n",
            "[INFO|modeling_utils.py:3426] 2024-04-11 04:48:23,282 >> loading weights file models/basev2/model.safetensors\n",
            "[INFO|modeling_utils.py:4170] 2024-04-11 04:48:23,406 >> All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
            "\n",
            "[WARNING|modeling_utils.py:4172] 2024-04-11 04:48:23,406 >> Some weights of BertForSequenceClassification were not initialized from the model checkpoint at models/basev2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Running tokenizer on dataset:   0% 0/8551 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/nyu-mll___glue/cola/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-892a2c2f737aa20b.arrow\n",
            "04/11/2024 04:48:23 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/nyu-mll___glue/cola/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-892a2c2f737aa20b.arrow\n",
            "Running tokenizer on dataset: 100% 8551/8551 [00:01<00:00, 5017.61 examples/s]\n",
            "Running tokenizer on dataset:   0% 0/1043 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/nyu-mll___glue/cola/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-3cc1e8bacab778fc.arrow\n",
            "04/11/2024 04:48:25 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/nyu-mll___glue/cola/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-3cc1e8bacab778fc.arrow\n",
            "Running tokenizer on dataset: 100% 1043/1043 [00:00<00:00, 6673.66 examples/s]\n",
            "Running tokenizer on dataset:   0% 0/1063 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/nyu-mll___glue/cola/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-669ca749aedaef46.arrow\n",
            "04/11/2024 04:48:25 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/nyu-mll___glue/cola/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-669ca749aedaef46.arrow\n",
            "Running tokenizer on dataset: 100% 1063/1063 [00:00<00:00, 6891.39 examples/s]\n",
            "04/11/2024 04:48:25 - INFO - __main__ - Sample 1824 of the training set: {'sentence': 'I acknowledged that my father, he was tight as an owl.', 'label': 0, 'idx': 1824, 'input_ids': [101, 1045, 8969, 2008, 2026, 2269, 1010, 2002, 2001, 4389, 2004, 2019, 13547, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "04/11/2024 04:48:25 - INFO - __main__ - Sample 409 of the training set: {'sentence': 'For him to do that would be a mistake.', 'label': 1, 'idx': 409, 'input_ids': [101, 2005, 2032, 2000, 2079, 2008, 2052, 2022, 1037, 6707, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "04/11/2024 04:48:25 - INFO - __main__ - Sample 4506 of the training set: {'sentence': 'Mary sang a song, but Lee never did.', 'label': 1, 'idx': 4506, 'input_ids': [101, 2984, 6369, 1037, 2299, 1010, 2021, 3389, 2196, 2106, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "Downloading builder script: 100% 5.75k/5.75k [00:00<00:00, 11.4MB/s]\n",
            "[INFO|trainer.py:776] 2024-04-11 04:48:26,310 >> The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence, idx. If sentence, idx are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2038] 2024-04-11 04:48:26,317 >> ***** Running training *****\n",
            "[INFO|trainer.py:2039] 2024-04-11 04:48:26,317 >>   Num examples = 8,551\n",
            "[INFO|trainer.py:2040] 2024-04-11 04:48:26,317 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:2041] 2024-04-11 04:48:26,317 >>   Instantaneous batch size per device = 32\n",
            "[INFO|trainer.py:2044] 2024-04-11 04:48:26,317 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "[INFO|trainer.py:2045] 2024-04-11 04:48:26,318 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:2046] 2024-04-11 04:48:26,318 >>   Total optimization steps = 804\n",
            "[INFO|trainer.py:2047] 2024-04-11 04:48:26,318 >>   Number of trainable parameters = 14,350,874\n",
            "{'loss': 0.6091, 'grad_norm': 0.6430385112762451, 'learning_rate': 7.5621890547263685e-06, 'epoch': 1.87}\n",
            " 62% 500/804 [24:37<15:38,  3.09s/it][INFO|trainer.py:3295] 2024-04-11 05:13:03,500 >> Saving model checkpoint to test_results_tinyBERT/checkpoint-500\n",
            "[INFO|configuration_utils.py:471] 2024-04-11 05:13:03,501 >> Configuration saved in test_results_tinyBERT/checkpoint-500/config.json\n",
            "[INFO|modeling_utils.py:2590] 2024-04-11 05:13:03,579 >> Model weights saved in test_results_tinyBERT/checkpoint-500/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2488] 2024-04-11 05:13:03,580 >> tokenizer config file saved in test_results_tinyBERT/checkpoint-500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2497] 2024-04-11 05:13:03,581 >> Special tokens file saved in test_results_tinyBERT/checkpoint-500/special_tokens_map.json\n",
            "100% 804/804 [39:18<00:00,  2.42s/it][INFO|trainer.py:2306] 2024-04-11 05:27:44,675 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 2358.3567, 'train_samples_per_second': 10.877, 'train_steps_per_second': 0.341, 'train_loss': 0.6022488418503187, 'epoch': 3.0}\n",
            "100% 804/804 [39:18<00:00,  2.93s/it]\n",
            "[INFO|trainer.py:3295] 2024-04-11 05:27:44,677 >> Saving model checkpoint to test_results_tinyBERT\n",
            "[INFO|configuration_utils.py:471] 2024-04-11 05:27:44,679 >> Configuration saved in test_results_tinyBERT/config.json\n",
            "[INFO|modeling_utils.py:2590] 2024-04-11 05:27:44,747 >> Model weights saved in test_results_tinyBERT/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2488] 2024-04-11 05:27:44,748 >> tokenizer config file saved in test_results_tinyBERT/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2497] 2024-04-11 05:27:44,749 >> Special tokens file saved in test_results_tinyBERT/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        3.0\n",
            "  train_loss               =     0.6022\n",
            "  train_runtime            = 0:39:18.35\n",
            "  train_samples            =       8551\n",
            "  train_samples_per_second =     10.877\n",
            "  train_steps_per_second   =      0.341\n",
            "04/11/2024 05:27:44 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:776] 2024-04-11 05:27:44,770 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence, idx. If sentence, idx are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3604] 2024-04-11 05:27:44,773 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3606] 2024-04-11 05:27:44,774 >>   Num examples = 1043\n",
            "[INFO|trainer.py:3609] 2024-04-11 05:27:44,774 >>   Batch size = 8\n",
            "100% 131/131 [00:29<00:00,  4.49it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                     =        3.0\n",
            "  eval_loss                 =     0.6029\n",
            "  eval_matthews_correlation =        0.0\n",
            "  eval_runtime              = 0:00:29.39\n",
            "  eval_samples              =       1043\n",
            "  eval_samples_per_second   =     35.485\n",
            "  eval_steps_per_second     =      4.457\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Code for running benchmark\n",
        "# !mkdir test_results_small\n",
        "!python run_glue.py --model_name_or_path models/small --task_name cola --do_train --do_eval --max_seq_length 128 --per_device_train_batch_size 32 --learning_rate 2e-5 --num_train_epochs 3 --output_dir test_results_small"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wB7mPqJmGBm_",
        "outputId": "64209c6a-dc41-46a9-c2a2-2abbf858ef52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-04-11 05:30:58.870410: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-04-11 05:30:58.870477: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-04-11 05:30:58.872035: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-04-11 05:31:00.431629: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "04/11/2024 05:31:03 - WARNING - __main__ - Process rank: 0, device: cpu, n_gpu: 0, distributed training: False, 16-bits training: False\n",
            "04/11/2024 05:31:03 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=0,\n",
            "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'gradient_accumulation_kwargs': None},\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_persistent_workers=False,\n",
            "dataloader_pin_memory=True,\n",
            "dataloader_prefetch_factor=None,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_num_input_tokens_seen=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=2e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=test_results_small/runs/Apr11_05-31-03_3ec82996b54c,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "optim_target_modules=None,\n",
            "output_dir=test_results_small,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=32,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=test_results_small,\n",
            "save_on_each_node=False,\n",
            "save_only_model=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "split_batches=None,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "Overwrite dataset info from restored data version if exists.\n",
            "04/11/2024 05:31:04 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/nyu-mll___glue/cola/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c\n",
            "04/11/2024 05:31:04 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/nyu-mll___glue/cola/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c\n",
            "Found cached dataset glue (/root/.cache/huggingface/datasets/nyu-mll___glue/cola/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)\n",
            "04/11/2024 05:31:04 - INFO - datasets.builder - Found cached dataset glue (/root/.cache/huggingface/datasets/nyu-mll___glue/cola/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/nyu-mll___glue/cola/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c\n",
            "04/11/2024 05:31:04 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/nyu-mll___glue/cola/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c\n",
            "[INFO|configuration_utils.py:724] 2024-04-11 05:31:04,447 >> loading configuration file models/small/config.json\n",
            "[INFO|configuration_utils.py:789] 2024-04-11 05:31:04,451 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"models/small\",\n",
            "  \"architectures\": [\n",
            "    \"BertModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"cell\": {},\n",
            "  \"classifier_dropout\": null,\n",
            "  \"emb_size\": 312,\n",
            "  \"finetuning_task\": \"cola\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 312,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 1200,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 4,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"pre_trained\": \"\",\n",
            "  \"structure\": [],\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.40.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2085] 2024-04-11 05:31:04,452 >> loading file vocab.txt\n",
            "[INFO|tokenization_utils_base.py:2085] 2024-04-11 05:31:04,452 >> loading file tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2085] 2024-04-11 05:31:04,452 >> loading file added_tokens.json\n",
            "[INFO|tokenization_utils_base.py:2085] 2024-04-11 05:31:04,452 >> loading file special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2085] 2024-04-11 05:31:04,452 >> loading file tokenizer_config.json\n",
            "[INFO|modeling_utils.py:3426] 2024-04-11 05:31:04,558 >> loading weights file models/small/model.safetensors\n",
            "[INFO|modeling_utils.py:4170] 2024-04-11 05:31:04,674 >> All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
            "\n",
            "[WARNING|modeling_utils.py:4172] 2024-04-11 05:31:04,674 >> Some weights of BertForSequenceClassification were not initialized from the model checkpoint at models/small and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Running tokenizer on dataset:   0% 0/8551 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/nyu-mll___glue/cola/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-850871cc0c304437.arrow\n",
            "04/11/2024 05:31:04 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/nyu-mll___glue/cola/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-850871cc0c304437.arrow\n",
            "Running tokenizer on dataset: 100% 8551/8551 [00:01<00:00, 5959.79 examples/s]\n",
            "Running tokenizer on dataset:   0% 0/1043 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/nyu-mll___glue/cola/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-16dfee71d38c9ee3.arrow\n",
            "04/11/2024 05:31:06 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/nyu-mll___glue/cola/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-16dfee71d38c9ee3.arrow\n",
            "Running tokenizer on dataset: 100% 1043/1043 [00:00<00:00, 6883.29 examples/s]\n",
            "Running tokenizer on dataset:   0% 0/1063 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/nyu-mll___glue/cola/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-0be5e665febee2ce.arrow\n",
            "04/11/2024 05:31:06 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/nyu-mll___glue/cola/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-0be5e665febee2ce.arrow\n",
            "Running tokenizer on dataset: 100% 1063/1063 [00:00<00:00, 6246.96 examples/s]\n",
            "04/11/2024 05:31:06 - INFO - __main__ - Sample 1824 of the training set: {'sentence': 'I acknowledged that my father, he was tight as an owl.', 'label': 0, 'idx': 1824, 'input_ids': [101, 1045, 8969, 2008, 2026, 2269, 1010, 2002, 2001, 4389, 2004, 2019, 13547, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "04/11/2024 05:31:06 - INFO - __main__ - Sample 409 of the training set: {'sentence': 'For him to do that would be a mistake.', 'label': 1, 'idx': 409, 'input_ids': [101, 2005, 2032, 2000, 2079, 2008, 2052, 2022, 1037, 6707, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "04/11/2024 05:31:06 - INFO - __main__ - Sample 4506 of the training set: {'sentence': 'Mary sang a song, but Lee never did.', 'label': 1, 'idx': 4506, 'input_ids': [101, 2984, 6369, 1037, 2299, 1010, 2021, 3389, 2196, 2106, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "[INFO|trainer.py:776] 2024-04-11 05:31:07,219 >> The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2038] 2024-04-11 05:31:07,227 >> ***** Running training *****\n",
            "[INFO|trainer.py:2039] 2024-04-11 05:31:07,228 >>   Num examples = 8,551\n",
            "[INFO|trainer.py:2040] 2024-04-11 05:31:07,228 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:2041] 2024-04-11 05:31:07,228 >>   Instantaneous batch size per device = 32\n",
            "[INFO|trainer.py:2044] 2024-04-11 05:31:07,228 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "[INFO|trainer.py:2045] 2024-04-11 05:31:07,228 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:2046] 2024-04-11 05:31:07,228 >>   Total optimization steps = 804\n",
            "[INFO|trainer.py:2047] 2024-04-11 05:31:07,228 >>   Number of trainable parameters = 14,350,874\n",
            "{'loss': 0.6097, 'grad_norm': 0.30329155921936035, 'learning_rate': 7.5621890547263685e-06, 'epoch': 1.87}\n",
            " 62% 500/804 [25:03<14:57,  2.95s/it][INFO|trainer.py:3295] 2024-04-11 05:56:10,894 >> Saving model checkpoint to test_results_small/checkpoint-500\n",
            "[INFO|configuration_utils.py:471] 2024-04-11 05:56:10,895 >> Configuration saved in test_results_small/checkpoint-500/config.json\n",
            "[INFO|modeling_utils.py:2590] 2024-04-11 05:56:11,012 >> Model weights saved in test_results_small/checkpoint-500/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2488] 2024-04-11 05:56:11,013 >> tokenizer config file saved in test_results_small/checkpoint-500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2497] 2024-04-11 05:56:11,014 >> Special tokens file saved in test_results_small/checkpoint-500/special_tokens_map.json\n",
            "100% 804/804 [40:04<00:00,  2.23s/it][INFO|trainer.py:2306] 2024-04-11 06:11:12,025 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 2404.7968, 'train_samples_per_second': 10.667, 'train_steps_per_second': 0.334, 'train_loss': 0.6071161108823558, 'epoch': 3.0}\n",
            "100% 804/804 [40:04<00:00,  2.99s/it]\n",
            "[INFO|trainer.py:3295] 2024-04-11 06:11:12,029 >> Saving model checkpoint to test_results_small\n",
            "[INFO|configuration_utils.py:471] 2024-04-11 06:11:12,031 >> Configuration saved in test_results_small/config.json\n",
            "[INFO|modeling_utils.py:2590] 2024-04-11 06:11:12,103 >> Model weights saved in test_results_small/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2488] 2024-04-11 06:11:12,105 >> tokenizer config file saved in test_results_small/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2497] 2024-04-11 06:11:12,105 >> Special tokens file saved in test_results_small/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        3.0\n",
            "  train_loss               =     0.6071\n",
            "  train_runtime            = 0:40:04.79\n",
            "  train_samples            =       8551\n",
            "  train_samples_per_second =     10.667\n",
            "  train_steps_per_second   =      0.334\n",
            "04/11/2024 06:11:12 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:776] 2024-04-11 06:11:12,126 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: idx, sentence. If idx, sentence are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3604] 2024-04-11 06:11:12,129 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3606] 2024-04-11 06:11:12,130 >>   Num examples = 1043\n",
            "[INFO|trainer.py:3609] 2024-04-11 06:11:12,130 >>   Batch size = 8\n",
            "100% 131/131 [00:30<00:00,  4.35it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                     =        3.0\n",
            "  eval_loss                 =     0.6155\n",
            "  eval_matthews_correlation =        0.0\n",
            "  eval_runtime              = 0:00:30.35\n",
            "  eval_samples              =       1043\n",
            "  eval_samples_per_second   =     34.363\n",
            "  eval_steps_per_second     =      4.316\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "from transformers import BertModel, BertConfig, BertTokenizer\n",
        "\n",
        "model = torch.load('models/medium.mod', map_location=device)\n",
        "model.save_pretrained(\"models/medium\")\n",
        "# model_name = \"huawei-noah/TinyBERT_General_4L_312D\"\n",
        "config = BertConfig.from_pretrained(model_name)#, output_hidden_states=True, output_attentions=True)\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "model = BertModel.from_pretrained('models/medium', config=config)\n",
        "tokenizer.save_pretrained(\"models/medium_v2\")\n",
        "config.save_pretrained(\"models/medium_v2\")\n",
        "model.save_pretrained(\"models/medium_v2\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "cgkJSNe3zOfJ",
        "outputId": "fa9e399d-d732-4df8-8544-96e866dfa886"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "PytorchStreamReader failed reading zip archive: failed finding central directory",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-4a836dda3448>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBertModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBertConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'models/int_expanded.mod'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"models/int_expanded_v2\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"huawei-noah/TinyBERT_General_4L_312D\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1003\u001b[0m             \u001b[0morig_position\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1004\u001b[0m             \u001b[0moverall_storage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1005\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_reader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1006\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0m_is_torchscript_zip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_zipfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1007\u001b[0m                     warnings.warn(\"'torch.load' received a zip file that looks like a TorchScript archive\"\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name_or_buffer)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_zipfile_reader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPyTorchFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: PytorchStreamReader failed reading zip archive: failed finding central directory"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Code for running benchmark\n",
        "!mkdir test_results_medium\n",
        "!python run_glue.py --model_name_or_path models/medium_v2 --task_name cola --do_train --do_eval --max_seq_length 128 --per_device_train_batch_size 32 --learning_rate 2e-5 --num_train_epochs 3 --output_dir test_results_medium"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wL-u_Nfurh_8",
        "outputId": "2cee0f8b-1893-41e0-bc64-f82e21443643"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘test_results_medium’: File exists\n",
            "2024-04-11 06:20:56.746690: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-04-11 06:20:56.746824: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-04-11 06:20:56.749320: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-04-11 06:20:59.717007: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "04/11/2024 06:21:03 - WARNING - __main__ - Process rank: 0, device: cpu, n_gpu: 0, distributed training: False, 16-bits training: False\n",
            "04/11/2024 06:21:03 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=0,\n",
            "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'gradient_accumulation_kwargs': None},\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_persistent_workers=False,\n",
            "dataloader_pin_memory=True,\n",
            "dataloader_prefetch_factor=None,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_num_input_tokens_seen=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=2e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=test_results_medium/runs/Apr11_06-21-03_3ec82996b54c,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "optim_target_modules=None,\n",
            "output_dir=test_results_medium,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=32,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=test_results_medium,\n",
            "save_on_each_node=False,\n",
            "save_only_model=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "split_batches=None,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "Overwrite dataset info from restored data version if exists.\n",
            "04/11/2024 06:21:04 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/nyu-mll___glue/cola/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c\n",
            "04/11/2024 06:21:04 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/nyu-mll___glue/cola/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c\n",
            "Found cached dataset glue (/root/.cache/huggingface/datasets/nyu-mll___glue/cola/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)\n",
            "04/11/2024 06:21:04 - INFO - datasets.builder - Found cached dataset glue (/root/.cache/huggingface/datasets/nyu-mll___glue/cola/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/nyu-mll___glue/cola/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c\n",
            "04/11/2024 06:21:04 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/nyu-mll___glue/cola/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c\n",
            "[INFO|configuration_utils.py:724] 2024-04-11 06:21:04,652 >> loading configuration file models/medium_v2/config.json\n",
            "[INFO|configuration_utils.py:789] 2024-04-11 06:21:04,662 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"models/medium_v2\",\n",
            "  \"architectures\": [\n",
            "    \"BertModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"cell\": {},\n",
            "  \"classifier_dropout\": null,\n",
            "  \"emb_size\": 312,\n",
            "  \"finetuning_task\": \"cola\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 312,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 1200,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 4,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"pre_trained\": \"\",\n",
            "  \"structure\": [],\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.40.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2085] 2024-04-11 06:21:04,665 >> loading file vocab.txt\n",
            "[INFO|tokenization_utils_base.py:2085] 2024-04-11 06:21:04,665 >> loading file tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2085] 2024-04-11 06:21:04,665 >> loading file added_tokens.json\n",
            "[INFO|tokenization_utils_base.py:2085] 2024-04-11 06:21:04,666 >> loading file special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2085] 2024-04-11 06:21:04,666 >> loading file tokenizer_config.json\n",
            "[INFO|modeling_utils.py:3426] 2024-04-11 06:21:05,030 >> loading weights file models/medium_v2/model.safetensors\n",
            "[INFO|modeling_utils.py:4170] 2024-04-11 06:21:05,371 >> All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
            "\n",
            "[WARNING|modeling_utils.py:4172] 2024-04-11 06:21:05,372 >> Some weights of BertForSequenceClassification were not initialized from the model checkpoint at models/medium_v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Running tokenizer on dataset:   0% 0/8551 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/nyu-mll___glue/cola/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-55b001045ce627c1.arrow\n",
            "04/11/2024 06:21:05 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/nyu-mll___glue/cola/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-55b001045ce627c1.arrow\n",
            "Running tokenizer on dataset: 100% 8551/8551 [00:02<00:00, 4018.65 examples/s]\n",
            "Running tokenizer on dataset:   0% 0/1043 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/nyu-mll___glue/cola/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-1785a025bb4a64ff.arrow\n",
            "04/11/2024 06:21:07 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/nyu-mll___glue/cola/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-1785a025bb4a64ff.arrow\n",
            "Running tokenizer on dataset: 100% 1043/1043 [00:00<00:00, 3411.22 examples/s]\n",
            "Running tokenizer on dataset:   0% 0/1063 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/nyu-mll___glue/cola/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-e1fc2c6443858055.arrow\n",
            "04/11/2024 06:21:08 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/nyu-mll___glue/cola/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-e1fc2c6443858055.arrow\n",
            "Running tokenizer on dataset: 100% 1063/1063 [00:00<00:00, 3054.53 examples/s]\n",
            "04/11/2024 06:21:08 - INFO - __main__ - Sample 1824 of the training set: {'sentence': 'I acknowledged that my father, he was tight as an owl.', 'label': 0, 'idx': 1824, 'input_ids': [101, 1045, 8969, 2008, 2026, 2269, 1010, 2002, 2001, 4389, 2004, 2019, 13547, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "04/11/2024 06:21:08 - INFO - __main__ - Sample 409 of the training set: {'sentence': 'For him to do that would be a mistake.', 'label': 1, 'idx': 409, 'input_ids': [101, 2005, 2032, 2000, 2079, 2008, 2052, 2022, 1037, 6707, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "04/11/2024 06:21:08 - INFO - __main__ - Sample 4506 of the training set: {'sentence': 'Mary sang a song, but Lee never did.', 'label': 1, 'idx': 4506, 'input_ids': [101, 2984, 6369, 1037, 2299, 1010, 2021, 3389, 2196, 2106, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "[INFO|trainer.py:776] 2024-04-11 06:21:09,444 >> The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence, idx. If sentence, idx are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2038] 2024-04-11 06:21:09,459 >> ***** Running training *****\n",
            "[INFO|trainer.py:2039] 2024-04-11 06:21:09,459 >>   Num examples = 8,551\n",
            "[INFO|trainer.py:2040] 2024-04-11 06:21:09,459 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:2041] 2024-04-11 06:21:09,459 >>   Instantaneous batch size per device = 32\n",
            "[INFO|trainer.py:2044] 2024-04-11 06:21:09,459 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "[INFO|trainer.py:2045] 2024-04-11 06:21:09,459 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:2046] 2024-04-11 06:21:09,459 >>   Total optimization steps = 804\n",
            "[INFO|trainer.py:2047] 2024-04-11 06:21:09,460 >>   Number of trainable parameters = 14,350,874\n",
            "{'loss': 0.6089, 'grad_norm': 0.5686226487159729, 'learning_rate': 7.5621890547263685e-06, 'epoch': 1.87}\n",
            " 62% 500/804 [25:11<15:09,  2.99s/it][INFO|trainer.py:3295] 2024-04-11 06:46:20,936 >> Saving model checkpoint to test_results_medium/checkpoint-500\n",
            "[INFO|configuration_utils.py:471] 2024-04-11 06:46:20,938 >> Configuration saved in test_results_medium/checkpoint-500/config.json\n",
            "[INFO|modeling_utils.py:2590] 2024-04-11 06:46:21,022 >> Model weights saved in test_results_medium/checkpoint-500/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2488] 2024-04-11 06:46:21,024 >> tokenizer config file saved in test_results_medium/checkpoint-500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2497] 2024-04-11 06:46:21,024 >> Special tokens file saved in test_results_medium/checkpoint-500/special_tokens_map.json\n",
            "100% 804/804 [39:53<00:00,  2.21s/it][INFO|trainer.py:2306] 2024-04-11 07:01:02,731 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 2393.2706, 'train_samples_per_second': 10.719, 'train_steps_per_second': 0.336, 'train_loss': 0.6029605011441814, 'epoch': 3.0}\n",
            "100% 804/804 [39:53<00:00,  2.98s/it]\n",
            "[INFO|trainer.py:3295] 2024-04-11 07:01:02,734 >> Saving model checkpoint to test_results_medium\n",
            "[INFO|configuration_utils.py:471] 2024-04-11 07:01:02,735 >> Configuration saved in test_results_medium/config.json\n",
            "[INFO|modeling_utils.py:2590] 2024-04-11 07:01:02,812 >> Model weights saved in test_results_medium/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2488] 2024-04-11 07:01:02,813 >> tokenizer config file saved in test_results_medium/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2497] 2024-04-11 07:01:02,813 >> Special tokens file saved in test_results_medium/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        3.0\n",
            "  train_loss               =      0.603\n",
            "  train_runtime            = 0:39:53.27\n",
            "  train_samples            =       8551\n",
            "  train_samples_per_second =     10.719\n",
            "  train_steps_per_second   =      0.336\n",
            "04/11/2024 07:01:02 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:776] 2024-04-11 07:01:02,831 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence, idx. If sentence, idx are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3604] 2024-04-11 07:01:02,834 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3606] 2024-04-11 07:01:02,834 >>   Num examples = 1043\n",
            "[INFO|trainer.py:3609] 2024-04-11 07:01:02,834 >>   Batch size = 8\n",
            "100% 131/131 [00:29<00:00,  4.45it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                     =        3.0\n",
            "  eval_loss                 =     0.6052\n",
            "  eval_matthews_correlation =        0.0\n",
            "  eval_runtime              = 0:00:29.65\n",
            "  eval_samples              =       1043\n",
            "  eval_samples_per_second   =     35.173\n",
            "  eval_steps_per_second     =      4.418\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "from transformers import BertModel, BertConfig, BertTokenizer\n",
        "\n",
        "model = torch.load('models/int_expanded.mod', map_location=device)\n",
        "model.save_pretrained(\"models/int_expanded_v2\")\n",
        "model_name = \"huawei-noah/TinyBERT_General_4L_312D\"\n",
        "config = BertConfig.from_pretrained(model_name)#, output_hidden_states=True, output_attentions=True)\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "# model = BertModel.from_pretrained('models/int_expanded_v2', config=config)\n",
        "tokenizer.save_pretrained(\"models/int_expanded_v2\")\n",
        "config.save_pretrained(\"models/int_expanded_v2\")\n",
        "# model.save_pretrained(\"models/int_expanded_v2\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205,
          "referenced_widgets": [
            "9d2779b010d24a9ea6353d0ca9707102",
            "7480d2b4ccf641c2bcac8d57f2c8e578",
            "78f7eabe96794c6e9763662bbf1773c5",
            "eb76ac75df6245098d4cc9456f65e57a",
            "d16c9aaefc134859928298197da5b461",
            "3231b9f700004de681daf5cebc942481",
            "b55e6dd67c2d44298a3c38dba1c6f9c3",
            "f64e4d63bd1d4bd59af549d1c60ed533",
            "f394b1627d9049eeb59ebeeb5bb3cc4f",
            "8609d8a6e86a461d93de072fd158ed5f",
            "b0f782ba6bf04afa93113ce31e35f9c5",
            "c6b8a993658044e7be690e9aacbc6fe2",
            "e053175f18c94a01b3e4bda59b14e0fa",
            "de14010e542b49da8d113ea30773559b",
            "c19c3d335a33498dad62d8a65f257d62",
            "b0881173f1cd4eb9bc27a0a68de1a52f",
            "c17923d465dc4d3fa6abde9ffb3ca3ab",
            "dea47177a0fd4868bb5ea82b8e25c045",
            "077a079546c5445181f612547f3ce593",
            "dd1e82f4c50e4ba3b8ec53500aba56dc",
            "6876ace9912243309871e676cee18f01",
            "dd4cbf43dd204b4fa9b5c4d345b64ddf"
          ]
        },
        "id": "akbzaejVJ7Jb",
        "outputId": "9e3ec659-ad10-4fca-a740-339a2ff508bd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/409 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9d2779b010d24a9ea6353d0ca9707102"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c6b8a993658044e7be690e9aacbc6fe2"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Code for running benchmark\n",
        "# !mkdir test_results_int_expanded\n",
        "!python run_glue.py --model_name_or_path models/int_expanded_v2 --task_name cola --do_train --do_eval --max_seq_length 128 --per_device_train_batch_size 32 --learning_rate 2e-5 --num_train_epochs 3 --output_dir test_results_int_expanded"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bd5hBSAzJ7Ts",
        "outputId": "3c30b002-6489-48e4-fd2a-0fed6eb77ad3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-04-11 17:23:58.347961: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-04-11 17:23:58.348062: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-04-11 17:23:58.497024: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-04-11 17:24:00.255854: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "04/11/2024 17:24:04 - WARNING - __main__ - Process rank: 0, device: cpu, n_gpu: 0, distributed training: False, 16-bits training: False\n",
            "04/11/2024 17:24:04 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=0,\n",
            "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'gradient_accumulation_kwargs': None},\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_persistent_workers=False,\n",
            "dataloader_pin_memory=True,\n",
            "dataloader_prefetch_factor=None,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_num_input_tokens_seen=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=2e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=test_results_int_expanded/runs/Apr11_17-24-04_79b69d194d2b,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "optim_target_modules=None,\n",
            "output_dir=test_results_int_expanded,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=32,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=test_results_int_expanded,\n",
            "save_on_each_node=False,\n",
            "save_only_model=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "split_batches=None,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "https://huggingface.co/datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/88644a214f5ff54714fe06b4fe093ddb2a8b6a14231e936ad93863dfb228440f.2a68834d42622b6a5a53ccfa60ef536828f83a5e011882a000ccbdace73dd8ac.incomplete\n",
            "04/11/2024 17:24:05 - INFO - datasets.utils.file_utils - https://huggingface.co/datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/88644a214f5ff54714fe06b4fe093ddb2a8b6a14231e936ad93863dfb228440f.2a68834d42622b6a5a53ccfa60ef536828f83a5e011882a000ccbdace73dd8ac.incomplete\n",
            "Downloading readme: 100% 35.3k/35.3k [00:00<00:00, 21.5MB/s]\n",
            "storing https://huggingface.co/datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md in cache at /root/.cache/huggingface/datasets/downloads/88644a214f5ff54714fe06b4fe093ddb2a8b6a14231e936ad93863dfb228440f.2a68834d42622b6a5a53ccfa60ef536828f83a5e011882a000ccbdace73dd8ac\n",
            "04/11/2024 17:24:06 - INFO - datasets.utils.file_utils - storing https://huggingface.co/datasets/nyu-mll/glue/resolve/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/README.md in cache at /root/.cache/huggingface/datasets/downloads/88644a214f5ff54714fe06b4fe093ddb2a8b6a14231e936ad93863dfb228440f.2a68834d42622b6a5a53ccfa60ef536828f83a5e011882a000ccbdace73dd8ac\n",
            "creating metadata file for /root/.cache/huggingface/datasets/downloads/88644a214f5ff54714fe06b4fe093ddb2a8b6a14231e936ad93863dfb228440f.2a68834d42622b6a5a53ccfa60ef536828f83a5e011882a000ccbdace73dd8ac\n",
            "04/11/2024 17:24:06 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/88644a214f5ff54714fe06b4fe093ddb2a8b6a14231e936ad93863dfb228440f.2a68834d42622b6a5a53ccfa60ef536828f83a5e011882a000ccbdace73dd8ac\n",
            "Generating dataset glue (/root/.cache/huggingface/datasets/nyu-mll___glue/cola/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)\n",
            "04/11/2024 17:24:07 - INFO - datasets.builder - Generating dataset glue (/root/.cache/huggingface/datasets/nyu-mll___glue/cola/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)\n",
            "Downloading and preparing dataset glue/cola to /root/.cache/huggingface/datasets/nyu-mll___glue/cola/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c...\n",
            "04/11/2024 17:24:07 - INFO - datasets.builder - Downloading and preparing dataset glue/cola to /root/.cache/huggingface/datasets/nyu-mll___glue/cola/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c...\n",
            "Dataset not on Hf google storage. Downloading and preparing it from source\n",
            "04/11/2024 17:24:08 - INFO - datasets.builder - Dataset not on Hf google storage. Downloading and preparing it from source\n",
            "hf://datasets/nyu-mll/glue@bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cola/train-00000-of-00001.parquet not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/4343391cce15f9a7afa06f06e66411bbbf75eaba0417eeab6737b928df550426.incomplete\n",
            "04/11/2024 17:24:08 - INFO - datasets.utils.file_utils - hf://datasets/nyu-mll/glue@bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cola/train-00000-of-00001.parquet not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/4343391cce15f9a7afa06f06e66411bbbf75eaba0417eeab6737b928df550426.incomplete\n",
            "Downloading data: 100% 251k/251k [00:00<00:00, 727kB/s]\n",
            "storing hf://datasets/nyu-mll/glue@bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cola/train-00000-of-00001.parquet in cache at /root/.cache/huggingface/datasets/downloads/4343391cce15f9a7afa06f06e66411bbbf75eaba0417eeab6737b928df550426\n",
            "04/11/2024 17:24:08 - INFO - datasets.utils.file_utils - storing hf://datasets/nyu-mll/glue@bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cola/train-00000-of-00001.parquet in cache at /root/.cache/huggingface/datasets/downloads/4343391cce15f9a7afa06f06e66411bbbf75eaba0417eeab6737b928df550426\n",
            "creating metadata file for /root/.cache/huggingface/datasets/downloads/4343391cce15f9a7afa06f06e66411bbbf75eaba0417eeab6737b928df550426\n",
            "04/11/2024 17:24:08 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/4343391cce15f9a7afa06f06e66411bbbf75eaba0417eeab6737b928df550426\n",
            "hf://datasets/nyu-mll/glue@bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cola/validation-00000-of-00001.parquet not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/6a6a3961a2a114ccc77ce15f77da89e5b3bbfa4f6e4d005d3617c5a94af00dac.incomplete\n",
            "04/11/2024 17:24:08 - INFO - datasets.utils.file_utils - hf://datasets/nyu-mll/glue@bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cola/validation-00000-of-00001.parquet not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/6a6a3961a2a114ccc77ce15f77da89e5b3bbfa4f6e4d005d3617c5a94af00dac.incomplete\n",
            "Downloading data: 100% 37.6k/37.6k [00:00<00:00, 70.5kB/s]\n",
            "storing hf://datasets/nyu-mll/glue@bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cola/validation-00000-of-00001.parquet in cache at /root/.cache/huggingface/datasets/downloads/6a6a3961a2a114ccc77ce15f77da89e5b3bbfa4f6e4d005d3617c5a94af00dac\n",
            "04/11/2024 17:24:09 - INFO - datasets.utils.file_utils - storing hf://datasets/nyu-mll/glue@bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cola/validation-00000-of-00001.parquet in cache at /root/.cache/huggingface/datasets/downloads/6a6a3961a2a114ccc77ce15f77da89e5b3bbfa4f6e4d005d3617c5a94af00dac\n",
            "creating metadata file for /root/.cache/huggingface/datasets/downloads/6a6a3961a2a114ccc77ce15f77da89e5b3bbfa4f6e4d005d3617c5a94af00dac\n",
            "04/11/2024 17:24:09 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/6a6a3961a2a114ccc77ce15f77da89e5b3bbfa4f6e4d005d3617c5a94af00dac\n",
            "hf://datasets/nyu-mll/glue@bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cola/test-00000-of-00001.parquet not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/0c7399dd33acf42fb5970d1dff297d39e7adacfaf4e96d640a3a33594584a65f.incomplete\n",
            "04/11/2024 17:24:09 - INFO - datasets.utils.file_utils - hf://datasets/nyu-mll/glue@bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cola/test-00000-of-00001.parquet not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/0c7399dd33acf42fb5970d1dff297d39e7adacfaf4e96d640a3a33594584a65f.incomplete\n",
            "Downloading data: 100% 37.7k/37.7k [00:00<00:00, 440kB/s]\n",
            "storing hf://datasets/nyu-mll/glue@bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cola/test-00000-of-00001.parquet in cache at /root/.cache/huggingface/datasets/downloads/0c7399dd33acf42fb5970d1dff297d39e7adacfaf4e96d640a3a33594584a65f\n",
            "04/11/2024 17:24:09 - INFO - datasets.utils.file_utils - storing hf://datasets/nyu-mll/glue@bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cola/test-00000-of-00001.parquet in cache at /root/.cache/huggingface/datasets/downloads/0c7399dd33acf42fb5970d1dff297d39e7adacfaf4e96d640a3a33594584a65f\n",
            "creating metadata file for /root/.cache/huggingface/datasets/downloads/0c7399dd33acf42fb5970d1dff297d39e7adacfaf4e96d640a3a33594584a65f\n",
            "04/11/2024 17:24:09 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/0c7399dd33acf42fb5970d1dff297d39e7adacfaf4e96d640a3a33594584a65f\n",
            "Downloading took 0.0 min\n",
            "04/11/2024 17:24:09 - INFO - datasets.download.download_manager - Downloading took 0.0 min\n",
            "Checksum Computation took 0.0 min\n",
            "04/11/2024 17:24:09 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min\n",
            "Generating train split\n",
            "04/11/2024 17:24:09 - INFO - datasets.builder - Generating train split\n",
            "Generating train split: 100% 8551/8551 [00:00<00:00, 128389.09 examples/s]\n",
            "Generating validation split\n",
            "04/11/2024 17:24:09 - INFO - datasets.builder - Generating validation split\n",
            "Generating validation split: 100% 1043/1043 [00:00<00:00, 158968.68 examples/s]\n",
            "Generating test split\n",
            "04/11/2024 17:24:09 - INFO - datasets.builder - Generating test split\n",
            "Generating test split: 100% 1063/1063 [00:00<00:00, 182458.06 examples/s]\n",
            "All the splits matched successfully.\n",
            "04/11/2024 17:24:09 - INFO - datasets.utils.info_utils - All the splits matched successfully.\n",
            "Dataset glue downloaded and prepared to /root/.cache/huggingface/datasets/nyu-mll___glue/cola/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c. Subsequent calls will reuse this data.\n",
            "04/11/2024 17:24:09 - INFO - datasets.builder - Dataset glue downloaded and prepared to /root/.cache/huggingface/datasets/nyu-mll___glue/cola/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c. Subsequent calls will reuse this data.\n",
            "[INFO|configuration_utils.py:724] 2024-04-11 17:24:09,877 >> loading configuration file models/int_expanded_v2/config.json\n",
            "[INFO|configuration_utils.py:789] 2024-04-11 17:24:09,885 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"models/int_expanded_v2\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"cell\": {},\n",
            "  \"classifier_dropout\": null,\n",
            "  \"emb_size\": 312,\n",
            "  \"finetuning_task\": \"cola\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 312,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 1200,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 4,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"pre_trained\": \"\",\n",
            "  \"structure\": [],\n",
            "  \"transformers_version\": \"4.40.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2085] 2024-04-11 17:24:09,886 >> loading file vocab.txt\n",
            "[INFO|tokenization_utils_base.py:2085] 2024-04-11 17:24:09,887 >> loading file tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2085] 2024-04-11 17:24:09,887 >> loading file added_tokens.json\n",
            "[INFO|tokenization_utils_base.py:2085] 2024-04-11 17:24:09,887 >> loading file special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2085] 2024-04-11 17:24:09,887 >> loading file tokenizer_config.json\n",
            "[INFO|modeling_utils.py:3426] 2024-04-11 17:24:10,092 >> loading weights file models/int_expanded_v2/model.safetensors\n",
            "[INFO|modeling_utils.py:4170] 2024-04-11 17:24:10,331 >> All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
            "\n",
            "[WARNING|modeling_utils.py:4172] 2024-04-11 17:24:10,331 >> Some weights of BertForSequenceClassification were not initialized from the model checkpoint at models/int_expanded_v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Running tokenizer on dataset:   0% 0/8551 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/nyu-mll___glue/cola/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-8ffef68c88a0c382.arrow\n",
            "04/11/2024 17:24:10 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/nyu-mll___glue/cola/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-8ffef68c88a0c382.arrow\n",
            "Running tokenizer on dataset: 100% 8551/8551 [00:01<00:00, 4525.07 examples/s]\n",
            "Running tokenizer on dataset:   0% 0/1043 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/nyu-mll___glue/cola/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-e17d866f31144226.arrow\n",
            "04/11/2024 17:24:12 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/nyu-mll___glue/cola/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-e17d866f31144226.arrow\n",
            "Running tokenizer on dataset: 100% 1043/1043 [00:00<00:00, 5385.92 examples/s]\n",
            "Running tokenizer on dataset:   0% 0/1063 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/nyu-mll___glue/cola/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-86eea399132ef046.arrow\n",
            "04/11/2024 17:24:12 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/nyu-mll___glue/cola/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-86eea399132ef046.arrow\n",
            "Running tokenizer on dataset: 100% 1063/1063 [00:00<00:00, 5629.75 examples/s]\n",
            "04/11/2024 17:24:12 - INFO - __main__ - Sample 1824 of the training set: {'sentence': 'I acknowledged that my father, he was tight as an owl.', 'label': 0, 'idx': 1824, 'input_ids': [101, 1045, 8969, 2008, 2026, 2269, 1010, 2002, 2001, 4389, 2004, 2019, 13547, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "04/11/2024 17:24:12 - INFO - __main__ - Sample 409 of the training set: {'sentence': 'For him to do that would be a mistake.', 'label': 1, 'idx': 409, 'input_ids': [101, 2005, 2032, 2000, 2079, 2008, 2052, 2022, 1037, 6707, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "04/11/2024 17:24:12 - INFO - __main__ - Sample 4506 of the training set: {'sentence': 'Mary sang a song, but Lee never did.', 'label': 1, 'idx': 4506, 'input_ids': [101, 2984, 6369, 1037, 2299, 1010, 2021, 3389, 2196, 2106, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "Downloading builder script: 100% 5.75k/5.75k [00:00<00:00, 11.9MB/s]\n",
            "[INFO|trainer.py:785] 2024-04-11 17:24:13,810 >> The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence, idx. If sentence, idx are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2047] 2024-04-11 17:24:13,818 >> ***** Running training *****\n",
            "[INFO|trainer.py:2048] 2024-04-11 17:24:13,819 >>   Num examples = 8,551\n",
            "[INFO|trainer.py:2049] 2024-04-11 17:24:13,819 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:2050] 2024-04-11 17:24:13,819 >>   Instantaneous batch size per device = 32\n",
            "[INFO|trainer.py:2053] 2024-04-11 17:24:13,819 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "[INFO|trainer.py:2054] 2024-04-11 17:24:13,819 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:2055] 2024-04-11 17:24:13,819 >>   Total optimization steps = 804\n",
            "[INFO|trainer.py:2056] 2024-04-11 17:24:13,819 >>   Number of trainable parameters = 14,350,874\n",
            "{'loss': 0.6095, 'grad_norm': 0.31946131587028503, 'learning_rate': 7.5621890547263685e-06, 'epoch': 1.87}\n",
            " 62% 500/804 [30:12<16:21,  3.23s/it][INFO|trainer.py:3304] 2024-04-11 17:54:25,992 >> Saving model checkpoint to test_results_int_expanded/checkpoint-500\n",
            "[INFO|configuration_utils.py:471] 2024-04-11 17:54:25,994 >> Configuration saved in test_results_int_expanded/checkpoint-500/config.json\n",
            "[INFO|modeling_utils.py:2590] 2024-04-11 17:54:26,117 >> Model weights saved in test_results_int_expanded/checkpoint-500/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2488] 2024-04-11 17:54:26,118 >> tokenizer config file saved in test_results_int_expanded/checkpoint-500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2497] 2024-04-11 17:54:26,119 >> Special tokens file saved in test_results_int_expanded/checkpoint-500/special_tokens_map.json\n",
            "100% 804/804 [47:10<00:00,  2.63s/it][INFO|trainer.py:2315] 2024-04-11 18:11:24,364 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 2830.5442, 'train_samples_per_second': 9.063, 'train_steps_per_second': 0.284, 'train_loss': 0.606472679631627, 'epoch': 3.0}\n",
            "100% 804/804 [47:10<00:00,  3.52s/it]\n",
            "[INFO|trainer.py:3304] 2024-04-11 18:11:24,367 >> Saving model checkpoint to test_results_int_expanded\n",
            "[INFO|configuration_utils.py:471] 2024-04-11 18:11:24,369 >> Configuration saved in test_results_int_expanded/config.json\n",
            "[INFO|modeling_utils.py:2590] 2024-04-11 18:11:24,455 >> Model weights saved in test_results_int_expanded/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2488] 2024-04-11 18:11:24,456 >> tokenizer config file saved in test_results_int_expanded/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2497] 2024-04-11 18:11:24,456 >> Special tokens file saved in test_results_int_expanded/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        3.0\n",
            "  train_loss               =     0.6065\n",
            "  train_runtime            = 0:47:10.54\n",
            "  train_samples            =       8551\n",
            "  train_samples_per_second =      9.063\n",
            "  train_steps_per_second   =      0.284\n",
            "04/11/2024 18:11:24 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:785] 2024-04-11 18:11:24,477 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence, idx. If sentence, idx are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3613] 2024-04-11 18:11:24,480 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3615] 2024-04-11 18:11:24,480 >>   Num examples = 1043\n",
            "[INFO|trainer.py:3618] 2024-04-11 18:11:24,480 >>   Batch size = 8\n",
            "100% 131/131 [00:35<00:00,  3.69it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                     =        3.0\n",
            "  eval_loss                 =     0.6137\n",
            "  eval_matthews_correlation =        0.0\n",
            "  eval_runtime              = 0:00:35.79\n",
            "  eval_samples              =       1043\n",
            "  eval_samples_per_second   =     29.138\n",
            "  eval_steps_per_second     =       3.66\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RTE Benchmark**"
      ],
      "metadata": {
        "id": "jTeFjWBzQErq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "from transformers import BertModel, BertConfig, BertTokenizer\n",
        "\n",
        "model_name = \"huawei-noah/TinyBERT_General_4L_312D\"\n",
        "config = BertConfig.from_pretrained(model_name)#, output_hidden_states=True, output_attentions=True)\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "model = BertModel.from_pretrained(model_name, config=config)\n",
        "tokenizer.save_pretrained(\"models/basev2\")\n",
        "config.save_pretrained(\"models/basev2\")\n",
        "model.save_pretrained(\"models/basev2\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "faea33c75fd249c2a08c728c95f2fd66",
            "b9b5361cbe984caa9c972edccbba3c2c",
            "c6ffc89b9f944d5face06b010d03c342",
            "4ef0fbe5baa849b59d81f1ce3d2d6460",
            "6fbfd58481f941ef9e2cc6431fd481a5",
            "7d3d515d41b740a78e0567a98e816d8d",
            "361e127d50e44aa38c0527481e94fe20",
            "8b26d3d4762348b3a0203085fa34b894",
            "d88503c7ac6e4fd1b0f3f1eceb69c284",
            "f4e8a4c81b7a4544a6ee856909133b79",
            "7f6f95ad4a0b42c081e88df65f520c8a"
          ]
        },
        "id": "GpcuuP80Q5y-",
        "outputId": "7930bb13-8131-4162-ad56-71f5a0d5dc27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/62.7M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "faea33c75fd249c2a08c728c95f2fd66"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Code for running benchmark\n",
        "!mkdir test_results_tinyBERT\n",
        "!python run_glue.py --model_name_or_path models/basev2 --task_name RTE --do_train --do_eval --max_seq_length 128 --per_device_train_batch_size 32 --learning_rate 2e-5 --num_train_epochs 3 --output_dir test_results_tinyBERT"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wvPfo6jvQ_3h",
        "outputId": "5a7d1f07-dae6-4686-dbfc-07d63164a05d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-04-11 18:13:02.862697: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-04-11 18:13:02.862799: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-04-11 18:13:02.868559: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-04-11 18:13:05.270970: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "04/11/2024 18:13:08 - WARNING - __main__ - Process rank: 0, device: cpu, n_gpu: 0, distributed training: False, 16-bits training: False\n",
            "04/11/2024 18:13:08 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=0,\n",
            "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'gradient_accumulation_kwargs': None},\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_persistent_workers=False,\n",
            "dataloader_pin_memory=True,\n",
            "dataloader_prefetch_factor=None,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_num_input_tokens_seen=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=2e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=test_results_tinyBERT/runs/Apr11_18-13-08_79b69d194d2b,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "optim_target_modules=None,\n",
            "output_dir=test_results_tinyBERT,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=32,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=test_results_tinyBERT,\n",
            "save_on_each_node=False,\n",
            "save_only_model=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "split_batches=None,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "Generating dataset glue (/root/.cache/huggingface/datasets/nyu-mll___glue/rte/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)\n",
            "04/11/2024 18:13:10 - INFO - datasets.builder - Generating dataset glue (/root/.cache/huggingface/datasets/nyu-mll___glue/rte/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)\n",
            "Downloading and preparing dataset glue/rte to /root/.cache/huggingface/datasets/nyu-mll___glue/rte/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c...\n",
            "04/11/2024 18:13:10 - INFO - datasets.builder - Downloading and preparing dataset glue/rte to /root/.cache/huggingface/datasets/nyu-mll___glue/rte/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c...\n",
            "Dataset not on Hf google storage. Downloading and preparing it from source\n",
            "04/11/2024 18:13:10 - INFO - datasets.builder - Dataset not on Hf google storage. Downloading and preparing it from source\n",
            "hf://datasets/nyu-mll/glue@bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/rte/train-00000-of-00001.parquet not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/596933fec4164676c17f3ac6b1ec5457820a0dce25edcbdf4c05d7a5285c43d9.incomplete\n",
            "04/11/2024 18:13:11 - INFO - datasets.utils.file_utils - hf://datasets/nyu-mll/glue@bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/rte/train-00000-of-00001.parquet not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/596933fec4164676c17f3ac6b1ec5457820a0dce25edcbdf4c05d7a5285c43d9.incomplete\n",
            "Downloading data: 100% 584k/584k [00:00<00:00, 2.85MB/s]\n",
            "storing hf://datasets/nyu-mll/glue@bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/rte/train-00000-of-00001.parquet in cache at /root/.cache/huggingface/datasets/downloads/596933fec4164676c17f3ac6b1ec5457820a0dce25edcbdf4c05d7a5285c43d9\n",
            "04/11/2024 18:13:11 - INFO - datasets.utils.file_utils - storing hf://datasets/nyu-mll/glue@bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/rte/train-00000-of-00001.parquet in cache at /root/.cache/huggingface/datasets/downloads/596933fec4164676c17f3ac6b1ec5457820a0dce25edcbdf4c05d7a5285c43d9\n",
            "creating metadata file for /root/.cache/huggingface/datasets/downloads/596933fec4164676c17f3ac6b1ec5457820a0dce25edcbdf4c05d7a5285c43d9\n",
            "04/11/2024 18:13:11 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/596933fec4164676c17f3ac6b1ec5457820a0dce25edcbdf4c05d7a5285c43d9\n",
            "hf://datasets/nyu-mll/glue@bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/rte/validation-00000-of-00001.parquet not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/3596fc0232cf9a5364a906ac38deec21041bccc197d94fd1868667f05be605be.incomplete\n",
            "04/11/2024 18:13:11 - INFO - datasets.utils.file_utils - hf://datasets/nyu-mll/glue@bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/rte/validation-00000-of-00001.parquet not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/3596fc0232cf9a5364a906ac38deec21041bccc197d94fd1868667f05be605be.incomplete\n",
            "Downloading data: 100% 69.0k/69.0k [00:00<00:00, 885kB/s]\n",
            "storing hf://datasets/nyu-mll/glue@bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/rte/validation-00000-of-00001.parquet in cache at /root/.cache/huggingface/datasets/downloads/3596fc0232cf9a5364a906ac38deec21041bccc197d94fd1868667f05be605be\n",
            "04/11/2024 18:13:12 - INFO - datasets.utils.file_utils - storing hf://datasets/nyu-mll/glue@bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/rte/validation-00000-of-00001.parquet in cache at /root/.cache/huggingface/datasets/downloads/3596fc0232cf9a5364a906ac38deec21041bccc197d94fd1868667f05be605be\n",
            "creating metadata file for /root/.cache/huggingface/datasets/downloads/3596fc0232cf9a5364a906ac38deec21041bccc197d94fd1868667f05be605be\n",
            "04/11/2024 18:13:12 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/3596fc0232cf9a5364a906ac38deec21041bccc197d94fd1868667f05be605be\n",
            "hf://datasets/nyu-mll/glue@bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/rte/test-00000-of-00001.parquet not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/4338eed985e07aeb47b083c2f053bf2e45e85284844032e6d0e7c702f4bea421.incomplete\n",
            "04/11/2024 18:13:12 - INFO - datasets.utils.file_utils - hf://datasets/nyu-mll/glue@bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/rte/test-00000-of-00001.parquet not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/4338eed985e07aeb47b083c2f053bf2e45e85284844032e6d0e7c702f4bea421.incomplete\n",
            "Downloading data: 100% 621k/621k [00:00<00:00, 3.37MB/s]\n",
            "storing hf://datasets/nyu-mll/glue@bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/rte/test-00000-of-00001.parquet in cache at /root/.cache/huggingface/datasets/downloads/4338eed985e07aeb47b083c2f053bf2e45e85284844032e6d0e7c702f4bea421\n",
            "04/11/2024 18:13:12 - INFO - datasets.utils.file_utils - storing hf://datasets/nyu-mll/glue@bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/rte/test-00000-of-00001.parquet in cache at /root/.cache/huggingface/datasets/downloads/4338eed985e07aeb47b083c2f053bf2e45e85284844032e6d0e7c702f4bea421\n",
            "creating metadata file for /root/.cache/huggingface/datasets/downloads/4338eed985e07aeb47b083c2f053bf2e45e85284844032e6d0e7c702f4bea421\n",
            "04/11/2024 18:13:12 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/4338eed985e07aeb47b083c2f053bf2e45e85284844032e6d0e7c702f4bea421\n",
            "Downloading took 0.0 min\n",
            "04/11/2024 18:13:12 - INFO - datasets.download.download_manager - Downloading took 0.0 min\n",
            "Checksum Computation took 0.0 min\n",
            "04/11/2024 18:13:12 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min\n",
            "Generating train split\n",
            "04/11/2024 18:13:12 - INFO - datasets.builder - Generating train split\n",
            "Generating train split: 100% 2490/2490 [00:00<00:00, 137310.24 examples/s]\n",
            "Generating validation split\n",
            "04/11/2024 18:13:12 - INFO - datasets.builder - Generating validation split\n",
            "Generating validation split: 100% 277/277 [00:00<00:00, 92288.68 examples/s]\n",
            "Generating test split\n",
            "04/11/2024 18:13:12 - INFO - datasets.builder - Generating test split\n",
            "Generating test split: 100% 3000/3000 [00:00<00:00, 270019.57 examples/s]\n",
            "All the splits matched successfully.\n",
            "04/11/2024 18:13:12 - INFO - datasets.utils.info_utils - All the splits matched successfully.\n",
            "Dataset glue downloaded and prepared to /root/.cache/huggingface/datasets/nyu-mll___glue/rte/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c. Subsequent calls will reuse this data.\n",
            "04/11/2024 18:13:12 - INFO - datasets.builder - Dataset glue downloaded and prepared to /root/.cache/huggingface/datasets/nyu-mll___glue/rte/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c. Subsequent calls will reuse this data.\n",
            "[INFO|configuration_utils.py:724] 2024-04-11 18:13:12,424 >> loading configuration file models/basev2/config.json\n",
            "[INFO|configuration_utils.py:789] 2024-04-11 18:13:12,429 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"models/basev2\",\n",
            "  \"architectures\": [\n",
            "    \"BertModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"cell\": {},\n",
            "  \"classifier_dropout\": null,\n",
            "  \"emb_size\": 312,\n",
            "  \"finetuning_task\": \"rte\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 312,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 1200,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 4,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"pre_trained\": \"\",\n",
            "  \"structure\": [],\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.40.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2085] 2024-04-11 18:13:12,430 >> loading file vocab.txt\n",
            "[INFO|tokenization_utils_base.py:2085] 2024-04-11 18:13:12,430 >> loading file tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2085] 2024-04-11 18:13:12,430 >> loading file added_tokens.json\n",
            "[INFO|tokenization_utils_base.py:2085] 2024-04-11 18:13:12,430 >> loading file special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2085] 2024-04-11 18:13:12,430 >> loading file tokenizer_config.json\n",
            "[INFO|modeling_utils.py:3426] 2024-04-11 18:13:12,661 >> loading weights file models/basev2/model.safetensors\n",
            "[INFO|modeling_utils.py:4170] 2024-04-11 18:13:12,798 >> All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
            "\n",
            "[WARNING|modeling_utils.py:4172] 2024-04-11 18:13:12,798 >> Some weights of BertForSequenceClassification were not initialized from the model checkpoint at models/basev2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Running tokenizer on dataset:   0% 0/2490 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/nyu-mll___glue/rte/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-c37d9914bb50db86.arrow\n",
            "04/11/2024 18:13:13 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/nyu-mll___glue/rte/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-c37d9914bb50db86.arrow\n",
            "Running tokenizer on dataset: 100% 2490/2490 [00:00<00:00, 2762.88 examples/s]\n",
            "Running tokenizer on dataset:   0% 0/277 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/nyu-mll___glue/rte/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-8364eda200fb2168.arrow\n",
            "04/11/2024 18:13:13 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/nyu-mll___glue/rte/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-8364eda200fb2168.arrow\n",
            "Running tokenizer on dataset: 100% 277/277 [00:00<00:00, 2778.07 examples/s]\n",
            "Running tokenizer on dataset:   0% 0/3000 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/nyu-mll___glue/rte/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-82c5e4eca9121157.arrow\n",
            "04/11/2024 18:13:14 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/nyu-mll___glue/rte/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-82c5e4eca9121157.arrow\n",
            "Running tokenizer on dataset: 100% 3000/3000 [00:01<00:00, 1877.23 examples/s]\n",
            "04/11/2024 18:13:15 - INFO - __main__ - Sample 456 of the training set: {'sentence1': \"A computer system failure closed down share trading at the Tokyo Stock Exchange for most of yesterday, the worst disruption to date for Asia's largest bourse.\", 'sentence2': 'The Tokyo Stock Exchange was closed down by computer system failure.', 'label': 0, 'idx': 456, 'input_ids': [101, 1037, 3274, 2291, 4945, 2701, 2091, 3745, 6202, 2012, 1996, 5522, 4518, 3863, 2005, 2087, 1997, 7483, 1010, 1996, 5409, 20461, 2000, 3058, 2005, 4021, 1005, 1055, 2922, 8945, 28393, 1012, 102, 1996, 5522, 4518, 3863, 2001, 2701, 2091, 2011, 3274, 2291, 4945, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "04/11/2024 18:13:15 - INFO - __main__ - Sample 102 of the training set: {'sentence1': 'Nagin defended his plan to return up to 180,000 people to the city, within a week and a half, despite concerns about the short supply of drinking water and heavily polluted floodwaters.', 'sentence2': 'Thousands of people are expected to return to New Orleans this week, as areas of the city are opened up to residents.', 'label': 1, 'idx': 102, 'input_ids': [101, 6583, 11528, 8047, 2010, 2933, 2000, 2709, 2039, 2000, 8380, 1010, 2199, 2111, 2000, 1996, 2103, 1010, 2306, 1037, 2733, 1998, 1037, 2431, 1010, 2750, 5936, 2055, 1996, 2460, 4425, 1997, 5948, 2300, 1998, 4600, 8554, 12926, 7186, 5880, 2015, 1012, 102, 5190, 1997, 2111, 2024, 3517, 2000, 2709, 2000, 2047, 5979, 2023, 2733, 1010, 2004, 2752, 1997, 1996, 2103, 2024, 2441, 2039, 2000, 3901, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "04/11/2024 18:13:15 - INFO - __main__ - Sample 1126 of the training set: {'sentence1': \"In 1969, he drew up the report proposing the expulsion from the party of the Manifesto group. In 1984, after Berlinguer's death, Natta was elected as party secretary.\", 'sentence2': 'Natta supported the Manifesto group.', 'label': 1, 'idx': 1126, 'input_ids': [101, 1999, 3440, 1010, 2002, 3881, 2039, 1996, 3189, 21991, 1996, 18272, 2013, 1996, 2283, 1997, 1996, 17124, 2177, 1012, 1999, 3118, 1010, 2044, 4068, 9077, 2099, 1005, 1055, 2331, 1010, 14085, 2696, 2001, 2700, 2004, 2283, 3187, 1012, 102, 14085, 2696, 3569, 1996, 17124, 2177, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "[INFO|trainer.py:785] 2024-04-11 18:13:16,652 >> The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2047] 2024-04-11 18:13:16,663 >> ***** Running training *****\n",
            "[INFO|trainer.py:2048] 2024-04-11 18:13:16,664 >>   Num examples = 2,490\n",
            "[INFO|trainer.py:2049] 2024-04-11 18:13:16,664 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:2050] 2024-04-11 18:13:16,664 >>   Instantaneous batch size per device = 32\n",
            "[INFO|trainer.py:2053] 2024-04-11 18:13:16,664 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "[INFO|trainer.py:2054] 2024-04-11 18:13:16,664 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:2055] 2024-04-11 18:13:16,664 >>   Total optimization steps = 234\n",
            "[INFO|trainer.py:2056] 2024-04-11 18:13:16,665 >>   Number of trainable parameters = 14,350,874\n",
            "100% 234/234 [13:22<00:00,  3.24s/it][INFO|trainer.py:2315] 2024-04-11 18:26:39,090 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 802.4256, 'train_samples_per_second': 9.309, 'train_steps_per_second': 0.292, 'train_loss': 0.6069478744115585, 'epoch': 3.0}\n",
            "100% 234/234 [13:22<00:00,  3.43s/it]\n",
            "[INFO|trainer.py:3304] 2024-04-11 18:26:39,093 >> Saving model checkpoint to test_results_tinyBERT\n",
            "[INFO|configuration_utils.py:471] 2024-04-11 18:26:39,094 >> Configuration saved in test_results_tinyBERT/config.json\n",
            "[INFO|modeling_utils.py:2590] 2024-04-11 18:26:39,166 >> Model weights saved in test_results_tinyBERT/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2488] 2024-04-11 18:26:39,167 >> tokenizer config file saved in test_results_tinyBERT/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2497] 2024-04-11 18:26:39,168 >> Special tokens file saved in test_results_tinyBERT/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        3.0\n",
            "  train_loss               =     0.6069\n",
            "  train_runtime            = 0:13:22.42\n",
            "  train_samples            =       2490\n",
            "  train_samples_per_second =      9.309\n",
            "  train_steps_per_second   =      0.292\n",
            "04/11/2024 18:26:39 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:785] 2024-04-11 18:26:39,186 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3613] 2024-04-11 18:26:39,188 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3615] 2024-04-11 18:26:39,188 >>   Num examples = 277\n",
            "[INFO|trainer.py:3618] 2024-04-11 18:26:39,188 >>   Batch size = 8\n",
            "100% 35/35 [00:08<00:00,  4.27it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        3.0\n",
            "  eval_accuracy           =      0.639\n",
            "  eval_loss               =     0.6342\n",
            "  eval_runtime            = 0:00:08.45\n",
            "  eval_samples            =        277\n",
            "  eval_samples_per_second =     32.774\n",
            "  eval_steps_per_second   =      4.141\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Code for running benchmark\n",
        "!mkdir test_results_small\n",
        "!python run_glue.py --model_name_or_path models/small --task_name RTE --do_train --do_eval --max_seq_length 128 --per_device_train_batch_size 32 --learning_rate 2e-5 --num_train_epochs 3 --output_dir test_results_small"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iinB8IXWRDtK",
        "outputId": "278ebe26-f563-4da5-9ec6-5973c093369a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-04-11 18:28:25.247330: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-04-11 18:28:25.247405: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-04-11 18:28:25.249101: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-04-11 18:28:26.847473: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "04/11/2024 18:28:31 - WARNING - __main__ - Process rank: 0, device: cpu, n_gpu: 0, distributed training: False, 16-bits training: False\n",
            "04/11/2024 18:28:31 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=0,\n",
            "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'gradient_accumulation_kwargs': None},\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_persistent_workers=False,\n",
            "dataloader_pin_memory=True,\n",
            "dataloader_prefetch_factor=None,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_num_input_tokens_seen=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=2e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=test_results_small/runs/Apr11_18-28-31_79b69d194d2b,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "optim_target_modules=None,\n",
            "output_dir=test_results_small,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=32,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=test_results_small,\n",
            "save_on_each_node=False,\n",
            "save_only_model=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "split_batches=None,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "Overwrite dataset info from restored data version if exists.\n",
            "04/11/2024 18:28:34 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/nyu-mll___glue/rte/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c\n",
            "04/11/2024 18:28:34 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/nyu-mll___glue/rte/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c\n",
            "Found cached dataset glue (/root/.cache/huggingface/datasets/nyu-mll___glue/rte/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)\n",
            "04/11/2024 18:28:34 - INFO - datasets.builder - Found cached dataset glue (/root/.cache/huggingface/datasets/nyu-mll___glue/rte/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/nyu-mll___glue/rte/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c\n",
            "04/11/2024 18:28:34 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/nyu-mll___glue/rte/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c\n",
            "[INFO|configuration_utils.py:724] 2024-04-11 18:28:34,965 >> loading configuration file models/small/config.json\n",
            "[INFO|configuration_utils.py:789] 2024-04-11 18:28:34,981 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"models/small\",\n",
            "  \"architectures\": [\n",
            "    \"BertModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"cell\": {},\n",
            "  \"classifier_dropout\": null,\n",
            "  \"emb_size\": 312,\n",
            "  \"finetuning_task\": \"rte\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 312,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 1200,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 4,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"pre_trained\": \"\",\n",
            "  \"structure\": [],\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.40.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2085] 2024-04-11 18:28:34,985 >> loading file vocab.txt\n",
            "[INFO|tokenization_utils_base.py:2085] 2024-04-11 18:28:34,985 >> loading file tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2085] 2024-04-11 18:28:34,985 >> loading file added_tokens.json\n",
            "[INFO|tokenization_utils_base.py:2085] 2024-04-11 18:28:34,985 >> loading file special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2085] 2024-04-11 18:28:34,985 >> loading file tokenizer_config.json\n",
            "[INFO|modeling_utils.py:3426] 2024-04-11 18:28:35,343 >> loading weights file models/small/model.safetensors\n",
            "[INFO|modeling_utils.py:4170] 2024-04-11 18:28:35,667 >> All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
            "\n",
            "[WARNING|modeling_utils.py:4172] 2024-04-11 18:28:35,674 >> Some weights of BertForSequenceClassification were not initialized from the model checkpoint at models/small and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Running tokenizer on dataset:   0% 0/2490 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/nyu-mll___glue/rte/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-c582fa6c0d01e564.arrow\n",
            "04/11/2024 18:28:36 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/nyu-mll___glue/rte/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-c582fa6c0d01e564.arrow\n",
            "Running tokenizer on dataset: 100% 2490/2490 [00:02<00:00, 990.14 examples/s] \n",
            "Running tokenizer on dataset:   0% 0/277 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/nyu-mll___glue/rte/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-036b092eaab8a9ed.arrow\n",
            "04/11/2024 18:28:38 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/nyu-mll___glue/rte/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-036b092eaab8a9ed.arrow\n",
            "Running tokenizer on dataset: 100% 277/277 [00:00<00:00, 1366.04 examples/s]\n",
            "Running tokenizer on dataset:   0% 0/3000 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/nyu-mll___glue/rte/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-116ef84e70091182.arrow\n",
            "04/11/2024 18:28:38 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/nyu-mll___glue/rte/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-116ef84e70091182.arrow\n",
            "Running tokenizer on dataset: 100% 3000/3000 [00:02<00:00, 1395.03 examples/s]\n",
            "04/11/2024 18:28:40 - INFO - __main__ - Sample 456 of the training set: {'sentence1': \"A computer system failure closed down share trading at the Tokyo Stock Exchange for most of yesterday, the worst disruption to date for Asia's largest bourse.\", 'sentence2': 'The Tokyo Stock Exchange was closed down by computer system failure.', 'label': 0, 'idx': 456, 'input_ids': [101, 1037, 3274, 2291, 4945, 2701, 2091, 3745, 6202, 2012, 1996, 5522, 4518, 3863, 2005, 2087, 1997, 7483, 1010, 1996, 5409, 20461, 2000, 3058, 2005, 4021, 1005, 1055, 2922, 8945, 28393, 1012, 102, 1996, 5522, 4518, 3863, 2001, 2701, 2091, 2011, 3274, 2291, 4945, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "04/11/2024 18:28:40 - INFO - __main__ - Sample 102 of the training set: {'sentence1': 'Nagin defended his plan to return up to 180,000 people to the city, within a week and a half, despite concerns about the short supply of drinking water and heavily polluted floodwaters.', 'sentence2': 'Thousands of people are expected to return to New Orleans this week, as areas of the city are opened up to residents.', 'label': 1, 'idx': 102, 'input_ids': [101, 6583, 11528, 8047, 2010, 2933, 2000, 2709, 2039, 2000, 8380, 1010, 2199, 2111, 2000, 1996, 2103, 1010, 2306, 1037, 2733, 1998, 1037, 2431, 1010, 2750, 5936, 2055, 1996, 2460, 4425, 1997, 5948, 2300, 1998, 4600, 8554, 12926, 7186, 5880, 2015, 1012, 102, 5190, 1997, 2111, 2024, 3517, 2000, 2709, 2000, 2047, 5979, 2023, 2733, 1010, 2004, 2752, 1997, 1996, 2103, 2024, 2441, 2039, 2000, 3901, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "04/11/2024 18:28:40 - INFO - __main__ - Sample 1126 of the training set: {'sentence1': \"In 1969, he drew up the report proposing the expulsion from the party of the Manifesto group. In 1984, after Berlinguer's death, Natta was elected as party secretary.\", 'sentence2': 'Natta supported the Manifesto group.', 'label': 1, 'idx': 1126, 'input_ids': [101, 1999, 3440, 1010, 2002, 3881, 2039, 1996, 3189, 21991, 1996, 18272, 2013, 1996, 2283, 1997, 1996, 17124, 2177, 1012, 1999, 3118, 1010, 2044, 4068, 9077, 2099, 1005, 1055, 2331, 1010, 14085, 2696, 2001, 2700, 2004, 2283, 3187, 1012, 102, 14085, 2696, 3569, 1996, 17124, 2177, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "[INFO|trainer.py:785] 2024-04-11 18:28:41,680 >> The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2047] 2024-04-11 18:28:41,691 >> ***** Running training *****\n",
            "[INFO|trainer.py:2048] 2024-04-11 18:28:41,691 >>   Num examples = 2,490\n",
            "[INFO|trainer.py:2049] 2024-04-11 18:28:41,691 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:2050] 2024-04-11 18:28:41,691 >>   Instantaneous batch size per device = 32\n",
            "[INFO|trainer.py:2053] 2024-04-11 18:28:41,691 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "[INFO|trainer.py:2054] 2024-04-11 18:28:41,692 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:2055] 2024-04-11 18:28:41,692 >>   Total optimization steps = 234\n",
            "[INFO|trainer.py:2056] 2024-04-11 18:28:41,692 >>   Number of trainable parameters = 14,350,874\n",
            "100% 234/234 [12:49<00:00,  3.34s/it][INFO|trainer.py:2315] 2024-04-11 18:41:30,884 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 769.1921, 'train_samples_per_second': 9.711, 'train_steps_per_second': 0.304, 'train_loss': 0.6802058423686231, 'epoch': 3.0}\n",
            "100% 234/234 [12:49<00:00,  3.29s/it]\n",
            "[INFO|trainer.py:3304] 2024-04-11 18:41:30,888 >> Saving model checkpoint to test_results_small\n",
            "[INFO|configuration_utils.py:471] 2024-04-11 18:41:30,890 >> Configuration saved in test_results_small/config.json\n",
            "[INFO|modeling_utils.py:2590] 2024-04-11 18:41:30,976 >> Model weights saved in test_results_small/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2488] 2024-04-11 18:41:30,977 >> tokenizer config file saved in test_results_small/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2497] 2024-04-11 18:41:30,978 >> Special tokens file saved in test_results_small/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        3.0\n",
            "  train_loss               =     0.6802\n",
            "  train_runtime            = 0:12:49.19\n",
            "  train_samples            =       2490\n",
            "  train_samples_per_second =      9.711\n",
            "  train_steps_per_second   =      0.304\n",
            "04/11/2024 18:41:30 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:785] 2024-04-11 18:41:30,999 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3613] 2024-04-11 18:41:31,002 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3615] 2024-04-11 18:41:31,003 >>   Num examples = 277\n",
            "[INFO|trainer.py:3618] 2024-04-11 18:41:31,003 >>   Batch size = 8\n",
            "100% 35/35 [00:09<00:00,  3.82it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        3.0\n",
            "  eval_accuracy           =     0.6318\n",
            "  eval_loss               =     0.6542\n",
            "  eval_runtime            = 0:00:09.42\n",
            "  eval_samples            =        277\n",
            "  eval_samples_per_second =     29.404\n",
            "  eval_steps_per_second   =      3.715\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "from transformers import BertModel, BertConfig, BertTokenizer\n",
        "\n",
        "model = torch.load('models/medium.mod', map_location=device)\n",
        "model.save_pretrained(\"models/medium\")\n",
        "# model_name = \"huawei-noah/TinyBERT_General_4L_312D\"\n",
        "config = BertConfig.from_pretrained(model_name)#, output_hidden_states=True, output_attentions=True)\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "model = BertModel.from_pretrained('models/medium', config=config)\n",
        "tokenizer.save_pretrained(\"models/medium_v2\")\n",
        "config.save_pretrained(\"models/medium_v2\")\n",
        "model.save_pretrained(\"models/medium_v2\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "2FD2RAMQROPw",
        "outputId": "e32ca000-d848-4d72-f680-063e6f70beb7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "PytorchStreamReader failed reading zip archive: failed finding central directory",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-68e2ed79cb78>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBertModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBertConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBertTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'models/medium.mod'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"models/medium\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# model_name = \"huawei-noah/TinyBERT_General_4L_312D\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1003\u001b[0m             \u001b[0morig_position\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1004\u001b[0m             \u001b[0moverall_storage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1005\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_reader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1006\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0m_is_torchscript_zip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_zipfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1007\u001b[0m                     warnings.warn(\"'torch.load' received a zip file that looks like a TorchScript archive\"\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name_or_buffer)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_zipfile_reader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPyTorchFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: PytorchStreamReader failed reading zip archive: failed finding central directory"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Code for running benchmark\n",
        "# !mkdir test_results_medium_RTE\n",
        "!python run_glue.py --model_name_or_path models/medium --task_name RTE --do_train --do_eval --max_seq_length 128 --per_device_train_batch_size 32 --learning_rate 2e-5 --num_train_epochs 3 --output_dir test_results_medium_RTE"
      ],
      "metadata": {
        "id": "WVjTUG6uRaKL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6063be2a-6e5e-49a0-eb91-e372df738975"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-04-11 20:25:02.063484: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-04-11 20:25:02.063625: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-04-11 20:25:02.066269: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-04-11 20:25:05.126814: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "04/11/2024 20:25:08 - WARNING - __main__ - Process rank: 0, device: cpu, n_gpu: 0, distributed training: False, 16-bits training: False\n",
            "04/11/2024 20:25:08 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=0,\n",
            "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'gradient_accumulation_kwargs': None},\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_persistent_workers=False,\n",
            "dataloader_pin_memory=True,\n",
            "dataloader_prefetch_factor=None,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_num_input_tokens_seen=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=2e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=test_results_medium_RTE/runs/Apr11_20-25-08_79b69d194d2b,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "optim_target_modules=None,\n",
            "output_dir=test_results_medium_RTE,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=32,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=test_results_medium_RTE,\n",
            "save_on_each_node=False,\n",
            "save_only_model=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "split_batches=None,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "Overwrite dataset info from restored data version if exists.\n",
            "04/11/2024 20:25:11 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/nyu-mll___glue/rte/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c\n",
            "04/11/2024 20:25:11 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/nyu-mll___glue/rte/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c\n",
            "Found cached dataset glue (/root/.cache/huggingface/datasets/nyu-mll___glue/rte/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)\n",
            "04/11/2024 20:25:11 - INFO - datasets.builder - Found cached dataset glue (/root/.cache/huggingface/datasets/nyu-mll___glue/rte/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/nyu-mll___glue/rte/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c\n",
            "04/11/2024 20:25:11 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/nyu-mll___glue/rte/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c\n",
            "[INFO|configuration_utils.py:724] 2024-04-11 20:25:11,421 >> loading configuration file models/medium/config.json\n",
            "[INFO|configuration_utils.py:789] 2024-04-11 20:25:11,425 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"models/medium\",\n",
            "  \"architectures\": [\n",
            "    \"BertModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"cell\": {},\n",
            "  \"classifier_dropout\": null,\n",
            "  \"emb_size\": 312,\n",
            "  \"finetuning_task\": \"rte\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 312,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 1200,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 4,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"pre_trained\": \"\",\n",
            "  \"structure\": [],\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.40.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2085] 2024-04-11 20:25:11,426 >> loading file vocab.txt\n",
            "[INFO|tokenization_utils_base.py:2085] 2024-04-11 20:25:11,426 >> loading file tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2085] 2024-04-11 20:25:11,426 >> loading file added_tokens.json\n",
            "[INFO|tokenization_utils_base.py:2085] 2024-04-11 20:25:11,426 >> loading file special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2085] 2024-04-11 20:25:11,426 >> loading file tokenizer_config.json\n",
            "[INFO|modeling_utils.py:3426] 2024-04-11 20:25:11,539 >> loading weights file models/medium/model.safetensors\n",
            "[INFO|modeling_utils.py:4170] 2024-04-11 20:25:11,970 >> All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
            "\n",
            "[WARNING|modeling_utils.py:4172] 2024-04-11 20:25:11,971 >> Some weights of BertForSequenceClassification were not initialized from the model checkpoint at models/medium and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Running tokenizer on dataset:   0% 0/2490 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/nyu-mll___glue/rte/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-aab2051ce83745be.arrow\n",
            "04/11/2024 20:25:12 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/nyu-mll___glue/rte/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-aab2051ce83745be.arrow\n",
            "Running tokenizer on dataset: 100% 2490/2490 [00:00<00:00, 2745.81 examples/s]\n",
            "Running tokenizer on dataset:   0% 0/277 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/nyu-mll___glue/rte/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-ac56380d8962e923.arrow\n",
            "04/11/2024 20:25:12 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/nyu-mll___glue/rte/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-ac56380d8962e923.arrow\n",
            "Running tokenizer on dataset: 100% 277/277 [00:00<00:00, 2704.94 examples/s]\n",
            "Running tokenizer on dataset:   0% 0/3000 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/nyu-mll___glue/rte/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-4b62a8355fccb352.arrow\n",
            "04/11/2024 20:25:13 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/nyu-mll___glue/rte/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-4b62a8355fccb352.arrow\n",
            "Running tokenizer on dataset: 100% 3000/3000 [00:01<00:00, 2075.19 examples/s]\n",
            "04/11/2024 20:25:14 - INFO - __main__ - Sample 456 of the training set: {'sentence1': \"A computer system failure closed down share trading at the Tokyo Stock Exchange for most of yesterday, the worst disruption to date for Asia's largest bourse.\", 'sentence2': 'The Tokyo Stock Exchange was closed down by computer system failure.', 'label': 0, 'idx': 456, 'input_ids': [101, 1037, 3274, 2291, 4945, 2701, 2091, 3745, 6202, 2012, 1996, 5522, 4518, 3863, 2005, 2087, 1997, 7483, 1010, 1996, 5409, 20461, 2000, 3058, 2005, 4021, 1005, 1055, 2922, 8945, 28393, 1012, 102, 1996, 5522, 4518, 3863, 2001, 2701, 2091, 2011, 3274, 2291, 4945, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "04/11/2024 20:25:14 - INFO - __main__ - Sample 102 of the training set: {'sentence1': 'Nagin defended his plan to return up to 180,000 people to the city, within a week and a half, despite concerns about the short supply of drinking water and heavily polluted floodwaters.', 'sentence2': 'Thousands of people are expected to return to New Orleans this week, as areas of the city are opened up to residents.', 'label': 1, 'idx': 102, 'input_ids': [101, 6583, 11528, 8047, 2010, 2933, 2000, 2709, 2039, 2000, 8380, 1010, 2199, 2111, 2000, 1996, 2103, 1010, 2306, 1037, 2733, 1998, 1037, 2431, 1010, 2750, 5936, 2055, 1996, 2460, 4425, 1997, 5948, 2300, 1998, 4600, 8554, 12926, 7186, 5880, 2015, 1012, 102, 5190, 1997, 2111, 2024, 3517, 2000, 2709, 2000, 2047, 5979, 2023, 2733, 1010, 2004, 2752, 1997, 1996, 2103, 2024, 2441, 2039, 2000, 3901, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "04/11/2024 20:25:14 - INFO - __main__ - Sample 1126 of the training set: {'sentence1': \"In 1969, he drew up the report proposing the expulsion from the party of the Manifesto group. In 1984, after Berlinguer's death, Natta was elected as party secretary.\", 'sentence2': 'Natta supported the Manifesto group.', 'label': 1, 'idx': 1126, 'input_ids': [101, 1999, 3440, 1010, 2002, 3881, 2039, 1996, 3189, 21991, 1996, 18272, 2013, 1996, 2283, 1997, 1996, 17124, 2177, 1012, 1999, 3118, 1010, 2044, 4068, 9077, 2099, 1005, 1055, 2331, 1010, 14085, 2696, 2001, 2700, 2004, 2283, 3187, 1012, 102, 14085, 2696, 3569, 1996, 17124, 2177, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "[INFO|trainer.py:785] 2024-04-11 20:25:15,479 >> The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2047] 2024-04-11 20:25:15,487 >> ***** Running training *****\n",
            "[INFO|trainer.py:2048] 2024-04-11 20:25:15,487 >>   Num examples = 2,490\n",
            "[INFO|trainer.py:2049] 2024-04-11 20:25:15,487 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:2050] 2024-04-11 20:25:15,487 >>   Instantaneous batch size per device = 32\n",
            "[INFO|trainer.py:2053] 2024-04-11 20:25:15,487 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "[INFO|trainer.py:2054] 2024-04-11 20:25:15,487 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:2055] 2024-04-11 20:25:15,487 >>   Total optimization steps = 234\n",
            "[INFO|trainer.py:2056] 2024-04-11 20:25:15,488 >>   Number of trainable parameters = 14,350,874\n",
            "100% 234/234 [13:40<00:00,  3.15s/it][INFO|trainer.py:2315] 2024-04-11 20:38:56,050 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 820.5624, 'train_samples_per_second': 9.104, 'train_steps_per_second': 0.285, 'train_loss': 0.6256874280098157, 'epoch': 3.0}\n",
            "100% 234/234 [13:40<00:00,  3.51s/it]\n",
            "[INFO|trainer.py:3304] 2024-04-11 20:38:56,053 >> Saving model checkpoint to test_results_medium_RTE\n",
            "[INFO|configuration_utils.py:471] 2024-04-11 20:38:56,054 >> Configuration saved in test_results_medium_RTE/config.json\n",
            "[INFO|modeling_utils.py:2590] 2024-04-11 20:38:56,141 >> Model weights saved in test_results_medium_RTE/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2488] 2024-04-11 20:38:56,142 >> tokenizer config file saved in test_results_medium_RTE/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2497] 2024-04-11 20:38:56,142 >> Special tokens file saved in test_results_medium_RTE/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        3.0\n",
            "  train_loss               =     0.6257\n",
            "  train_runtime            = 0:13:40.56\n",
            "  train_samples            =       2490\n",
            "  train_samples_per_second =      9.104\n",
            "  train_steps_per_second   =      0.285\n",
            "04/11/2024 20:38:56 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:785] 2024-04-11 20:38:56,164 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3613] 2024-04-11 20:38:56,167 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3615] 2024-04-11 20:38:56,167 >>   Num examples = 277\n",
            "[INFO|trainer.py:3618] 2024-04-11 20:38:56,167 >>   Batch size = 8\n",
            "100% 35/35 [00:09<00:00,  3.68it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        3.0\n",
            "  eval_accuracy           =     0.6679\n",
            "  eval_loss               =     0.6325\n",
            "  eval_runtime            = 0:00:09.76\n",
            "  eval_samples            =        277\n",
            "  eval_samples_per_second =     28.381\n",
            "  eval_steps_per_second   =      3.586\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "from transformers import BertModel, BertConfig, BertTokenizer\n",
        "\n",
        "model = torch.load('models/int_expanded.mod', map_location=device)\n",
        "model.save_pretrained(\"models/int_expanded_v2\")\n",
        "model_name = \"huawei-noah/TinyBERT_General_4L_312D\"\n",
        "config = BertConfig.from_pretrained(model_name)#, output_hidden_states=True, output_attentions=True)\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "# model = BertModel.from_pretrained('models/int_expanded_v2', config=config)\n",
        "tokenizer.save_pretrained(\"models/int_expanded_v2\")\n",
        "config.save_pretrained(\"models/int_expanded_v2\")\n",
        "# model.save_pretrained(\"models/int_expanded_v2\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w_G29hngRcNa",
        "outputId": "3b33fa11-9cac-4397-e827-e3a4de64544c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Code for running benchmark\n",
        "!mkdir test_results_int_expanded_RTE\n",
        "!python run_glue.py --model_name_or_path models/int_expanded_v2 --task_name RTE --do_train --do_eval --max_seq_length 128 --per_device_train_batch_size 32 --learning_rate 2e-5 --num_train_epochs 3 --output_dir test_results_int_expanded_RTE"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5L1Bj-0TRcUt",
        "outputId": "dae3d30f-c4de-4941-e746-5ac4b9f17461"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-04-11 18:47:54.092715: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-04-11 18:47:54.092835: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-04-11 18:47:54.095473: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-04-11 18:47:57.772224: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "04/11/2024 18:48:01 - WARNING - __main__ - Process rank: 0, device: cpu, n_gpu: 0, distributed training: False, 16-bits training: False\n",
            "04/11/2024 18:48:01 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=0,\n",
            "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'gradient_accumulation_kwargs': None},\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_persistent_workers=False,\n",
            "dataloader_pin_memory=True,\n",
            "dataloader_prefetch_factor=None,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_num_input_tokens_seen=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=2e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=test_results_int_expanded_RTE/runs/Apr11_18-48-01_79b69d194d2b,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "optim_target_modules=None,\n",
            "output_dir=test_results_int_expanded_RTE,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=32,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=test_results_int_expanded_RTE,\n",
            "save_on_each_node=False,\n",
            "save_only_model=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "split_batches=None,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "Overwrite dataset info from restored data version if exists.\n",
            "04/11/2024 18:48:03 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/nyu-mll___glue/rte/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c\n",
            "04/11/2024 18:48:03 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/nyu-mll___glue/rte/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c\n",
            "Found cached dataset glue (/root/.cache/huggingface/datasets/nyu-mll___glue/rte/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)\n",
            "04/11/2024 18:48:03 - INFO - datasets.builder - Found cached dataset glue (/root/.cache/huggingface/datasets/nyu-mll___glue/rte/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/nyu-mll___glue/rte/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c\n",
            "04/11/2024 18:48:03 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/nyu-mll___glue/rte/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c\n",
            "[INFO|configuration_utils.py:724] 2024-04-11 18:48:03,649 >> loading configuration file models/int_expanded_v2/config.json\n",
            "[INFO|configuration_utils.py:789] 2024-04-11 18:48:03,655 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"models/int_expanded_v2\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"cell\": {},\n",
            "  \"classifier_dropout\": null,\n",
            "  \"emb_size\": 312,\n",
            "  \"finetuning_task\": \"rte\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 312,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 1200,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 4,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"pre_trained\": \"\",\n",
            "  \"structure\": [],\n",
            "  \"transformers_version\": \"4.40.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2085] 2024-04-11 18:48:03,656 >> loading file vocab.txt\n",
            "[INFO|tokenization_utils_base.py:2085] 2024-04-11 18:48:03,656 >> loading file tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2085] 2024-04-11 18:48:03,656 >> loading file added_tokens.json\n",
            "[INFO|tokenization_utils_base.py:2085] 2024-04-11 18:48:03,656 >> loading file special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2085] 2024-04-11 18:48:03,656 >> loading file tokenizer_config.json\n",
            "[INFO|modeling_utils.py:3426] 2024-04-11 18:48:03,769 >> loading weights file models/int_expanded_v2/model.safetensors\n",
            "[INFO|modeling_utils.py:4170] 2024-04-11 18:48:03,837 >> All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
            "\n",
            "[WARNING|modeling_utils.py:4172] 2024-04-11 18:48:03,837 >> Some weights of BertForSequenceClassification were not initialized from the model checkpoint at models/int_expanded_v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Running tokenizer on dataset:   0% 0/2490 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/nyu-mll___glue/rte/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-2c29b1f40872fb49.arrow\n",
            "04/11/2024 18:48:04 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/nyu-mll___glue/rte/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-2c29b1f40872fb49.arrow\n",
            "Running tokenizer on dataset: 100% 2490/2490 [00:00<00:00, 2821.27 examples/s]\n",
            "Running tokenizer on dataset:   0% 0/277 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/nyu-mll___glue/rte/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-40b68e2ed9c36c6d.arrow\n",
            "04/11/2024 18:48:04 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/nyu-mll___glue/rte/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-40b68e2ed9c36c6d.arrow\n",
            "Running tokenizer on dataset: 100% 277/277 [00:00<00:00, 2721.89 examples/s]\n",
            "Running tokenizer on dataset:   0% 0/3000 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/nyu-mll___glue/rte/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-4ac210523ec2dac6.arrow\n",
            "04/11/2024 18:48:05 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/nyu-mll___glue/rte/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-4ac210523ec2dac6.arrow\n",
            "Running tokenizer on dataset: 100% 3000/3000 [00:01<00:00, 2188.00 examples/s]\n",
            "04/11/2024 18:48:06 - INFO - __main__ - Sample 456 of the training set: {'sentence1': \"A computer system failure closed down share trading at the Tokyo Stock Exchange for most of yesterday, the worst disruption to date for Asia's largest bourse.\", 'sentence2': 'The Tokyo Stock Exchange was closed down by computer system failure.', 'label': 0, 'idx': 456, 'input_ids': [101, 1037, 3274, 2291, 4945, 2701, 2091, 3745, 6202, 2012, 1996, 5522, 4518, 3863, 2005, 2087, 1997, 7483, 1010, 1996, 5409, 20461, 2000, 3058, 2005, 4021, 1005, 1055, 2922, 8945, 28393, 1012, 102, 1996, 5522, 4518, 3863, 2001, 2701, 2091, 2011, 3274, 2291, 4945, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "04/11/2024 18:48:06 - INFO - __main__ - Sample 102 of the training set: {'sentence1': 'Nagin defended his plan to return up to 180,000 people to the city, within a week and a half, despite concerns about the short supply of drinking water and heavily polluted floodwaters.', 'sentence2': 'Thousands of people are expected to return to New Orleans this week, as areas of the city are opened up to residents.', 'label': 1, 'idx': 102, 'input_ids': [101, 6583, 11528, 8047, 2010, 2933, 2000, 2709, 2039, 2000, 8380, 1010, 2199, 2111, 2000, 1996, 2103, 1010, 2306, 1037, 2733, 1998, 1037, 2431, 1010, 2750, 5936, 2055, 1996, 2460, 4425, 1997, 5948, 2300, 1998, 4600, 8554, 12926, 7186, 5880, 2015, 1012, 102, 5190, 1997, 2111, 2024, 3517, 2000, 2709, 2000, 2047, 5979, 2023, 2733, 1010, 2004, 2752, 1997, 1996, 2103, 2024, 2441, 2039, 2000, 3901, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "04/11/2024 18:48:06 - INFO - __main__ - Sample 1126 of the training set: {'sentence1': \"In 1969, he drew up the report proposing the expulsion from the party of the Manifesto group. In 1984, after Berlinguer's death, Natta was elected as party secretary.\", 'sentence2': 'Natta supported the Manifesto group.', 'label': 1, 'idx': 1126, 'input_ids': [101, 1999, 3440, 1010, 2002, 3881, 2039, 1996, 3189, 21991, 1996, 18272, 2013, 1996, 2283, 1997, 1996, 17124, 2177, 1012, 1999, 3118, 1010, 2044, 4068, 9077, 2099, 1005, 1055, 2331, 1010, 14085, 2696, 2001, 2700, 2004, 2283, 3187, 1012, 102, 14085, 2696, 3569, 1996, 17124, 2177, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "[INFO|trainer.py:785] 2024-04-11 18:48:07,184 >> The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2047] 2024-04-11 18:48:07,192 >> ***** Running training *****\n",
            "[INFO|trainer.py:2048] 2024-04-11 18:48:07,192 >>   Num examples = 2,490\n",
            "[INFO|trainer.py:2049] 2024-04-11 18:48:07,192 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:2050] 2024-04-11 18:48:07,192 >>   Instantaneous batch size per device = 32\n",
            "[INFO|trainer.py:2053] 2024-04-11 18:48:07,192 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "[INFO|trainer.py:2054] 2024-04-11 18:48:07,192 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:2055] 2024-04-11 18:48:07,192 >>   Total optimization steps = 234\n",
            "[INFO|trainer.py:2056] 2024-04-11 18:48:07,193 >>   Number of trainable parameters = 14,350,874\n",
            "100% 234/234 [15:58<00:00,  3.32s/it][INFO|trainer.py:2315] 2024-04-11 19:04:05,470 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 958.2771, 'train_samples_per_second': 7.795, 'train_steps_per_second': 0.244, 'train_loss': 0.6428497184036125, 'epoch': 3.0}\n",
            "100% 234/234 [15:58<00:00,  4.10s/it]\n",
            "[INFO|trainer.py:3304] 2024-04-11 19:04:05,473 >> Saving model checkpoint to test_results_int_expanded_RTE\n",
            "[INFO|configuration_utils.py:471] 2024-04-11 19:04:05,474 >> Configuration saved in test_results_int_expanded_RTE/config.json\n",
            "[INFO|modeling_utils.py:2590] 2024-04-11 19:04:05,561 >> Model weights saved in test_results_int_expanded_RTE/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2488] 2024-04-11 19:04:05,563 >> tokenizer config file saved in test_results_int_expanded_RTE/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2497] 2024-04-11 19:04:05,563 >> Special tokens file saved in test_results_int_expanded_RTE/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        3.0\n",
            "  train_loss               =     0.6428\n",
            "  train_runtime            = 0:15:58.27\n",
            "  train_samples            =       2490\n",
            "  train_samples_per_second =      7.795\n",
            "  train_steps_per_second   =      0.244\n",
            "04/11/2024 19:04:05 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:785] 2024-04-11 19:04:05,585 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3613] 2024-04-11 19:04:05,588 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3615] 2024-04-11 19:04:05,588 >>   Num examples = 277\n",
            "[INFO|trainer.py:3618] 2024-04-11 19:04:05,588 >>   Batch size = 8\n",
            "100% 35/35 [00:09<00:00,  3.61it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        3.0\n",
            "  eval_accuracy           =     0.6462\n",
            "  eval_loss               =      0.633\n",
            "  eval_runtime            = 0:00:09.96\n",
            "  eval_samples            =        277\n",
            "  eval_samples_per_second =     27.789\n",
            "  eval_steps_per_second   =      3.511\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**WNLI Benchmark**"
      ],
      "metadata": {
        "id": "6xI-L8l3hU-H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "from transformers import BertModel, BertConfig, BertTokenizer\n",
        "\n",
        "model_name = \"huawei-noah/TinyBERT_General_4L_312D\"\n",
        "config = BertConfig.from_pretrained(model_name)#, output_hidden_states=True, output_attentions=True)\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "model = BertModel.from_pretrained(model_name, config=config)\n",
        "tokenizer.save_pretrained(\"models/basev2\")\n",
        "config.save_pretrained(\"models/basev2\")\n",
        "model.save_pretrained(\"models/basev2\")"
      ],
      "metadata": {
        "id": "g9kdC-3KjTRz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Code for running benchmark\n",
        "!mkdir test_results_tinyBERT_WNLI\n",
        "!python run_glue.py --model_name_or_path models/basev2 --task_name WNLI --do_train --do_eval --max_seq_length 128 --per_device_train_batch_size 32 --learning_rate 2e-5 --num_train_epochs 3 --output_dir test_results_tinyBERT_WNLI"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1UDCfyBchgZy",
        "outputId": "898d67e4-a674-4cff-ad3e-3046166ede2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘test_results_tinyBERT_WNLI’: File exists\n",
            "2024-04-11 19:05:58.524608: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-04-11 19:05:58.524682: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-04-11 19:05:58.526420: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-04-11 19:06:00.690824: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "04/11/2024 19:06:04 - WARNING - __main__ - Process rank: 0, device: cpu, n_gpu: 0, distributed training: False, 16-bits training: False\n",
            "04/11/2024 19:06:04 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=0,\n",
            "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'gradient_accumulation_kwargs': None},\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_persistent_workers=False,\n",
            "dataloader_pin_memory=True,\n",
            "dataloader_prefetch_factor=None,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_num_input_tokens_seen=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=2e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=test_results_tinyBERT_WNLI/runs/Apr11_19-06-03_79b69d194d2b,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "optim_target_modules=None,\n",
            "output_dir=test_results_tinyBERT_WNLI,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=32,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=test_results_tinyBERT_WNLI,\n",
            "save_on_each_node=False,\n",
            "save_only_model=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "split_batches=None,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "Overwrite dataset info from restored data version if exists.\n",
            "04/11/2024 19:06:05 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/nyu-mll___glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c\n",
            "04/11/2024 19:06:05 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/nyu-mll___glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c\n",
            "Found cached dataset glue (/root/.cache/huggingface/datasets/nyu-mll___glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)\n",
            "04/11/2024 19:06:06 - INFO - datasets.builder - Found cached dataset glue (/root/.cache/huggingface/datasets/nyu-mll___glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/nyu-mll___glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c\n",
            "04/11/2024 19:06:06 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/nyu-mll___glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c\n",
            "[INFO|configuration_utils.py:724] 2024-04-11 19:06:06,032 >> loading configuration file models/basev2/config.json\n",
            "[INFO|configuration_utils.py:789] 2024-04-11 19:06:06,036 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"models/basev2\",\n",
            "  \"architectures\": [\n",
            "    \"BertModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"cell\": {},\n",
            "  \"classifier_dropout\": null,\n",
            "  \"emb_size\": 312,\n",
            "  \"finetuning_task\": \"wnli\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 312,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 1200,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 4,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"pre_trained\": \"\",\n",
            "  \"structure\": [],\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.40.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2085] 2024-04-11 19:06:06,037 >> loading file vocab.txt\n",
            "[INFO|tokenization_utils_base.py:2085] 2024-04-11 19:06:06,037 >> loading file tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2085] 2024-04-11 19:06:06,037 >> loading file added_tokens.json\n",
            "[INFO|tokenization_utils_base.py:2085] 2024-04-11 19:06:06,037 >> loading file special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2085] 2024-04-11 19:06:06,037 >> loading file tokenizer_config.json\n",
            "[INFO|modeling_utils.py:3426] 2024-04-11 19:06:06,179 >> loading weights file models/basev2/model.safetensors\n",
            "[INFO|modeling_utils.py:4170] 2024-04-11 19:06:06,263 >> All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
            "\n",
            "[WARNING|modeling_utils.py:4172] 2024-04-11 19:06:06,263 >> Some weights of BertForSequenceClassification were not initialized from the model checkpoint at models/basev2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Running tokenizer on dataset:   0% 0/635 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/nyu-mll___glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-997c9ac8c60167ba.arrow\n",
            "04/11/2024 19:06:06 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/nyu-mll___glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-997c9ac8c60167ba.arrow\n",
            "Running tokenizer on dataset: 100% 635/635 [00:00<00:00, 3556.03 examples/s]\n",
            "Running tokenizer on dataset:   0% 0/71 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/nyu-mll___glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-473c90c5b6d6276b.arrow\n",
            "04/11/2024 19:06:06 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/nyu-mll___glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-473c90c5b6d6276b.arrow\n",
            "Running tokenizer on dataset: 100% 71/71 [00:00<00:00, 2379.41 examples/s]\n",
            "Running tokenizer on dataset:   0% 0/146 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/nyu-mll___glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-85df8ed7d56d86c4.arrow\n",
            "04/11/2024 19:06:06 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/nyu-mll___glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-85df8ed7d56d86c4.arrow\n",
            "Running tokenizer on dataset: 100% 146/146 [00:00<00:00, 3008.86 examples/s]\n",
            "04/11/2024 19:06:06 - INFO - __main__ - Sample 114 of the training set: {'sentence1': \"In July, Kamtchatka declared war on Yakutsk. Since Yakutsk's army was much better equipped and ten times larger, they were defeated within weeks.\", 'sentence2': 'Yakutsk was defeated within weeks.', 'label': 0, 'idx': 114, 'input_ids': [101, 1999, 2251, 1010, 27829, 10649, 4017, 2912, 4161, 2162, 2006, 8038, 5283, 29064, 1012, 2144, 8038, 5283, 29064, 1005, 1055, 2390, 2001, 2172, 2488, 6055, 1998, 2702, 2335, 3469, 1010, 2027, 2020, 3249, 2306, 3134, 1012, 102, 8038, 5283, 29064, 2001, 3249, 2306, 3134, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "04/11/2024 19:06:06 - INFO - __main__ - Sample 25 of the training set: {'sentence1': \"The trophy doesn't fit into the brown suitcase because it is too large.\", 'sentence2': 'The suitcase is too large.', 'label': 0, 'idx': 25, 'input_ids': [101, 1996, 5384, 2987, 1005, 1056, 4906, 2046, 1996, 2829, 15940, 2138, 2009, 2003, 2205, 2312, 1012, 102, 1996, 15940, 2003, 2205, 2312, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "04/11/2024 19:06:06 - INFO - __main__ - Sample 281 of the training set: {'sentence1': \"Paul tried to call George on the phone, but he wasn't successful.\", 'sentence2': \"George wasn't successful.\", 'label': 0, 'idx': 281, 'input_ids': [101, 2703, 2699, 2000, 2655, 2577, 2006, 1996, 3042, 1010, 2021, 2002, 2347, 1005, 1056, 3144, 1012, 102, 2577, 2347, 1005, 1056, 3144, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "[INFO|trainer.py:785] 2024-04-11 19:06:07,469 >> The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: idx, sentence2, sentence1. If idx, sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2047] 2024-04-11 19:06:07,476 >> ***** Running training *****\n",
            "[INFO|trainer.py:2048] 2024-04-11 19:06:07,476 >>   Num examples = 635\n",
            "[INFO|trainer.py:2049] 2024-04-11 19:06:07,476 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:2050] 2024-04-11 19:06:07,477 >>   Instantaneous batch size per device = 32\n",
            "[INFO|trainer.py:2053] 2024-04-11 19:06:07,477 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "[INFO|trainer.py:2054] 2024-04-11 19:06:07,477 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:2055] 2024-04-11 19:06:07,477 >>   Total optimization steps = 60\n",
            "[INFO|trainer.py:2056] 2024-04-11 19:06:07,477 >>   Number of trainable parameters = 14,350,874\n",
            "100% 60/60 [04:08<00:00,  3.22s/it][INFO|trainer.py:2315] 2024-04-11 19:10:16,271 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 248.7936, 'train_samples_per_second': 7.657, 'train_steps_per_second': 0.241, 'train_loss': 0.6940051396687825, 'epoch': 3.0}\n",
            "100% 60/60 [04:08<00:00,  4.15s/it]\n",
            "[INFO|trainer.py:3304] 2024-04-11 19:10:16,274 >> Saving model checkpoint to test_results_tinyBERT_WNLI\n",
            "[INFO|configuration_utils.py:471] 2024-04-11 19:10:16,275 >> Configuration saved in test_results_tinyBERT_WNLI/config.json\n",
            "[INFO|modeling_utils.py:2590] 2024-04-11 19:10:16,356 >> Model weights saved in test_results_tinyBERT_WNLI/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2488] 2024-04-11 19:10:16,358 >> tokenizer config file saved in test_results_tinyBERT_WNLI/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2497] 2024-04-11 19:10:16,358 >> Special tokens file saved in test_results_tinyBERT_WNLI/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        3.0\n",
            "  train_loss               =      0.694\n",
            "  train_runtime            = 0:04:08.79\n",
            "  train_samples            =        635\n",
            "  train_samples_per_second =      7.657\n",
            "  train_steps_per_second   =      0.241\n",
            "04/11/2024 19:10:16 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:785] 2024-04-11 19:10:16,381 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: idx, sentence2, sentence1. If idx, sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3613] 2024-04-11 19:10:16,384 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3615] 2024-04-11 19:10:16,384 >>   Num examples = 71\n",
            "[INFO|trainer.py:3618] 2024-04-11 19:10:16,384 >>   Batch size = 8\n",
            "100% 9/9 [00:02<00:00,  4.43it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        3.0\n",
            "  eval_accuracy           =     0.5634\n",
            "  eval_loss               =     0.6931\n",
            "  eval_runtime            = 0:00:02.29\n",
            "  eval_samples            =         71\n",
            "  eval_samples_per_second =     30.957\n",
            "  eval_steps_per_second   =      3.924\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Code for running benchmark\n",
        "!mkdir test_results_small_WNLI\n",
        "!python run_glue.py --model_name_or_path models/small --task_name WNLI --do_train --do_eval --max_seq_length 128 --per_device_train_batch_size 32 --learning_rate 2e-5 --num_train_epochs 3 --output_dir test_results_small_WNLI"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j-_P4gYNhxEZ",
        "outputId": "249bf42b-c017-400b-b255-7048d9b57298"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-04-11 19:10:26.666077: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-04-11 19:10:26.666142: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-04-11 19:10:26.668447: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-04-11 19:10:28.139946: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "04/11/2024 19:10:30 - WARNING - __main__ - Process rank: 0, device: cpu, n_gpu: 0, distributed training: False, 16-bits training: False\n",
            "04/11/2024 19:10:30 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=0,\n",
            "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'gradient_accumulation_kwargs': None},\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_persistent_workers=False,\n",
            "dataloader_pin_memory=True,\n",
            "dataloader_prefetch_factor=None,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_num_input_tokens_seen=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=2e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=test_results_small_WNLI/runs/Apr11_19-10-30_79b69d194d2b,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "optim_target_modules=None,\n",
            "output_dir=test_results_small_WNLI,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=32,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=test_results_small_WNLI,\n",
            "save_on_each_node=False,\n",
            "save_only_model=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "split_batches=None,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "Overwrite dataset info from restored data version if exists.\n",
            "04/11/2024 19:10:32 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/nyu-mll___glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c\n",
            "04/11/2024 19:10:32 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/nyu-mll___glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c\n",
            "Found cached dataset glue (/root/.cache/huggingface/datasets/nyu-mll___glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)\n",
            "04/11/2024 19:10:32 - INFO - datasets.builder - Found cached dataset glue (/root/.cache/huggingface/datasets/nyu-mll___glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/nyu-mll___glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c\n",
            "04/11/2024 19:10:32 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/nyu-mll___glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c\n",
            "[INFO|configuration_utils.py:724] 2024-04-11 19:10:32,750 >> loading configuration file models/small/config.json\n",
            "[INFO|configuration_utils.py:789] 2024-04-11 19:10:32,756 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"models/small\",\n",
            "  \"architectures\": [\n",
            "    \"BertModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"cell\": {},\n",
            "  \"classifier_dropout\": null,\n",
            "  \"emb_size\": 312,\n",
            "  \"finetuning_task\": \"wnli\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 312,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 1200,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 4,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"pre_trained\": \"\",\n",
            "  \"structure\": [],\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.40.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2085] 2024-04-11 19:10:32,757 >> loading file vocab.txt\n",
            "[INFO|tokenization_utils_base.py:2085] 2024-04-11 19:10:32,757 >> loading file tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2085] 2024-04-11 19:10:32,757 >> loading file added_tokens.json\n",
            "[INFO|tokenization_utils_base.py:2085] 2024-04-11 19:10:32,757 >> loading file special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2085] 2024-04-11 19:10:32,757 >> loading file tokenizer_config.json\n",
            "[INFO|modeling_utils.py:3426] 2024-04-11 19:10:32,883 >> loading weights file models/small/model.safetensors\n",
            "[INFO|modeling_utils.py:4170] 2024-04-11 19:10:33,293 >> All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
            "\n",
            "[WARNING|modeling_utils.py:4172] 2024-04-11 19:10:33,294 >> Some weights of BertForSequenceClassification were not initialized from the model checkpoint at models/small and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Running tokenizer on dataset:   0% 0/635 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/nyu-mll___glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-689eeec2cc33f29d.arrow\n",
            "04/11/2024 19:10:33 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/nyu-mll___glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-689eeec2cc33f29d.arrow\n",
            "Running tokenizer on dataset: 100% 635/635 [00:00<00:00, 3752.53 examples/s]\n",
            "Running tokenizer on dataset:   0% 0/71 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/nyu-mll___glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-9b02f4b99996840f.arrow\n",
            "04/11/2024 19:10:33 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/nyu-mll___glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-9b02f4b99996840f.arrow\n",
            "Running tokenizer on dataset: 100% 71/71 [00:00<00:00, 2105.87 examples/s]\n",
            "Running tokenizer on dataset:   0% 0/146 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/nyu-mll___glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-5294dffdc25d1067.arrow\n",
            "04/11/2024 19:10:33 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/nyu-mll___glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-5294dffdc25d1067.arrow\n",
            "Running tokenizer on dataset: 100% 146/146 [00:00<00:00, 2703.82 examples/s]\n",
            "04/11/2024 19:10:33 - INFO - __main__ - Sample 114 of the training set: {'sentence1': \"In July, Kamtchatka declared war on Yakutsk. Since Yakutsk's army was much better equipped and ten times larger, they were defeated within weeks.\", 'sentence2': 'Yakutsk was defeated within weeks.', 'label': 0, 'idx': 114, 'input_ids': [101, 1999, 2251, 1010, 27829, 10649, 4017, 2912, 4161, 2162, 2006, 8038, 5283, 29064, 1012, 2144, 8038, 5283, 29064, 1005, 1055, 2390, 2001, 2172, 2488, 6055, 1998, 2702, 2335, 3469, 1010, 2027, 2020, 3249, 2306, 3134, 1012, 102, 8038, 5283, 29064, 2001, 3249, 2306, 3134, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "04/11/2024 19:10:33 - INFO - __main__ - Sample 25 of the training set: {'sentence1': \"The trophy doesn't fit into the brown suitcase because it is too large.\", 'sentence2': 'The suitcase is too large.', 'label': 0, 'idx': 25, 'input_ids': [101, 1996, 5384, 2987, 1005, 1056, 4906, 2046, 1996, 2829, 15940, 2138, 2009, 2003, 2205, 2312, 1012, 102, 1996, 15940, 2003, 2205, 2312, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "04/11/2024 19:10:33 - INFO - __main__ - Sample 281 of the training set: {'sentence1': \"Paul tried to call George on the phone, but he wasn't successful.\", 'sentence2': \"George wasn't successful.\", 'label': 0, 'idx': 281, 'input_ids': [101, 2703, 2699, 2000, 2655, 2577, 2006, 1996, 3042, 1010, 2021, 2002, 2347, 1005, 1056, 3144, 1012, 102, 2577, 2347, 1005, 1056, 3144, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "[INFO|trainer.py:785] 2024-04-11 19:10:35,129 >> The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: idx, sentence2, sentence1. If idx, sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2047] 2024-04-11 19:10:35,136 >> ***** Running training *****\n",
            "[INFO|trainer.py:2048] 2024-04-11 19:10:35,137 >>   Num examples = 635\n",
            "[INFO|trainer.py:2049] 2024-04-11 19:10:35,137 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:2050] 2024-04-11 19:10:35,137 >>   Instantaneous batch size per device = 32\n",
            "[INFO|trainer.py:2053] 2024-04-11 19:10:35,137 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "[INFO|trainer.py:2054] 2024-04-11 19:10:35,137 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:2055] 2024-04-11 19:10:35,137 >>   Total optimization steps = 60\n",
            "[INFO|trainer.py:2056] 2024-04-11 19:10:35,137 >>   Number of trainable parameters = 14,350,874\n",
            "100% 60/60 [03:18<00:00,  3.19s/it][INFO|trainer.py:2315] 2024-04-11 19:13:53,428 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 198.2915, 'train_samples_per_second': 9.607, 'train_steps_per_second': 0.303, 'train_loss': 0.6939754486083984, 'epoch': 3.0}\n",
            "100% 60/60 [03:18<00:00,  3.30s/it]\n",
            "[INFO|trainer.py:3304] 2024-04-11 19:13:53,431 >> Saving model checkpoint to test_results_small_WNLI\n",
            "[INFO|configuration_utils.py:471] 2024-04-11 19:13:53,433 >> Configuration saved in test_results_small_WNLI/config.json\n",
            "[INFO|modeling_utils.py:2590] 2024-04-11 19:13:53,504 >> Model weights saved in test_results_small_WNLI/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2488] 2024-04-11 19:13:53,506 >> tokenizer config file saved in test_results_small_WNLI/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2497] 2024-04-11 19:13:53,506 >> Special tokens file saved in test_results_small_WNLI/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        3.0\n",
            "  train_loss               =      0.694\n",
            "  train_runtime            = 0:03:18.29\n",
            "  train_samples            =        635\n",
            "  train_samples_per_second =      9.607\n",
            "  train_steps_per_second   =      0.303\n",
            "04/11/2024 19:13:53 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:785] 2024-04-11 19:13:53,531 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: idx, sentence2, sentence1. If idx, sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3613] 2024-04-11 19:13:53,534 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3615] 2024-04-11 19:13:53,534 >>   Num examples = 71\n",
            "[INFO|trainer.py:3618] 2024-04-11 19:13:53,534 >>   Batch size = 8\n",
            "100% 9/9 [00:01<00:00,  4.55it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        3.0\n",
            "  eval_accuracy           =     0.3521\n",
            "  eval_loss               =     0.6938\n",
            "  eval_runtime            = 0:00:02.20\n",
            "  eval_samples            =         71\n",
            "  eval_samples_per_second =     32.173\n",
            "  eval_steps_per_second   =      4.078\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Code for running benchmark\n",
        "# !mkdir test_results_medium_WNLI\n",
        "!python run_glue.py --model_name_or_path models/medium --task_name WNLI --do_train --do_eval --max_seq_length 128 --per_device_train_batch_size 32 --learning_rate 2e-5 --num_train_epochs 3 --output_dir test_results_medium_WNLI"
      ],
      "metadata": {
        "id": "z-HI4JHEhxG6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e344f14-db20-4499-ad5d-32bc9e4729a1"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-04-11 20:41:27.216557: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-04-11 20:41:27.216649: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-04-11 20:41:27.219138: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-04-11 20:41:30.539317: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "04/11/2024 20:41:33 - WARNING - __main__ - Process rank: 0, device: cpu, n_gpu: 0, distributed training: False, 16-bits training: False\n",
            "04/11/2024 20:41:33 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=0,\n",
            "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'gradient_accumulation_kwargs': None},\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_persistent_workers=False,\n",
            "dataloader_pin_memory=True,\n",
            "dataloader_prefetch_factor=None,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_num_input_tokens_seen=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=2e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=test_results_medium_WNLI/runs/Apr11_20-41-33_79b69d194d2b,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "optim_target_modules=None,\n",
            "output_dir=test_results_medium_WNLI,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=32,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=test_results_medium_WNLI,\n",
            "save_on_each_node=False,\n",
            "save_only_model=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "split_batches=None,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "Overwrite dataset info from restored data version if exists.\n",
            "04/11/2024 20:41:35 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/nyu-mll___glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c\n",
            "04/11/2024 20:41:35 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/nyu-mll___glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c\n",
            "Found cached dataset glue (/root/.cache/huggingface/datasets/nyu-mll___glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)\n",
            "04/11/2024 20:41:35 - INFO - datasets.builder - Found cached dataset glue (/root/.cache/huggingface/datasets/nyu-mll___glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/nyu-mll___glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c\n",
            "04/11/2024 20:41:35 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/nyu-mll___glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c\n",
            "[INFO|configuration_utils.py:724] 2024-04-11 20:41:35,920 >> loading configuration file models/medium/config.json\n",
            "[INFO|configuration_utils.py:789] 2024-04-11 20:41:35,937 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"models/medium\",\n",
            "  \"architectures\": [\n",
            "    \"BertModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"cell\": {},\n",
            "  \"classifier_dropout\": null,\n",
            "  \"emb_size\": 312,\n",
            "  \"finetuning_task\": \"wnli\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 312,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 1200,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 4,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"pre_trained\": \"\",\n",
            "  \"structure\": [],\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.40.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2085] 2024-04-11 20:41:35,938 >> loading file vocab.txt\n",
            "[INFO|tokenization_utils_base.py:2085] 2024-04-11 20:41:35,938 >> loading file tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2085] 2024-04-11 20:41:35,939 >> loading file added_tokens.json\n",
            "[INFO|tokenization_utils_base.py:2085] 2024-04-11 20:41:35,939 >> loading file special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2085] 2024-04-11 20:41:35,939 >> loading file tokenizer_config.json\n",
            "[INFO|modeling_utils.py:3426] 2024-04-11 20:41:36,141 >> loading weights file models/medium/model.safetensors\n",
            "[INFO|modeling_utils.py:4170] 2024-04-11 20:41:36,299 >> All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
            "\n",
            "[WARNING|modeling_utils.py:4172] 2024-04-11 20:41:36,299 >> Some weights of BertForSequenceClassification were not initialized from the model checkpoint at models/medium and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Running tokenizer on dataset:   0% 0/635 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/nyu-mll___glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-37b5aec99525b2fa.arrow\n",
            "04/11/2024 20:41:36 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/nyu-mll___glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-37b5aec99525b2fa.arrow\n",
            "Running tokenizer on dataset: 100% 635/635 [00:00<00:00, 2514.31 examples/s]\n",
            "Running tokenizer on dataset:   0% 0/71 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/nyu-mll___glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-b5ef43c3cdc78fae.arrow\n",
            "04/11/2024 20:41:36 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/nyu-mll___glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-b5ef43c3cdc78fae.arrow\n",
            "Running tokenizer on dataset: 100% 71/71 [00:00<00:00, 2636.95 examples/s]\n",
            "Running tokenizer on dataset:   0% 0/146 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/nyu-mll___glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-9e051f43fcd9a600.arrow\n",
            "04/11/2024 20:41:36 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/nyu-mll___glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-9e051f43fcd9a600.arrow\n",
            "Running tokenizer on dataset: 100% 146/146 [00:00<00:00, 3082.73 examples/s]\n",
            "04/11/2024 20:41:36 - INFO - __main__ - Sample 114 of the training set: {'sentence1': \"In July, Kamtchatka declared war on Yakutsk. Since Yakutsk's army was much better equipped and ten times larger, they were defeated within weeks.\", 'sentence2': 'Yakutsk was defeated within weeks.', 'label': 0, 'idx': 114, 'input_ids': [101, 1999, 2251, 1010, 27829, 10649, 4017, 2912, 4161, 2162, 2006, 8038, 5283, 29064, 1012, 2144, 8038, 5283, 29064, 1005, 1055, 2390, 2001, 2172, 2488, 6055, 1998, 2702, 2335, 3469, 1010, 2027, 2020, 3249, 2306, 3134, 1012, 102, 8038, 5283, 29064, 2001, 3249, 2306, 3134, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "04/11/2024 20:41:36 - INFO - __main__ - Sample 25 of the training set: {'sentence1': \"The trophy doesn't fit into the brown suitcase because it is too large.\", 'sentence2': 'The suitcase is too large.', 'label': 0, 'idx': 25, 'input_ids': [101, 1996, 5384, 2987, 1005, 1056, 4906, 2046, 1996, 2829, 15940, 2138, 2009, 2003, 2205, 2312, 1012, 102, 1996, 15940, 2003, 2205, 2312, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "04/11/2024 20:41:36 - INFO - __main__ - Sample 281 of the training set: {'sentence1': \"Paul tried to call George on the phone, but he wasn't successful.\", 'sentence2': \"George wasn't successful.\", 'label': 0, 'idx': 281, 'input_ids': [101, 2703, 2699, 2000, 2655, 2577, 2006, 1996, 3042, 1010, 2021, 2002, 2347, 1005, 1056, 3144, 1012, 102, 2577, 2347, 1005, 1056, 3144, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "[INFO|trainer.py:785] 2024-04-11 20:41:37,633 >> The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2047] 2024-04-11 20:41:37,640 >> ***** Running training *****\n",
            "[INFO|trainer.py:2048] 2024-04-11 20:41:37,640 >>   Num examples = 635\n",
            "[INFO|trainer.py:2049] 2024-04-11 20:41:37,640 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:2050] 2024-04-11 20:41:37,640 >>   Instantaneous batch size per device = 32\n",
            "[INFO|trainer.py:2053] 2024-04-11 20:41:37,640 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "[INFO|trainer.py:2054] 2024-04-11 20:41:37,640 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:2055] 2024-04-11 20:41:37,640 >>   Total optimization steps = 60\n",
            "[INFO|trainer.py:2056] 2024-04-11 20:41:37,641 >>   Number of trainable parameters = 14,350,874\n",
            "100% 60/60 [03:16<00:00,  2.93s/it][INFO|trainer.py:2315] 2024-04-11 20:44:54,342 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 196.7011, 'train_samples_per_second': 9.685, 'train_steps_per_second': 0.305, 'train_loss': 0.6939240137736002, 'epoch': 3.0}\n",
            "100% 60/60 [03:16<00:00,  3.28s/it]\n",
            "[INFO|trainer.py:3304] 2024-04-11 20:44:54,344 >> Saving model checkpoint to test_results_medium_WNLI\n",
            "[INFO|configuration_utils.py:471] 2024-04-11 20:44:54,346 >> Configuration saved in test_results_medium_WNLI/config.json\n",
            "[INFO|modeling_utils.py:2590] 2024-04-11 20:44:54,417 >> Model weights saved in test_results_medium_WNLI/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2488] 2024-04-11 20:44:54,418 >> tokenizer config file saved in test_results_medium_WNLI/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2497] 2024-04-11 20:44:54,419 >> Special tokens file saved in test_results_medium_WNLI/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        3.0\n",
            "  train_loss               =     0.6939\n",
            "  train_runtime            = 0:03:16.70\n",
            "  train_samples            =        635\n",
            "  train_samples_per_second =      9.685\n",
            "  train_steps_per_second   =      0.305\n",
            "04/11/2024 20:44:54 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:785] 2024-04-11 20:44:54,437 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3613] 2024-04-11 20:44:54,439 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3615] 2024-04-11 20:44:54,440 >>   Num examples = 71\n",
            "[INFO|trainer.py:3618] 2024-04-11 20:44:54,440 >>   Batch size = 8\n",
            "100% 9/9 [00:01<00:00,  4.93it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        3.0\n",
            "  eval_accuracy           =     0.5634\n",
            "  eval_loss               =     0.6933\n",
            "  eval_runtime            = 0:00:02.06\n",
            "  eval_samples            =         71\n",
            "  eval_samples_per_second =     34.386\n",
            "  eval_steps_per_second   =      4.359\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Code for running benchmark\n",
        "!mkdir test_results_int_expanded_WNLI\n",
        "!python run_glue.py --model_name_or_path models/int_expanded_v2 --task_name WNLI --do_train --do_eval --max_seq_length 128 --per_device_train_batch_size 32 --learning_rate 2e-5 --num_train_epochs 3 --output_dir test_results_int_expanded_WNLI"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QOoIDXHGh_21",
        "outputId": "b62d4a6c-7a2b-49cf-d28c-9ffb2877431e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-04-11 19:14:02.467968: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-04-11 19:14:02.468056: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-04-11 19:14:02.470511: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-04-11 19:14:04.587770: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "04/11/2024 19:14:07 - WARNING - __main__ - Process rank: 0, device: cpu, n_gpu: 0, distributed training: False, 16-bits training: False\n",
            "04/11/2024 19:14:07 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=0,\n",
            "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'gradient_accumulation_kwargs': None},\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_persistent_workers=False,\n",
            "dataloader_pin_memory=True,\n",
            "dataloader_prefetch_factor=None,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_num_input_tokens_seen=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=2e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=test_results_int_expanded_WNLI/runs/Apr11_19-14-07_79b69d194d2b,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "optim_target_modules=None,\n",
            "output_dir=test_results_int_expanded_WNLI,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=32,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=test_results_int_expanded_WNLI,\n",
            "save_on_each_node=False,\n",
            "save_only_model=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "split_batches=None,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "Overwrite dataset info from restored data version if exists.\n",
            "04/11/2024 19:14:09 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/nyu-mll___glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c\n",
            "04/11/2024 19:14:09 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/nyu-mll___glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c\n",
            "Found cached dataset glue (/root/.cache/huggingface/datasets/nyu-mll___glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)\n",
            "04/11/2024 19:14:09 - INFO - datasets.builder - Found cached dataset glue (/root/.cache/huggingface/datasets/nyu-mll___glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/nyu-mll___glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c\n",
            "04/11/2024 19:14:09 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/nyu-mll___glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c\n",
            "[INFO|configuration_utils.py:724] 2024-04-11 19:14:09,582 >> loading configuration file models/int_expanded_v2/config.json\n",
            "[INFO|configuration_utils.py:789] 2024-04-11 19:14:09,586 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"models/int_expanded_v2\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"cell\": {},\n",
            "  \"classifier_dropout\": null,\n",
            "  \"emb_size\": 312,\n",
            "  \"finetuning_task\": \"wnli\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 312,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 1200,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 4,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"pre_trained\": \"\",\n",
            "  \"structure\": [],\n",
            "  \"transformers_version\": \"4.40.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2085] 2024-04-11 19:14:09,587 >> loading file vocab.txt\n",
            "[INFO|tokenization_utils_base.py:2085] 2024-04-11 19:14:09,587 >> loading file tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2085] 2024-04-11 19:14:09,588 >> loading file added_tokens.json\n",
            "[INFO|tokenization_utils_base.py:2085] 2024-04-11 19:14:09,588 >> loading file special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2085] 2024-04-11 19:14:09,588 >> loading file tokenizer_config.json\n",
            "[INFO|modeling_utils.py:3426] 2024-04-11 19:14:09,707 >> loading weights file models/int_expanded_v2/model.safetensors\n",
            "[INFO|modeling_utils.py:4170] 2024-04-11 19:14:09,785 >> All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
            "\n",
            "[WARNING|modeling_utils.py:4172] 2024-04-11 19:14:09,786 >> Some weights of BertForSequenceClassification were not initialized from the model checkpoint at models/int_expanded_v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Running tokenizer on dataset:   0% 0/635 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/nyu-mll___glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-275dec5e4ca63c86.arrow\n",
            "04/11/2024 19:14:09 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/nyu-mll___glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-275dec5e4ca63c86.arrow\n",
            "Running tokenizer on dataset: 100% 635/635 [00:00<00:00, 3895.75 examples/s]\n",
            "Running tokenizer on dataset:   0% 0/71 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/nyu-mll___glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-1bb695c423e234a8.arrow\n",
            "04/11/2024 19:14:10 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/nyu-mll___glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-1bb695c423e234a8.arrow\n",
            "Running tokenizer on dataset: 100% 71/71 [00:00<00:00, 2711.00 examples/s]\n",
            "Running tokenizer on dataset:   0% 0/146 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/nyu-mll___glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-6007d236d57a7b99.arrow\n",
            "04/11/2024 19:14:10 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/nyu-mll___glue/wnli/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-6007d236d57a7b99.arrow\n",
            "Running tokenizer on dataset: 100% 146/146 [00:00<00:00, 2916.97 examples/s]\n",
            "04/11/2024 19:14:10 - INFO - __main__ - Sample 114 of the training set: {'sentence1': \"In July, Kamtchatka declared war on Yakutsk. Since Yakutsk's army was much better equipped and ten times larger, they were defeated within weeks.\", 'sentence2': 'Yakutsk was defeated within weeks.', 'label': 0, 'idx': 114, 'input_ids': [101, 1999, 2251, 1010, 27829, 10649, 4017, 2912, 4161, 2162, 2006, 8038, 5283, 29064, 1012, 2144, 8038, 5283, 29064, 1005, 1055, 2390, 2001, 2172, 2488, 6055, 1998, 2702, 2335, 3469, 1010, 2027, 2020, 3249, 2306, 3134, 1012, 102, 8038, 5283, 29064, 2001, 3249, 2306, 3134, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "04/11/2024 19:14:10 - INFO - __main__ - Sample 25 of the training set: {'sentence1': \"The trophy doesn't fit into the brown suitcase because it is too large.\", 'sentence2': 'The suitcase is too large.', 'label': 0, 'idx': 25, 'input_ids': [101, 1996, 5384, 2987, 1005, 1056, 4906, 2046, 1996, 2829, 15940, 2138, 2009, 2003, 2205, 2312, 1012, 102, 1996, 15940, 2003, 2205, 2312, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "04/11/2024 19:14:10 - INFO - __main__ - Sample 281 of the training set: {'sentence1': \"Paul tried to call George on the phone, but he wasn't successful.\", 'sentence2': \"George wasn't successful.\", 'label': 0, 'idx': 281, 'input_ids': [101, 2703, 2699, 2000, 2655, 2577, 2006, 1996, 3042, 1010, 2021, 2002, 2347, 1005, 1056, 3144, 1012, 102, 2577, 2347, 1005, 1056, 3144, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "[INFO|trainer.py:785] 2024-04-11 19:14:11,003 >> The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: idx, sentence2, sentence1. If idx, sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2047] 2024-04-11 19:14:11,010 >> ***** Running training *****\n",
            "[INFO|trainer.py:2048] 2024-04-11 19:14:11,010 >>   Num examples = 635\n",
            "[INFO|trainer.py:2049] 2024-04-11 19:14:11,010 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:2050] 2024-04-11 19:14:11,010 >>   Instantaneous batch size per device = 32\n",
            "[INFO|trainer.py:2053] 2024-04-11 19:14:11,010 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "[INFO|trainer.py:2054] 2024-04-11 19:14:11,010 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:2055] 2024-04-11 19:14:11,010 >>   Total optimization steps = 60\n",
            "[INFO|trainer.py:2056] 2024-04-11 19:14:11,010 >>   Number of trainable parameters = 14,350,874\n",
            "100% 60/60 [03:21<00:00,  3.34s/it][INFO|trainer.py:2315] 2024-04-11 19:17:32,302 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 201.2922, 'train_samples_per_second': 9.464, 'train_steps_per_second': 0.298, 'train_loss': 0.6939979553222656, 'epoch': 3.0}\n",
            "100% 60/60 [03:21<00:00,  3.35s/it]\n",
            "[INFO|trainer.py:3304] 2024-04-11 19:17:32,306 >> Saving model checkpoint to test_results_int_expanded_WNLI\n",
            "[INFO|configuration_utils.py:471] 2024-04-11 19:17:32,307 >> Configuration saved in test_results_int_expanded_WNLI/config.json\n",
            "[INFO|modeling_utils.py:2590] 2024-04-11 19:17:32,392 >> Model weights saved in test_results_int_expanded_WNLI/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2488] 2024-04-11 19:17:32,393 >> tokenizer config file saved in test_results_int_expanded_WNLI/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2497] 2024-04-11 19:17:32,394 >> Special tokens file saved in test_results_int_expanded_WNLI/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        3.0\n",
            "  train_loss               =      0.694\n",
            "  train_runtime            = 0:03:21.29\n",
            "  train_samples            =        635\n",
            "  train_samples_per_second =      9.464\n",
            "  train_steps_per_second   =      0.298\n",
            "04/11/2024 19:17:32 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:785] 2024-04-11 19:17:32,425 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: idx, sentence2, sentence1. If idx, sentence2, sentence1 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3613] 2024-04-11 19:17:32,429 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3615] 2024-04-11 19:17:32,429 >>   Num examples = 71\n",
            "[INFO|trainer.py:3618] 2024-04-11 19:17:32,429 >>   Batch size = 8\n",
            "100% 9/9 [00:01<00:00,  4.85it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        3.0\n",
            "  eval_accuracy           =     0.4789\n",
            "  eval_loss               =     0.6938\n",
            "  eval_runtime            = 0:00:02.09\n",
            "  eval_samples            =         71\n",
            "  eval_samples_per_second =     33.924\n",
            "  eval_steps_per_second   =        4.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**STS-B Benchmark**"
      ],
      "metadata": {
        "id": "3y9IYMUtjoyA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code for running benchmark\n",
        "# !mkdir test_results_tinyBERT_STS_B\n",
        "!python run_glue.py --model_name_or_path models/basev2 --task_name STSB --do_train --do_eval --max_seq_length 128 --per_device_train_batch_size 32 --learning_rate 2e-5 --num_train_epochs 3 --output_dir test_results_tinyBERT_STS_B"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ueYNpe4fjtw-",
        "outputId": "9fd2a4c1-e06a-4fd9-d606-db395ae8b5fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-04-11 19:43:53.167408: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-04-11 19:43:53.167485: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-04-11 19:43:53.169268: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-04-11 19:43:54.804147: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "04/11/2024 19:43:57 - WARNING - __main__ - Process rank: 0, device: cpu, n_gpu: 0, distributed training: False, 16-bits training: False\n",
            "04/11/2024 19:43:57 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=0,\n",
            "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'gradient_accumulation_kwargs': None},\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_persistent_workers=False,\n",
            "dataloader_pin_memory=True,\n",
            "dataloader_prefetch_factor=None,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_num_input_tokens_seen=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=2e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=test_results_tinyBERT_STS_B/runs/Apr11_19-43-57_79b69d194d2b,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "optim_target_modules=None,\n",
            "output_dir=test_results_tinyBERT_STS_B,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=32,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=test_results_tinyBERT_STS_B,\n",
            "save_on_each_node=False,\n",
            "save_only_model=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "split_batches=None,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "Generating dataset glue (/root/.cache/huggingface/datasets/nyu-mll___glue/stsb/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)\n",
            "04/11/2024 19:43:59 - INFO - datasets.builder - Generating dataset glue (/root/.cache/huggingface/datasets/nyu-mll___glue/stsb/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)\n",
            "Downloading and preparing dataset glue/stsb to /root/.cache/huggingface/datasets/nyu-mll___glue/stsb/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c...\n",
            "04/11/2024 19:43:59 - INFO - datasets.builder - Downloading and preparing dataset glue/stsb to /root/.cache/huggingface/datasets/nyu-mll___glue/stsb/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c...\n",
            "Dataset not on Hf google storage. Downloading and preparing it from source\n",
            "04/11/2024 19:43:59 - INFO - datasets.builder - Dataset not on Hf google storage. Downloading and preparing it from source\n",
            "hf://datasets/nyu-mll/glue@bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/stsb/train-00000-of-00001.parquet not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/61a59748fd35a52c1f4763fb4baeb83e8cce657d5918945c3894209f2d0997a5.incomplete\n",
            "04/11/2024 19:44:00 - INFO - datasets.utils.file_utils - hf://datasets/nyu-mll/glue@bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/stsb/train-00000-of-00001.parquet not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/61a59748fd35a52c1f4763fb4baeb83e8cce657d5918945c3894209f2d0997a5.incomplete\n",
            "Downloading data: 100% 502k/502k [00:00<00:00, 1.83MB/s]\n",
            "storing hf://datasets/nyu-mll/glue@bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/stsb/train-00000-of-00001.parquet in cache at /root/.cache/huggingface/datasets/downloads/61a59748fd35a52c1f4763fb4baeb83e8cce657d5918945c3894209f2d0997a5\n",
            "04/11/2024 19:44:00 - INFO - datasets.utils.file_utils - storing hf://datasets/nyu-mll/glue@bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/stsb/train-00000-of-00001.parquet in cache at /root/.cache/huggingface/datasets/downloads/61a59748fd35a52c1f4763fb4baeb83e8cce657d5918945c3894209f2d0997a5\n",
            "creating metadata file for /root/.cache/huggingface/datasets/downloads/61a59748fd35a52c1f4763fb4baeb83e8cce657d5918945c3894209f2d0997a5\n",
            "04/11/2024 19:44:00 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/61a59748fd35a52c1f4763fb4baeb83e8cce657d5918945c3894209f2d0997a5\n",
            "hf://datasets/nyu-mll/glue@bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/stsb/validation-00000-of-00001.parquet not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/075d720ce3689dc88c30111bb79bf02c6f8ce05f3fab92a4be737ecb048d943e.incomplete\n",
            "04/11/2024 19:44:00 - INFO - datasets.utils.file_utils - hf://datasets/nyu-mll/glue@bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/stsb/validation-00000-of-00001.parquet not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/075d720ce3689dc88c30111bb79bf02c6f8ce05f3fab92a4be737ecb048d943e.incomplete\n",
            "Downloading data: 100% 151k/151k [00:00<00:00, 773kB/s]\n",
            "storing hf://datasets/nyu-mll/glue@bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/stsb/validation-00000-of-00001.parquet in cache at /root/.cache/huggingface/datasets/downloads/075d720ce3689dc88c30111bb79bf02c6f8ce05f3fab92a4be737ecb048d943e\n",
            "04/11/2024 19:44:01 - INFO - datasets.utils.file_utils - storing hf://datasets/nyu-mll/glue@bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/stsb/validation-00000-of-00001.parquet in cache at /root/.cache/huggingface/datasets/downloads/075d720ce3689dc88c30111bb79bf02c6f8ce05f3fab92a4be737ecb048d943e\n",
            "creating metadata file for /root/.cache/huggingface/datasets/downloads/075d720ce3689dc88c30111bb79bf02c6f8ce05f3fab92a4be737ecb048d943e\n",
            "04/11/2024 19:44:01 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/075d720ce3689dc88c30111bb79bf02c6f8ce05f3fab92a4be737ecb048d943e\n",
            "hf://datasets/nyu-mll/glue@bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/stsb/test-00000-of-00001.parquet not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/7c157b26365b80d012cd207dd1b2bceb59293165ccc9bfb3da3572262f9f9de3.incomplete\n",
            "04/11/2024 19:44:01 - INFO - datasets.utils.file_utils - hf://datasets/nyu-mll/glue@bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/stsb/test-00000-of-00001.parquet not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/7c157b26365b80d012cd207dd1b2bceb59293165ccc9bfb3da3572262f9f9de3.incomplete\n",
            "Downloading data: 100% 114k/114k [00:00<00:00, 1.35MB/s]\n",
            "storing hf://datasets/nyu-mll/glue@bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/stsb/test-00000-of-00001.parquet in cache at /root/.cache/huggingface/datasets/downloads/7c157b26365b80d012cd207dd1b2bceb59293165ccc9bfb3da3572262f9f9de3\n",
            "04/11/2024 19:44:01 - INFO - datasets.utils.file_utils - storing hf://datasets/nyu-mll/glue@bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/stsb/test-00000-of-00001.parquet in cache at /root/.cache/huggingface/datasets/downloads/7c157b26365b80d012cd207dd1b2bceb59293165ccc9bfb3da3572262f9f9de3\n",
            "creating metadata file for /root/.cache/huggingface/datasets/downloads/7c157b26365b80d012cd207dd1b2bceb59293165ccc9bfb3da3572262f9f9de3\n",
            "04/11/2024 19:44:01 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/7c157b26365b80d012cd207dd1b2bceb59293165ccc9bfb3da3572262f9f9de3\n",
            "Downloading took 0.0 min\n",
            "04/11/2024 19:44:01 - INFO - datasets.download.download_manager - Downloading took 0.0 min\n",
            "Checksum Computation took 0.0 min\n",
            "04/11/2024 19:44:01 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min\n",
            "Generating train split\n",
            "04/11/2024 19:44:01 - INFO - datasets.builder - Generating train split\n",
            "Generating train split: 100% 5749/5749 [00:00<00:00, 392746.33 examples/s]\n",
            "Generating validation split\n",
            "04/11/2024 19:44:01 - INFO - datasets.builder - Generating validation split\n",
            "Generating validation split: 100% 1500/1500 [00:00<00:00, 374647.53 examples/s]\n",
            "Generating test split\n",
            "04/11/2024 19:44:01 - INFO - datasets.builder - Generating test split\n",
            "Generating test split: 100% 1379/1379 [00:00<00:00, 413848.40 examples/s]\n",
            "All the splits matched successfully.\n",
            "04/11/2024 19:44:01 - INFO - datasets.utils.info_utils - All the splits matched successfully.\n",
            "Dataset glue downloaded and prepared to /root/.cache/huggingface/datasets/nyu-mll___glue/stsb/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c. Subsequent calls will reuse this data.\n",
            "04/11/2024 19:44:01 - INFO - datasets.builder - Dataset glue downloaded and prepared to /root/.cache/huggingface/datasets/nyu-mll___glue/stsb/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c. Subsequent calls will reuse this data.\n",
            "[INFO|configuration_utils.py:724] 2024-04-11 19:44:01,368 >> loading configuration file models/basev2/config.json\n",
            "[INFO|configuration_utils.py:789] 2024-04-11 19:44:01,380 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"models/basev2\",\n",
            "  \"architectures\": [\n",
            "    \"BertModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"cell\": {},\n",
            "  \"classifier_dropout\": null,\n",
            "  \"emb_size\": 312,\n",
            "  \"finetuning_task\": \"stsb\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 312,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 1200,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 4,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"pre_trained\": \"\",\n",
            "  \"structure\": [],\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.40.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2085] 2024-04-11 19:44:01,381 >> loading file vocab.txt\n",
            "[INFO|tokenization_utils_base.py:2085] 2024-04-11 19:44:01,381 >> loading file tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2085] 2024-04-11 19:44:01,381 >> loading file added_tokens.json\n",
            "[INFO|tokenization_utils_base.py:2085] 2024-04-11 19:44:01,381 >> loading file special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2085] 2024-04-11 19:44:01,381 >> loading file tokenizer_config.json\n",
            "[INFO|modeling_utils.py:3426] 2024-04-11 19:44:01,493 >> loading weights file models/basev2/model.safetensors\n",
            "[INFO|modeling_utils.py:4170] 2024-04-11 19:44:01,572 >> All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
            "\n",
            "[WARNING|modeling_utils.py:4172] 2024-04-11 19:44:01,572 >> Some weights of BertForSequenceClassification were not initialized from the model checkpoint at models/basev2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Running tokenizer on dataset:   0% 0/5749 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/nyu-mll___glue/stsb/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-387cc490fee0d04f.arrow\n",
            "04/11/2024 19:44:01 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/nyu-mll___glue/stsb/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-387cc490fee0d04f.arrow\n",
            "Running tokenizer on dataset: 100% 5749/5749 [00:01<00:00, 3015.11 examples/s]\n",
            "Running tokenizer on dataset:   0% 0/1500 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/nyu-mll___glue/stsb/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-1b3d746bac38b9ec.arrow\n",
            "04/11/2024 19:44:03 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/nyu-mll___glue/stsb/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-1b3d746bac38b9ec.arrow\n",
            "Running tokenizer on dataset: 100% 1500/1500 [00:00<00:00, 2738.80 examples/s]\n",
            "Running tokenizer on dataset:   0% 0/1379 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/nyu-mll___glue/stsb/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-577b3855256c68d7.arrow\n",
            "04/11/2024 19:44:04 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/nyu-mll___glue/stsb/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-577b3855256c68d7.arrow\n",
            "Running tokenizer on dataset: 100% 1379/1379 [00:00<00:00, 1566.50 examples/s]\n",
            "04/11/2024 19:44:04 - INFO - __main__ - Sample 5238 of the training set: {'sentence1': 'Didier Reynders on Syria', 'sentence2': \"Obama's day: Prime time on Syria\", 'label': 1.600000023841858, 'idx': 5238, 'input_ids': [101, 2106, 3771, 12569, 11563, 2015, 2006, 7795, 102, 8112, 1005, 1055, 2154, 1024, 3539, 2051, 2006, 7795, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "04/11/2024 19:44:04 - INFO - __main__ - Sample 912 of the training set: {'sentence1': 'A man is cleaning the windows.', 'sentence2': 'A man is driving a car.', 'label': 0.4000000059604645, 'idx': 912, 'input_ids': [101, 1037, 2158, 2003, 9344, 1996, 3645, 1012, 102, 1037, 2158, 2003, 4439, 1037, 2482, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "04/11/2024 19:44:04 - INFO - __main__ - Sample 204 of the training set: {'sentence1': 'A woman holds a kangaroo.', 'sentence2': 'A woman is picking up a kangaroo.', 'label': 3.25, 'idx': 204, 'input_ids': [101, 1037, 2450, 4324, 1037, 21652, 1012, 102, 1037, 2450, 2003, 8130, 2039, 1037, 21652, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "[INFO|trainer.py:785] 2024-04-11 19:44:05,890 >> The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2047] 2024-04-11 19:44:05,897 >> ***** Running training *****\n",
            "[INFO|trainer.py:2048] 2024-04-11 19:44:05,897 >>   Num examples = 5,749\n",
            "[INFO|trainer.py:2049] 2024-04-11 19:44:05,897 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:2050] 2024-04-11 19:44:05,897 >>   Instantaneous batch size per device = 32\n",
            "[INFO|trainer.py:2053] 2024-04-11 19:44:05,897 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "[INFO|trainer.py:2054] 2024-04-11 19:44:05,897 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:2055] 2024-04-11 19:44:05,897 >>   Total optimization steps = 540\n",
            "[INFO|trainer.py:2056] 2024-04-11 19:44:05,898 >>   Number of trainable parameters = 14,350,561\n",
            " 76% 411/540 [22:39<06:53,  3.20s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Code for running benchmark\n",
        "!mkdir test_results_small_STSB\n",
        "!python run_glue.py --model_name_or_path models/small --task_name STSB --do_train --do_eval --max_seq_length 128 --per_device_train_batch_size 32 --learning_rate 2e-5 --num_train_epochs 3 --output_dir test_results_small_STSB"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "huygIIIPj2fq",
        "outputId": "a434c257-6e89-4bd3-9dfa-9eababf55967"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-04-11 20:45:58.128027: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-04-11 20:45:58.128085: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-04-11 20:45:58.130397: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-04-11 20:45:59.710524: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "04/11/2024 20:46:02 - WARNING - __main__ - Process rank: 0, device: cpu, n_gpu: 0, distributed training: False, 16-bits training: False\n",
            "04/11/2024 20:46:02 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=0,\n",
            "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'gradient_accumulation_kwargs': None},\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_persistent_workers=False,\n",
            "dataloader_pin_memory=True,\n",
            "dataloader_prefetch_factor=None,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_num_input_tokens_seen=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=2e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=test_results_small_STSB/runs/Apr11_20-46-02_79b69d194d2b,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "optim_target_modules=None,\n",
            "output_dir=test_results_small_STSB,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=32,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=test_results_small_STSB,\n",
            "save_on_each_node=False,\n",
            "save_only_model=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "split_batches=None,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "Overwrite dataset info from restored data version if exists.\n",
            "04/11/2024 20:46:04 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/nyu-mll___glue/stsb/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c\n",
            "04/11/2024 20:46:04 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/nyu-mll___glue/stsb/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c\n",
            "Found cached dataset glue (/root/.cache/huggingface/datasets/nyu-mll___glue/stsb/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)\n",
            "04/11/2024 20:46:04 - INFO - datasets.builder - Found cached dataset glue (/root/.cache/huggingface/datasets/nyu-mll___glue/stsb/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/nyu-mll___glue/stsb/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c\n",
            "04/11/2024 20:46:04 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/nyu-mll___glue/stsb/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c\n",
            "[INFO|configuration_utils.py:724] 2024-04-11 20:46:04,601 >> loading configuration file models/small/config.json\n",
            "[INFO|configuration_utils.py:789] 2024-04-11 20:46:04,605 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"models/small\",\n",
            "  \"architectures\": [\n",
            "    \"BertModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"cell\": {},\n",
            "  \"classifier_dropout\": null,\n",
            "  \"emb_size\": 312,\n",
            "  \"finetuning_task\": \"stsb\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 312,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 1200,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 4,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"pre_trained\": \"\",\n",
            "  \"structure\": [],\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.40.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2085] 2024-04-11 20:46:04,607 >> loading file vocab.txt\n",
            "[INFO|tokenization_utils_base.py:2085] 2024-04-11 20:46:04,607 >> loading file tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2085] 2024-04-11 20:46:04,607 >> loading file added_tokens.json\n",
            "[INFO|tokenization_utils_base.py:2085] 2024-04-11 20:46:04,607 >> loading file special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2085] 2024-04-11 20:46:04,607 >> loading file tokenizer_config.json\n",
            "[INFO|modeling_utils.py:3426] 2024-04-11 20:46:04,706 >> loading weights file models/small/model.safetensors\n",
            "[INFO|modeling_utils.py:4170] 2024-04-11 20:46:04,804 >> All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
            "\n",
            "[WARNING|modeling_utils.py:4172] 2024-04-11 20:46:04,804 >> Some weights of BertForSequenceClassification were not initialized from the model checkpoint at models/small and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Running tokenizer on dataset:   0% 0/5749 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/nyu-mll___glue/stsb/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-1fbde2ca6d04d750.arrow\n",
            "04/11/2024 20:46:04 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/nyu-mll___glue/stsb/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-1fbde2ca6d04d750.arrow\n",
            "Running tokenizer on dataset: 100% 5749/5749 [00:01<00:00, 5298.20 examples/s]\n",
            "Running tokenizer on dataset:   0% 0/1500 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/nyu-mll___glue/stsb/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-86cda51e07e571a5.arrow\n",
            "04/11/2024 20:46:06 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/nyu-mll___glue/stsb/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-86cda51e07e571a5.arrow\n",
            "Running tokenizer on dataset: 100% 1500/1500 [00:00<00:00, 2413.51 examples/s]\n",
            "Running tokenizer on dataset:   0% 0/1379 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/nyu-mll___glue/stsb/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-bfa6705930f1b557.arrow\n",
            "04/11/2024 20:46:06 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/nyu-mll___glue/stsb/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-bfa6705930f1b557.arrow\n",
            "Running tokenizer on dataset: 100% 1379/1379 [00:00<00:00, 4966.15 examples/s]\n",
            "04/11/2024 20:46:06 - INFO - __main__ - Sample 5238 of the training set: {'sentence1': 'Didier Reynders on Syria', 'sentence2': \"Obama's day: Prime time on Syria\", 'label': 1.600000023841858, 'idx': 5238, 'input_ids': [101, 2106, 3771, 12569, 11563, 2015, 2006, 7795, 102, 8112, 1005, 1055, 2154, 1024, 3539, 2051, 2006, 7795, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "04/11/2024 20:46:06 - INFO - __main__ - Sample 912 of the training set: {'sentence1': 'A man is cleaning the windows.', 'sentence2': 'A man is driving a car.', 'label': 0.4000000059604645, 'idx': 912, 'input_ids': [101, 1037, 2158, 2003, 9344, 1996, 3645, 1012, 102, 1037, 2158, 2003, 4439, 1037, 2482, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "04/11/2024 20:46:06 - INFO - __main__ - Sample 204 of the training set: {'sentence1': 'A woman holds a kangaroo.', 'sentence2': 'A woman is picking up a kangaroo.', 'label': 3.25, 'idx': 204, 'input_ids': [101, 1037, 2450, 4324, 1037, 21652, 1012, 102, 1037, 2450, 2003, 8130, 2039, 1037, 21652, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "[INFO|trainer.py:785] 2024-04-11 20:46:07,707 >> The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2047] 2024-04-11 20:46:07,714 >> ***** Running training *****\n",
            "[INFO|trainer.py:2048] 2024-04-11 20:46:07,714 >>   Num examples = 5,749\n",
            "[INFO|trainer.py:2049] 2024-04-11 20:46:07,714 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:2050] 2024-04-11 20:46:07,714 >>   Instantaneous batch size per device = 32\n",
            "[INFO|trainer.py:2053] 2024-04-11 20:46:07,714 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "[INFO|trainer.py:2054] 2024-04-11 20:46:07,714 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:2055] 2024-04-11 20:46:07,714 >>   Total optimization steps = 540\n",
            "[INFO|trainer.py:2056] 2024-04-11 20:46:07,714 >>   Number of trainable parameters = 14,350,561\n",
            "{'loss': 2.2831, 'grad_norm': 15.564866065979004, 'learning_rate': 1.4814814814814815e-06, 'epoch': 2.78}\n",
            " 93% 500/540 [29:49<02:13,  3.33s/it][INFO|trainer.py:3304] 2024-04-11 21:15:57,614 >> Saving model checkpoint to test_results_small_STSB/checkpoint-500\n",
            "[INFO|configuration_utils.py:471] 2024-04-11 21:15:57,617 >> Configuration saved in test_results_small_STSB/checkpoint-500/config.json\n",
            "[INFO|modeling_utils.py:2590] 2024-04-11 21:15:57,732 >> Model weights saved in test_results_small_STSB/checkpoint-500/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2488] 2024-04-11 21:15:57,734 >> tokenizer config file saved in test_results_small_STSB/checkpoint-500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2497] 2024-04-11 21:15:57,734 >> Special tokens file saved in test_results_small_STSB/checkpoint-500/special_tokens_map.json\n",
            "100% 540/540 [32:03<00:00,  3.05s/it][INFO|trainer.py:2315] 2024-04-11 21:18:10,925 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 1923.211, 'train_samples_per_second': 8.968, 'train_steps_per_second': 0.281, 'train_loss': 2.1797675803855614, 'epoch': 3.0}\n",
            "100% 540/540 [32:03<00:00,  3.56s/it]\n",
            "[INFO|trainer.py:3304] 2024-04-11 21:18:10,929 >> Saving model checkpoint to test_results_small_STSB\n",
            "[INFO|configuration_utils.py:471] 2024-04-11 21:18:10,930 >> Configuration saved in test_results_small_STSB/config.json\n",
            "[INFO|modeling_utils.py:2590] 2024-04-11 21:18:11,011 >> Model weights saved in test_results_small_STSB/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2488] 2024-04-11 21:18:11,013 >> tokenizer config file saved in test_results_small_STSB/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2497] 2024-04-11 21:18:11,013 >> Special tokens file saved in test_results_small_STSB/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        3.0\n",
            "  train_loss               =     2.1798\n",
            "  train_runtime            = 0:32:03.21\n",
            "  train_samples            =       5749\n",
            "  train_samples_per_second =      8.968\n",
            "  train_steps_per_second   =      0.281\n",
            "04/11/2024 21:18:11 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:785] 2024-04-11 21:18:11,035 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3613] 2024-04-11 21:18:11,038 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3615] 2024-04-11 21:18:11,038 >>   Num examples = 1500\n",
            "[INFO|trainer.py:3618] 2024-04-11 21:18:11,038 >>   Batch size = 8\n",
            "100% 188/188 [00:48<00:00,  3.85it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        3.0\n",
            "  eval_combined_score     =      0.807\n",
            "  eval_loss               =     0.7861\n",
            "  eval_pearson            =     0.8073\n",
            "  eval_runtime            = 0:00:49.14\n",
            "  eval_samples            =       1500\n",
            "  eval_samples_per_second =     30.524\n",
            "  eval_spearmanr          =     0.8067\n",
            "  eval_steps_per_second   =      3.826\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Code for running benchmark\n",
        "!mkdir test_results_medium_STSB\n",
        "!python run_glue.py --model_name_or_path models/medium --task_name STSB --do_train --do_eval --max_seq_length 128 --per_device_train_batch_size 32 --learning_rate 2e-5 --num_train_epochs 3 --output_dir test_results_medium_STSB"
      ],
      "metadata": {
        "id": "XtVaThEij8YE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "912a30a7-8342-4feb-c673-40a9fd18d74a"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-04-11 21:19:48.407735: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-04-11 21:19:48.407834: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-04-11 21:19:48.409755: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-04-11 21:19:50.168767: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "04/11/2024 21:19:53 - WARNING - __main__ - Process rank: 0, device: cpu, n_gpu: 0, distributed training: False, 16-bits training: False\n",
            "04/11/2024 21:19:53 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=0,\n",
            "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'gradient_accumulation_kwargs': None},\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_persistent_workers=False,\n",
            "dataloader_pin_memory=True,\n",
            "dataloader_prefetch_factor=None,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_num_input_tokens_seen=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=2e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=test_results_medium_STSB/runs/Apr11_21-19-53_79b69d194d2b,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "optim_target_modules=None,\n",
            "output_dir=test_results_medium_STSB,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=32,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=test_results_medium_STSB,\n",
            "save_on_each_node=False,\n",
            "save_only_model=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "split_batches=None,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "Overwrite dataset info from restored data version if exists.\n",
            "04/11/2024 21:19:55 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/nyu-mll___glue/stsb/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c\n",
            "04/11/2024 21:19:55 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/nyu-mll___glue/stsb/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c\n",
            "Found cached dataset glue (/root/.cache/huggingface/datasets/nyu-mll___glue/stsb/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)\n",
            "04/11/2024 21:19:55 - INFO - datasets.builder - Found cached dataset glue (/root/.cache/huggingface/datasets/nyu-mll___glue/stsb/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/nyu-mll___glue/stsb/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c\n",
            "04/11/2024 21:19:55 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/nyu-mll___glue/stsb/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c\n",
            "[INFO|configuration_utils.py:724] 2024-04-11 21:19:55,377 >> loading configuration file models/medium/config.json\n",
            "[INFO|configuration_utils.py:789] 2024-04-11 21:19:55,381 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"models/medium\",\n",
            "  \"architectures\": [\n",
            "    \"BertModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"cell\": {},\n",
            "  \"classifier_dropout\": null,\n",
            "  \"emb_size\": 312,\n",
            "  \"finetuning_task\": \"stsb\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 312,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 1200,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 4,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"pre_trained\": \"\",\n",
            "  \"structure\": [],\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.40.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2085] 2024-04-11 21:19:55,382 >> loading file vocab.txt\n",
            "[INFO|tokenization_utils_base.py:2085] 2024-04-11 21:19:55,382 >> loading file tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2085] 2024-04-11 21:19:55,382 >> loading file added_tokens.json\n",
            "[INFO|tokenization_utils_base.py:2085] 2024-04-11 21:19:55,382 >> loading file special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2085] 2024-04-11 21:19:55,382 >> loading file tokenizer_config.json\n",
            "[INFO|modeling_utils.py:3426] 2024-04-11 21:19:55,498 >> loading weights file models/medium/model.safetensors\n",
            "[INFO|modeling_utils.py:4170] 2024-04-11 21:19:55,570 >> All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
            "\n",
            "[WARNING|modeling_utils.py:4172] 2024-04-11 21:19:55,570 >> Some weights of BertForSequenceClassification were not initialized from the model checkpoint at models/medium and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Running tokenizer on dataset:   0% 0/5749 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/nyu-mll___glue/stsb/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-975d517bfe689676.arrow\n",
            "04/11/2024 21:19:55 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/nyu-mll___glue/stsb/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-975d517bfe689676.arrow\n",
            "Running tokenizer on dataset: 100% 5749/5749 [00:01<00:00, 5084.08 examples/s]\n",
            "Running tokenizer on dataset:   0% 0/1500 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/nyu-mll___glue/stsb/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-a05aee930de3f509.arrow\n",
            "04/11/2024 21:19:57 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/nyu-mll___glue/stsb/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-a05aee930de3f509.arrow\n",
            "Running tokenizer on dataset: 100% 1500/1500 [00:01<00:00, 1354.97 examples/s]\n",
            "Running tokenizer on dataset:   0% 0/1379 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/nyu-mll___glue/stsb/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-82b83ee1ea09539e.arrow\n",
            "04/11/2024 21:19:58 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/nyu-mll___glue/stsb/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-82b83ee1ea09539e.arrow\n",
            "Running tokenizer on dataset: 100% 1379/1379 [00:00<00:00, 3014.90 examples/s]\n",
            "04/11/2024 21:19:58 - INFO - __main__ - Sample 5238 of the training set: {'sentence1': 'Didier Reynders on Syria', 'sentence2': \"Obama's day: Prime time on Syria\", 'label': 1.600000023841858, 'idx': 5238, 'input_ids': [101, 2106, 3771, 12569, 11563, 2015, 2006, 7795, 102, 8112, 1005, 1055, 2154, 1024, 3539, 2051, 2006, 7795, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "04/11/2024 21:19:58 - INFO - __main__ - Sample 912 of the training set: {'sentence1': 'A man is cleaning the windows.', 'sentence2': 'A man is driving a car.', 'label': 0.4000000059604645, 'idx': 912, 'input_ids': [101, 1037, 2158, 2003, 9344, 1996, 3645, 1012, 102, 1037, 2158, 2003, 4439, 1037, 2482, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "04/11/2024 21:19:58 - INFO - __main__ - Sample 204 of the training set: {'sentence1': 'A woman holds a kangaroo.', 'sentence2': 'A woman is picking up a kangaroo.', 'label': 3.25, 'idx': 204, 'input_ids': [101, 1037, 2450, 4324, 1037, 21652, 1012, 102, 1037, 2450, 2003, 8130, 2039, 1037, 21652, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "[INFO|trainer.py:785] 2024-04-11 21:19:59,570 >> The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2047] 2024-04-11 21:19:59,579 >> ***** Running training *****\n",
            "[INFO|trainer.py:2048] 2024-04-11 21:19:59,579 >>   Num examples = 5,749\n",
            "[INFO|trainer.py:2049] 2024-04-11 21:19:59,579 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:2050] 2024-04-11 21:19:59,579 >>   Instantaneous batch size per device = 32\n",
            "[INFO|trainer.py:2053] 2024-04-11 21:19:59,579 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "[INFO|trainer.py:2054] 2024-04-11 21:19:59,579 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:2055] 2024-04-11 21:19:59,579 >>   Total optimization steps = 540\n",
            "[INFO|trainer.py:2056] 2024-04-11 21:19:59,579 >>   Number of trainable parameters = 14,350,561\n",
            "{'loss': 2.1253, 'grad_norm': 8.895903587341309, 'learning_rate': 1.4814814814814815e-06, 'epoch': 2.78}\n",
            " 93% 500/540 [28:09<02:14,  3.37s/it][INFO|trainer.py:3304] 2024-04-11 21:48:08,963 >> Saving model checkpoint to test_results_medium_STSB/checkpoint-500\n",
            "[INFO|configuration_utils.py:471] 2024-04-11 21:48:08,964 >> Configuration saved in test_results_medium_STSB/checkpoint-500/config.json\n",
            "[INFO|modeling_utils.py:2590] 2024-04-11 21:48:09,052 >> Model weights saved in test_results_medium_STSB/checkpoint-500/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2488] 2024-04-11 21:48:09,053 >> tokenizer config file saved in test_results_medium_STSB/checkpoint-500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2497] 2024-04-11 21:48:09,053 >> Special tokens file saved in test_results_medium_STSB/checkpoint-500/special_tokens_map.json\n",
            "100% 540/540 [30:22<00:00,  2.80s/it][INFO|trainer.py:2315] 2024-04-11 21:50:21,972 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 1822.3924, 'train_samples_per_second': 9.464, 'train_steps_per_second': 0.296, 'train_loss': 2.0255815188090005, 'epoch': 3.0}\n",
            "100% 540/540 [30:22<00:00,  3.37s/it]\n",
            "[INFO|trainer.py:3304] 2024-04-11 21:50:21,975 >> Saving model checkpoint to test_results_medium_STSB\n",
            "[INFO|configuration_utils.py:471] 2024-04-11 21:50:21,976 >> Configuration saved in test_results_medium_STSB/config.json\n",
            "[INFO|modeling_utils.py:2590] 2024-04-11 21:50:22,048 >> Model weights saved in test_results_medium_STSB/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2488] 2024-04-11 21:50:22,050 >> tokenizer config file saved in test_results_medium_STSB/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2497] 2024-04-11 21:50:22,050 >> Special tokens file saved in test_results_medium_STSB/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        3.0\n",
            "  train_loss               =     2.0256\n",
            "  train_runtime            = 0:30:22.39\n",
            "  train_samples            =       5749\n",
            "  train_samples_per_second =      9.464\n",
            "  train_steps_per_second   =      0.296\n",
            "04/11/2024 21:50:22 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:785] 2024-04-11 21:50:22,070 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence2, sentence1, idx. If sentence2, sentence1, idx are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3613] 2024-04-11 21:50:22,073 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3615] 2024-04-11 21:50:22,073 >>   Num examples = 1500\n",
            "[INFO|trainer.py:3618] 2024-04-11 21:50:22,073 >>   Batch size = 8\n",
            "100% 188/188 [00:51<00:00,  3.66it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        3.0\n",
            "  eval_combined_score     =     0.8093\n",
            "  eval_loss               =     0.7933\n",
            "  eval_pearson            =     0.8076\n",
            "  eval_runtime            = 0:00:51.59\n",
            "  eval_samples            =       1500\n",
            "  eval_samples_per_second =     29.075\n",
            "  eval_spearmanr          =      0.811\n",
            "  eval_steps_per_second   =      3.644\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Code for running benchmark\n",
        "!mkdir test_results_int_expanded_STSB\n",
        "!python run_glue.py --model_name_or_path models/int_expanded_v2 --task_name STSB --do_train --do_eval --max_seq_length 128 --per_device_train_batch_size 32 --learning_rate 2e-5 --num_train_epochs 3 --output_dir test_results_int_expanded_STSB"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r1nRq4QqkAlk",
        "outputId": "026b5deb-5f13-48ed-f1e0-c888f9219b99"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-04-11 21:51:21.175197: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-04-11 21:51:21.175265: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-04-11 21:51:21.176913: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-04-11 21:51:22.667272: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "04/11/2024 21:51:25 - WARNING - __main__ - Process rank: 0, device: cpu, n_gpu: 0, distributed training: False, 16-bits training: False\n",
            "04/11/2024 21:51:25 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=0,\n",
            "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'gradient_accumulation_kwargs': None},\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_persistent_workers=False,\n",
            "dataloader_pin_memory=True,\n",
            "dataloader_prefetch_factor=None,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_num_input_tokens_seen=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=2e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=test_results_int_expanded_STSB/runs/Apr11_21-51-25_79b69d194d2b,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "optim_target_modules=None,\n",
            "output_dir=test_results_int_expanded_STSB,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=32,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=test_results_int_expanded_STSB,\n",
            "save_on_each_node=False,\n",
            "save_only_model=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "split_batches=None,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "Overwrite dataset info from restored data version if exists.\n",
            "04/11/2024 21:51:28 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/nyu-mll___glue/stsb/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c\n",
            "04/11/2024 21:51:28 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/nyu-mll___glue/stsb/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c\n",
            "Found cached dataset glue (/root/.cache/huggingface/datasets/nyu-mll___glue/stsb/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)\n",
            "04/11/2024 21:51:28 - INFO - datasets.builder - Found cached dataset glue (/root/.cache/huggingface/datasets/nyu-mll___glue/stsb/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/nyu-mll___glue/stsb/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c\n",
            "04/11/2024 21:51:28 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/nyu-mll___glue/stsb/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c\n",
            "[INFO|configuration_utils.py:724] 2024-04-11 21:51:28,149 >> loading configuration file models/int_expanded_v2/config.json\n",
            "[INFO|configuration_utils.py:789] 2024-04-11 21:51:28,154 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"models/int_expanded_v2\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"cell\": {},\n",
            "  \"classifier_dropout\": null,\n",
            "  \"emb_size\": 312,\n",
            "  \"finetuning_task\": \"stsb\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 312,\n",
            "  \"id2label\": {\n",
            "    \"0\": \"LABEL_0\"\n",
            "  },\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 1200,\n",
            "  \"label2id\": {\n",
            "    \"LABEL_0\": 0\n",
            "  },\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 4,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"pre_trained\": \"\",\n",
            "  \"structure\": [],\n",
            "  \"transformers_version\": \"4.40.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2085] 2024-04-11 21:51:28,157 >> loading file vocab.txt\n",
            "[INFO|tokenization_utils_base.py:2085] 2024-04-11 21:51:28,157 >> loading file tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2085] 2024-04-11 21:51:28,157 >> loading file added_tokens.json\n",
            "[INFO|tokenization_utils_base.py:2085] 2024-04-11 21:51:28,157 >> loading file special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2085] 2024-04-11 21:51:28,157 >> loading file tokenizer_config.json\n",
            "[INFO|modeling_utils.py:3426] 2024-04-11 21:51:28,276 >> loading weights file models/int_expanded_v2/model.safetensors\n",
            "[INFO|modeling_utils.py:4170] 2024-04-11 21:51:28,401 >> All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
            "\n",
            "[WARNING|modeling_utils.py:4172] 2024-04-11 21:51:28,402 >> Some weights of BertForSequenceClassification were not initialized from the model checkpoint at models/int_expanded_v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Running tokenizer on dataset:   0% 0/5749 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/nyu-mll___glue/stsb/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-4626f3cba35f40ad.arrow\n",
            "04/11/2024 21:51:28 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/nyu-mll___glue/stsb/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-4626f3cba35f40ad.arrow\n",
            "Running tokenizer on dataset: 100% 5749/5749 [00:01<00:00, 4912.14 examples/s]\n",
            "Running tokenizer on dataset:   0% 0/1500 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/nyu-mll___glue/stsb/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-0bd6d89e48c94b28.arrow\n",
            "04/11/2024 21:51:30 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/nyu-mll___glue/stsb/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-0bd6d89e48c94b28.arrow\n",
            "Running tokenizer on dataset: 100% 1500/1500 [00:00<00:00, 2366.34 examples/s]\n",
            "Running tokenizer on dataset:   0% 0/1379 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/nyu-mll___glue/stsb/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-55114fe0b3e1b5d9.arrow\n",
            "04/11/2024 21:51:30 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/nyu-mll___glue/stsb/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-55114fe0b3e1b5d9.arrow\n",
            "Running tokenizer on dataset: 100% 1379/1379 [00:00<00:00, 5050.49 examples/s]\n",
            "04/11/2024 21:51:30 - INFO - __main__ - Sample 5238 of the training set: {'sentence1': 'Didier Reynders on Syria', 'sentence2': \"Obama's day: Prime time on Syria\", 'label': 1.600000023841858, 'idx': 5238, 'input_ids': [101, 2106, 3771, 12569, 11563, 2015, 2006, 7795, 102, 8112, 1005, 1055, 2154, 1024, 3539, 2051, 2006, 7795, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "04/11/2024 21:51:30 - INFO - __main__ - Sample 912 of the training set: {'sentence1': 'A man is cleaning the windows.', 'sentence2': 'A man is driving a car.', 'label': 0.4000000059604645, 'idx': 912, 'input_ids': [101, 1037, 2158, 2003, 9344, 1996, 3645, 1012, 102, 1037, 2158, 2003, 4439, 1037, 2482, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "04/11/2024 21:51:30 - INFO - __main__ - Sample 204 of the training set: {'sentence1': 'A woman holds a kangaroo.', 'sentence2': 'A woman is picking up a kangaroo.', 'label': 3.25, 'idx': 204, 'input_ids': [101, 1037, 2450, 4324, 1037, 21652, 1012, 102, 1037, 2450, 2003, 8130, 2039, 1037, 21652, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "[INFO|trainer.py:785] 2024-04-11 21:51:31,459 >> The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2047] 2024-04-11 21:51:31,467 >> ***** Running training *****\n",
            "[INFO|trainer.py:2048] 2024-04-11 21:51:31,467 >>   Num examples = 5,749\n",
            "[INFO|trainer.py:2049] 2024-04-11 21:51:31,467 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:2050] 2024-04-11 21:51:31,467 >>   Instantaneous batch size per device = 32\n",
            "[INFO|trainer.py:2053] 2024-04-11 21:51:31,467 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "[INFO|trainer.py:2054] 2024-04-11 21:51:31,467 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:2055] 2024-04-11 21:51:31,467 >>   Total optimization steps = 540\n",
            "[INFO|trainer.py:2056] 2024-04-11 21:51:31,468 >>   Number of trainable parameters = 14,350,561\n",
            "{'loss': 2.194, 'grad_norm': 10.12935733795166, 'learning_rate': 1.4814814814814815e-06, 'epoch': 2.78}\n",
            " 93% 500/540 [28:33<02:12,  3.30s/it][INFO|trainer.py:3304] 2024-04-11 22:20:04,494 >> Saving model checkpoint to test_results_int_expanded_STSB/checkpoint-500\n",
            "[INFO|configuration_utils.py:471] 2024-04-11 22:20:04,495 >> Configuration saved in test_results_int_expanded_STSB/checkpoint-500/config.json\n",
            "[INFO|modeling_utils.py:2590] 2024-04-11 22:20:04,572 >> Model weights saved in test_results_int_expanded_STSB/checkpoint-500/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2488] 2024-04-11 22:20:04,573 >> tokenizer config file saved in test_results_int_expanded_STSB/checkpoint-500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2497] 2024-04-11 22:20:04,573 >> Special tokens file saved in test_results_int_expanded_STSB/checkpoint-500/special_tokens_map.json\n",
            "100% 540/540 [30:46<00:00,  2.97s/it][INFO|trainer.py:2315] 2024-04-11 22:22:18,381 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 1846.9128, 'train_samples_per_second': 9.338, 'train_steps_per_second': 0.292, 'train_loss': 2.092318273473669, 'epoch': 3.0}\n",
            "100% 540/540 [30:46<00:00,  3.42s/it]\n",
            "[INFO|trainer.py:3304] 2024-04-11 22:22:18,384 >> Saving model checkpoint to test_results_int_expanded_STSB\n",
            "[INFO|configuration_utils.py:471] 2024-04-11 22:22:18,385 >> Configuration saved in test_results_int_expanded_STSB/config.json\n",
            "[INFO|modeling_utils.py:2590] 2024-04-11 22:22:18,493 >> Model weights saved in test_results_int_expanded_STSB/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2488] 2024-04-11 22:22:18,495 >> tokenizer config file saved in test_results_int_expanded_STSB/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2497] 2024-04-11 22:22:18,495 >> Special tokens file saved in test_results_int_expanded_STSB/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        3.0\n",
            "  train_loss               =     2.0923\n",
            "  train_runtime            = 0:30:46.91\n",
            "  train_samples            =       5749\n",
            "  train_samples_per_second =      9.338\n",
            "  train_steps_per_second   =      0.292\n",
            "04/11/2024 22:22:18 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:785] 2024-04-11 22:22:18,518 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3613] 2024-04-11 22:22:18,520 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3615] 2024-04-11 22:22:18,521 >>   Num examples = 1500\n",
            "[INFO|trainer.py:3618] 2024-04-11 22:22:18,521 >>   Batch size = 8\n",
            "100% 188/188 [00:49<00:00,  3.76it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        3.0\n",
            "  eval_combined_score     =     0.8113\n",
            "  eval_loss               =     0.7709\n",
            "  eval_pearson            =     0.8111\n",
            "  eval_runtime            = 0:00:50.21\n",
            "  eval_samples            =       1500\n",
            "  eval_samples_per_second =     29.871\n",
            "  eval_spearmanr          =     0.8115\n",
            "  eval_steps_per_second   =      3.744\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SST2 Evaluation**"
      ],
      "metadata": {
        "id": "blQFOrGs_tXB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code for running benchmark\n",
        "!mkdir test_results_tinyBERT_SST2\n",
        "!python run_glue.py --model_name_or_path models/basev2 --task_name SST2 --do_train --do_eval --max_seq_length 128 --per_device_train_batch_size 32 --learning_rate 2e-5 --num_train_epochs 3 --output_dir test_results_tinyBERT_SST2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YkVO-TDv_xid",
        "outputId": "6352943f-18a1-4d9b-e4e4-69a26c2f66dd"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-04-11 22:23:15.447336: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-04-11 22:23:15.447421: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-04-11 22:23:15.449415: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-04-11 22:23:17.467650: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "04/11/2024 22:23:20 - WARNING - __main__ - Process rank: 0, device: cpu, n_gpu: 0, distributed training: False, 16-bits training: False\n",
            "04/11/2024 22:23:20 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=0,\n",
            "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'gradient_accumulation_kwargs': None},\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_persistent_workers=False,\n",
            "dataloader_pin_memory=True,\n",
            "dataloader_prefetch_factor=None,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_num_input_tokens_seen=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=2e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=test_results_tinyBERT_SST2/runs/Apr11_22-23-20_79b69d194d2b,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "optim_target_modules=None,\n",
            "output_dir=test_results_tinyBERT_SST2,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=32,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=test_results_tinyBERT_SST2,\n",
            "save_on_each_node=False,\n",
            "save_only_model=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "split_batches=None,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "Generating dataset glue (/root/.cache/huggingface/datasets/nyu-mll___glue/sst2/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)\n",
            "04/11/2024 22:23:22 - INFO - datasets.builder - Generating dataset glue (/root/.cache/huggingface/datasets/nyu-mll___glue/sst2/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)\n",
            "Downloading and preparing dataset glue/sst2 to /root/.cache/huggingface/datasets/nyu-mll___glue/sst2/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c...\n",
            "04/11/2024 22:23:22 - INFO - datasets.builder - Downloading and preparing dataset glue/sst2 to /root/.cache/huggingface/datasets/nyu-mll___glue/sst2/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c...\n",
            "Dataset not on Hf google storage. Downloading and preparing it from source\n",
            "04/11/2024 22:23:22 - INFO - datasets.builder - Dataset not on Hf google storage. Downloading and preparing it from source\n",
            "hf://datasets/nyu-mll/glue@bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/sst2/train-00000-of-00001.parquet not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/407d9ad49c5bced2e446311ca160393c530952a3a892ed88e7f067fb20237c9c.incomplete\n",
            "04/11/2024 22:23:23 - INFO - datasets.utils.file_utils - hf://datasets/nyu-mll/glue@bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/sst2/train-00000-of-00001.parquet not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/407d9ad49c5bced2e446311ca160393c530952a3a892ed88e7f067fb20237c9c.incomplete\n",
            "Downloading data: 100% 3.11M/3.11M [00:00<00:00, 11.3MB/s]\n",
            "storing hf://datasets/nyu-mll/glue@bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/sst2/train-00000-of-00001.parquet in cache at /root/.cache/huggingface/datasets/downloads/407d9ad49c5bced2e446311ca160393c530952a3a892ed88e7f067fb20237c9c\n",
            "04/11/2024 22:23:23 - INFO - datasets.utils.file_utils - storing hf://datasets/nyu-mll/glue@bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/sst2/train-00000-of-00001.parquet in cache at /root/.cache/huggingface/datasets/downloads/407d9ad49c5bced2e446311ca160393c530952a3a892ed88e7f067fb20237c9c\n",
            "creating metadata file for /root/.cache/huggingface/datasets/downloads/407d9ad49c5bced2e446311ca160393c530952a3a892ed88e7f067fb20237c9c\n",
            "04/11/2024 22:23:23 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/407d9ad49c5bced2e446311ca160393c530952a3a892ed88e7f067fb20237c9c\n",
            "hf://datasets/nyu-mll/glue@bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/sst2/validation-00000-of-00001.parquet not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/6de9cec90ccbe9fbddc0ef8a8c92494358ec972ec5147c2730f1565d1f373a4b.incomplete\n",
            "04/11/2024 22:23:23 - INFO - datasets.utils.file_utils - hf://datasets/nyu-mll/glue@bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/sst2/validation-00000-of-00001.parquet not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/6de9cec90ccbe9fbddc0ef8a8c92494358ec972ec5147c2730f1565d1f373a4b.incomplete\n",
            "Downloading data: 100% 72.8k/72.8k [00:00<00:00, 1.15MB/s]\n",
            "storing hf://datasets/nyu-mll/glue@bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/sst2/validation-00000-of-00001.parquet in cache at /root/.cache/huggingface/datasets/downloads/6de9cec90ccbe9fbddc0ef8a8c92494358ec972ec5147c2730f1565d1f373a4b\n",
            "04/11/2024 22:23:23 - INFO - datasets.utils.file_utils - storing hf://datasets/nyu-mll/glue@bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/sst2/validation-00000-of-00001.parquet in cache at /root/.cache/huggingface/datasets/downloads/6de9cec90ccbe9fbddc0ef8a8c92494358ec972ec5147c2730f1565d1f373a4b\n",
            "creating metadata file for /root/.cache/huggingface/datasets/downloads/6de9cec90ccbe9fbddc0ef8a8c92494358ec972ec5147c2730f1565d1f373a4b\n",
            "04/11/2024 22:23:23 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/6de9cec90ccbe9fbddc0ef8a8c92494358ec972ec5147c2730f1565d1f373a4b\n",
            "hf://datasets/nyu-mll/glue@bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/sst2/test-00000-of-00001.parquet not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/9c43b3925f9941ee3d2977daa15b8da338f73dd4a84ad4d8100506ec0b532a18.incomplete\n",
            "04/11/2024 22:23:23 - INFO - datasets.utils.file_utils - hf://datasets/nyu-mll/glue@bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/sst2/test-00000-of-00001.parquet not found in cache or force_download set to True, downloading to /root/.cache/huggingface/datasets/downloads/9c43b3925f9941ee3d2977daa15b8da338f73dd4a84ad4d8100506ec0b532a18.incomplete\n",
            "Downloading data: 100% 148k/148k [00:00<00:00, 2.07MB/s]\n",
            "storing hf://datasets/nyu-mll/glue@bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/sst2/test-00000-of-00001.parquet in cache at /root/.cache/huggingface/datasets/downloads/9c43b3925f9941ee3d2977daa15b8da338f73dd4a84ad4d8100506ec0b532a18\n",
            "04/11/2024 22:23:24 - INFO - datasets.utils.file_utils - storing hf://datasets/nyu-mll/glue@bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/sst2/test-00000-of-00001.parquet in cache at /root/.cache/huggingface/datasets/downloads/9c43b3925f9941ee3d2977daa15b8da338f73dd4a84ad4d8100506ec0b532a18\n",
            "creating metadata file for /root/.cache/huggingface/datasets/downloads/9c43b3925f9941ee3d2977daa15b8da338f73dd4a84ad4d8100506ec0b532a18\n",
            "04/11/2024 22:23:24 - INFO - datasets.utils.file_utils - creating metadata file for /root/.cache/huggingface/datasets/downloads/9c43b3925f9941ee3d2977daa15b8da338f73dd4a84ad4d8100506ec0b532a18\n",
            "Downloading took 0.0 min\n",
            "04/11/2024 22:23:24 - INFO - datasets.download.download_manager - Downloading took 0.0 min\n",
            "Checksum Computation took 0.0 min\n",
            "04/11/2024 22:23:24 - INFO - datasets.download.download_manager - Checksum Computation took 0.0 min\n",
            "Generating train split\n",
            "04/11/2024 22:23:24 - INFO - datasets.builder - Generating train split\n",
            "Generating train split: 100% 67349/67349 [00:00<00:00, 1033453.75 examples/s]\n",
            "Generating validation split\n",
            "04/11/2024 22:23:24 - INFO - datasets.builder - Generating validation split\n",
            "Generating validation split: 100% 872/872 [00:00<00:00, 373169.38 examples/s]\n",
            "Generating test split\n",
            "04/11/2024 22:23:24 - INFO - datasets.builder - Generating test split\n",
            "Generating test split: 100% 1821/1821 [00:00<00:00, 212214.93 examples/s]\n",
            "All the splits matched successfully.\n",
            "04/11/2024 22:23:24 - INFO - datasets.utils.info_utils - All the splits matched successfully.\n",
            "Dataset glue downloaded and prepared to /root/.cache/huggingface/datasets/nyu-mll___glue/sst2/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c. Subsequent calls will reuse this data.\n",
            "04/11/2024 22:23:24 - INFO - datasets.builder - Dataset glue downloaded and prepared to /root/.cache/huggingface/datasets/nyu-mll___glue/sst2/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c. Subsequent calls will reuse this data.\n",
            "[INFO|configuration_utils.py:724] 2024-04-11 22:23:24,112 >> loading configuration file models/basev2/config.json\n",
            "[INFO|configuration_utils.py:789] 2024-04-11 22:23:24,116 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"models/basev2\",\n",
            "  \"architectures\": [\n",
            "    \"BertModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"cell\": {},\n",
            "  \"classifier_dropout\": null,\n",
            "  \"emb_size\": 312,\n",
            "  \"finetuning_task\": \"sst2\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 312,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 1200,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 4,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"pre_trained\": \"\",\n",
            "  \"structure\": [],\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.40.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2085] 2024-04-11 22:23:24,117 >> loading file vocab.txt\n",
            "[INFO|tokenization_utils_base.py:2085] 2024-04-11 22:23:24,117 >> loading file tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2085] 2024-04-11 22:23:24,117 >> loading file added_tokens.json\n",
            "[INFO|tokenization_utils_base.py:2085] 2024-04-11 22:23:24,117 >> loading file special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2085] 2024-04-11 22:23:24,117 >> loading file tokenizer_config.json\n",
            "[INFO|modeling_utils.py:3426] 2024-04-11 22:23:24,221 >> loading weights file models/basev2/model.safetensors\n",
            "[INFO|modeling_utils.py:4170] 2024-04-11 22:23:24,705 >> All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
            "\n",
            "[WARNING|modeling_utils.py:4172] 2024-04-11 22:23:24,705 >> Some weights of BertForSequenceClassification were not initialized from the model checkpoint at models/basev2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Running tokenizer on dataset:   0% 0/67349 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/nyu-mll___glue/sst2/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-4c5da224355cc130.arrow\n",
            "04/11/2024 22:23:24 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/nyu-mll___glue/sst2/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-4c5da224355cc130.arrow\n",
            "Running tokenizer on dataset: 100% 67349/67349 [00:11<00:00, 5743.22 examples/s]\n",
            "Running tokenizer on dataset:   0% 0/872 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/nyu-mll___glue/sst2/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-dbc4549a7aab0588.arrow\n",
            "04/11/2024 22:23:36 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/nyu-mll___glue/sst2/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-dbc4549a7aab0588.arrow\n",
            "Running tokenizer on dataset: 100% 872/872 [00:00<00:00, 5714.94 examples/s]\n",
            "Running tokenizer on dataset:   0% 0/1821 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/nyu-mll___glue/sst2/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-c46d7a370ccbeb14.arrow\n",
            "04/11/2024 22:23:36 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/nyu-mll___glue/sst2/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-c46d7a370ccbeb14.arrow\n",
            "Running tokenizer on dataset: 100% 1821/1821 [00:00<00:00, 5729.42 examples/s]\n",
            "04/11/2024 22:23:36 - INFO - __main__ - Sample 14592 of the training set: {'sentence': 'a great movie ', 'label': 1, 'idx': 14592, 'input_ids': [101, 1037, 2307, 3185, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "04/11/2024 22:23:36 - INFO - __main__ - Sample 3278 of the training set: {'sentence': 'entertaining , if somewhat standardized , action ', 'label': 1, 'idx': 3278, 'input_ids': [101, 14036, 1010, 2065, 5399, 16367, 1010, 2895, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "04/11/2024 22:23:36 - INFO - __main__ - Sample 36048 of the training set: {'sentence': 'even when there are lulls , the emotions seem authentic , ', 'label': 1, 'idx': 36048, 'input_ids': [101, 2130, 2043, 2045, 2024, 11320, 12718, 1010, 1996, 6699, 4025, 14469, 1010, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "[INFO|trainer.py:785] 2024-04-11 22:23:37,882 >> The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence, idx. If sentence, idx are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2047] 2024-04-11 22:23:37,890 >> ***** Running training *****\n",
            "[INFO|trainer.py:2048] 2024-04-11 22:23:37,890 >>   Num examples = 67,349\n",
            "[INFO|trainer.py:2049] 2024-04-11 22:23:37,890 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:2050] 2024-04-11 22:23:37,890 >>   Instantaneous batch size per device = 32\n",
            "[INFO|trainer.py:2053] 2024-04-11 22:23:37,890 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "[INFO|trainer.py:2054] 2024-04-11 22:23:37,890 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:2055] 2024-04-11 22:23:37,890 >>   Total optimization steps = 6,315\n",
            "[INFO|trainer.py:2056] 2024-04-11 22:23:37,890 >>   Number of trainable parameters = 14,350,874\n",
            "{'loss': 0.412, 'grad_norm': 12.423234939575195, 'learning_rate': 1.8416468725257326e-05, 'epoch': 0.24}\n",
            "  8% 500/6315 [27:28<5:34:29,  3.45s/it][INFO|trainer.py:3304] 2024-04-11 22:51:06,440 >> Saving model checkpoint to test_results_tinyBERT_SST2/checkpoint-500\n",
            "[INFO|configuration_utils.py:471] 2024-04-11 22:51:06,442 >> Configuration saved in test_results_tinyBERT_SST2/checkpoint-500/config.json\n",
            "[INFO|modeling_utils.py:2590] 2024-04-11 22:51:06,530 >> Model weights saved in test_results_tinyBERT_SST2/checkpoint-500/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2488] 2024-04-11 22:51:06,531 >> tokenizer config file saved in test_results_tinyBERT_SST2/checkpoint-500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2497] 2024-04-11 22:51:06,531 >> Special tokens file saved in test_results_tinyBERT_SST2/checkpoint-500/special_tokens_map.json\n",
            "{'loss': 0.3061, 'grad_norm': 6.774877071380615, 'learning_rate': 1.6832937450514647e-05, 'epoch': 0.48}\n",
            " 16% 1000/6315 [55:25<5:06:50,  3.46s/it][INFO|trainer.py:3304] 2024-04-11 23:19:03,361 >> Saving model checkpoint to test_results_tinyBERT_SST2/checkpoint-1000\n",
            "[INFO|configuration_utils.py:471] 2024-04-11 23:19:03,363 >> Configuration saved in test_results_tinyBERT_SST2/checkpoint-1000/config.json\n",
            "[INFO|modeling_utils.py:2590] 2024-04-11 23:19:03,459 >> Model weights saved in test_results_tinyBERT_SST2/checkpoint-1000/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2488] 2024-04-11 23:19:03,461 >> tokenizer config file saved in test_results_tinyBERT_SST2/checkpoint-1000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2497] 2024-04-11 23:19:03,461 >> Special tokens file saved in test_results_tinyBERT_SST2/checkpoint-1000/special_tokens_map.json\n",
            "{'loss': 0.2724, 'grad_norm': 12.891900062561035, 'learning_rate': 1.5249406175771972e-05, 'epoch': 0.71}\n",
            " 24% 1500/6315 [1:23:30<4:25:30,  3.31s/it][INFO|trainer.py:3304] 2024-04-11 23:47:08,727 >> Saving model checkpoint to test_results_tinyBERT_SST2/checkpoint-1500\n",
            "[INFO|configuration_utils.py:471] 2024-04-11 23:47:08,729 >> Configuration saved in test_results_tinyBERT_SST2/checkpoint-1500/config.json\n",
            "[INFO|modeling_utils.py:2590] 2024-04-11 23:47:08,850 >> Model weights saved in test_results_tinyBERT_SST2/checkpoint-1500/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2488] 2024-04-11 23:47:08,853 >> tokenizer config file saved in test_results_tinyBERT_SST2/checkpoint-1500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2497] 2024-04-11 23:47:08,854 >> Special tokens file saved in test_results_tinyBERT_SST2/checkpoint-1500/special_tokens_map.json\n",
            " 26% 1615/6315 [1:29:53<4:31:56,  3.47s/it]Traceback (most recent call last):\n",
            "  File \"/content/run_glue.py\", line 652, in <module>\n",
            "    main()\n",
            "  File \"/content/run_glue.py\", line 560, in main\n",
            "    train_result = trainer.train(resume_from_checkpoint=checkpoint)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 1858, in train\n",
            "    return inner_training_loop(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 2202, in _inner_training_loop\n",
            "    tr_loss_step = self.training_step(model, inputs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 3146, in training_step\n",
            "    self.accelerator.backward(loss)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py\", line 2013, in backward\n",
            "    loss.backward(**kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\", line 522, in backward\n",
            "    torch.autograd.backward(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\", line 266, in backward\n",
            "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
            "KeyboardInterrupt\n",
            " 26% 1615/6315 [1:29:56<4:21:45,  3.34s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Code for running benchmark\n",
        "!mkdir test_results_small_SST2\n",
        "!python run_glue.py --model_name_or_path models/small --task_name SST2 --do_train --do_eval --max_seq_length 128 --per_device_train_batch_size 32 --learning_rate 2e-5 --num_train_epochs 3 --output_dir test_results_small_SST2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZYSQ4RcD_45h",
        "outputId": "446d7feb-0049-42ae-e707-489ea6d93525"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exception ignored in: <function _xla_gc_callback at 0x7bf91210a0e0>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/jax/_src/lib/__init__.py\", line 98, in _xla_gc_callback\n",
            "    def _xla_gc_callback(*args):\n",
            "KeyboardInterrupt: \n",
            "2024-04-11 23:53:41.300566: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-04-11 23:53:41.300649: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-04-11 23:53:41.303235: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-04-11 23:53:43.334417: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "04/11/2024 23:53:46 - WARNING - __main__ - Process rank: 0, device: cpu, n_gpu: 0, distributed training: False, 16-bits training: False\n",
            "04/11/2024 23:53:46 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=0,\n",
            "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'gradient_accumulation_kwargs': None},\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_persistent_workers=False,\n",
            "dataloader_pin_memory=True,\n",
            "dataloader_prefetch_factor=None,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_num_input_tokens_seen=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=2e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=test_results_small_SST2/runs/Apr11_23-53-46_79b69d194d2b,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "optim_target_modules=None,\n",
            "output_dir=test_results_small_SST2,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=32,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=test_results_small_SST2,\n",
            "save_on_each_node=False,\n",
            "save_only_model=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "split_batches=None,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "Overwrite dataset info from restored data version if exists.\n",
            "04/11/2024 23:53:48 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/nyu-mll___glue/sst2/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c\n",
            "04/11/2024 23:53:48 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/nyu-mll___glue/sst2/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c\n",
            "Found cached dataset glue (/root/.cache/huggingface/datasets/nyu-mll___glue/sst2/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)\n",
            "04/11/2024 23:53:49 - INFO - datasets.builder - Found cached dataset glue (/root/.cache/huggingface/datasets/nyu-mll___glue/sst2/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/nyu-mll___glue/sst2/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c\n",
            "04/11/2024 23:53:49 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/nyu-mll___glue/sst2/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c\n",
            "[INFO|configuration_utils.py:724] 2024-04-11 23:53:49,036 >> loading configuration file models/small/config.json\n",
            "[INFO|configuration_utils.py:789] 2024-04-11 23:53:49,041 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"models/small\",\n",
            "  \"architectures\": [\n",
            "    \"BertModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"cell\": {},\n",
            "  \"classifier_dropout\": null,\n",
            "  \"emb_size\": 312,\n",
            "  \"finetuning_task\": \"sst2\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 312,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 1200,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 4,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"pre_trained\": \"\",\n",
            "  \"structure\": [],\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.40.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2085] 2024-04-11 23:53:49,042 >> loading file vocab.txt\n",
            "[INFO|tokenization_utils_base.py:2085] 2024-04-11 23:53:49,042 >> loading file tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2085] 2024-04-11 23:53:49,042 >> loading file added_tokens.json\n",
            "[INFO|tokenization_utils_base.py:2085] 2024-04-11 23:53:49,042 >> loading file special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2085] 2024-04-11 23:53:49,042 >> loading file tokenizer_config.json\n",
            "[INFO|modeling_utils.py:3426] 2024-04-11 23:53:49,174 >> loading weights file models/small/model.safetensors\n",
            "[INFO|modeling_utils.py:4170] 2024-04-11 23:53:49,272 >> All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
            "\n",
            "[WARNING|modeling_utils.py:4172] 2024-04-11 23:53:49,272 >> Some weights of BertForSequenceClassification were not initialized from the model checkpoint at models/small and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Running tokenizer on dataset:   0% 0/67349 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/nyu-mll___glue/sst2/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-7f6a8026acde7a4c.arrow\n",
            "04/11/2024 23:53:49 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/nyu-mll___glue/sst2/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-7f6a8026acde7a4c.arrow\n",
            "Running tokenizer on dataset:   1% 1000/67349 [00:00<00:12, 5524.43 examples/s]^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Code for running benchmark\n",
        "!mkdir test_results_medium_SST2\n",
        "!python run_glue.py --model_name_or_path models/medium --task_name SST2 --do_train --do_eval --max_seq_length 128 --per_device_train_batch_size 32 --learning_rate 2e-5 --num_train_epochs 3 --output_dir test_results_medium_SST2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lcnC1H_OAAhP",
        "outputId": "0bc273c4-e5b5-4a60-82f6-140440d6b088"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exception ignored in: <function _xla_gc_callback at 0x7fac4bde20e0>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/jax/_src/lib/__init__.py\", line 98, in _xla_gc_callback\n",
            "    def _xla_gc_callback(*args):\n",
            "KeyboardInterrupt: \n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Code for running benchmark\n",
        "!mkdir test_results_int_expanded_SST2\n",
        "!python run_glue.py --model_name_or_path models/int_expanded_v2 --task_name SST2 --do_train --do_eval --max_seq_length 128 --per_device_train_batch_size 32 --learning_rate 2e-5 --num_train_epochs 3 --output_dir test_results_int_expanded_SST2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jyt9h-PvAAcA",
        "outputId": "fe5dac8e-32af-4a3c-879b-e69b15fc5f90"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MRPC Evaluation**"
      ],
      "metadata": {
        "id": "OTRX3F36AhrT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code for running benchmark\n",
        "!mkdir test_results_tinyBERT_MRPC\n",
        "!python run_glue.py --model_name_or_path models/basev2 --task_name MRPC --do_train --do_eval --max_seq_length 128 --per_device_train_batch_size 32 --learning_rate 2e-5 --num_train_epochs 3 --output_dir test_results_tinyBERT_MRPC"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jT45uODiAjvo",
        "outputId": "52957684-32c3-44d8-e1fe-81ff66051c42"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘test_results_tinyBERT_MRPC’: File exists\n",
            "2024-04-12 00:51:19.166463: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-04-12 00:51:19.166533: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-04-12 00:51:19.168175: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-04-12 00:51:20.586150: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "04/12/2024 00:51:23 - WARNING - __main__ - Process rank: 0, device: cpu, n_gpu: 0, distributed training: False, 16-bits training: False\n",
            "04/12/2024 00:51:23 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=0,\n",
            "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'gradient_accumulation_kwargs': None},\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_persistent_workers=False,\n",
            "dataloader_pin_memory=True,\n",
            "dataloader_prefetch_factor=None,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_num_input_tokens_seen=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=2e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=test_results_tinyBERT_MRPC/runs/Apr12_00-51-23_4c4acca1938d,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "optim_target_modules=None,\n",
            "output_dir=test_results_tinyBERT_MRPC,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=32,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=test_results_tinyBERT_MRPC,\n",
            "save_on_each_node=False,\n",
            "save_only_model=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "split_batches=None,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "Overwrite dataset info from restored data version if exists.\n",
            "04/12/2024 00:51:25 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/nyu-mll___glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c\n",
            "04/12/2024 00:51:25 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/nyu-mll___glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c\n",
            "Found cached dataset glue (/root/.cache/huggingface/datasets/nyu-mll___glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)\n",
            "04/12/2024 00:51:25 - INFO - datasets.builder - Found cached dataset glue (/root/.cache/huggingface/datasets/nyu-mll___glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/nyu-mll___glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c\n",
            "04/12/2024 00:51:25 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/nyu-mll___glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c\n",
            "[INFO|configuration_utils.py:724] 2024-04-12 00:51:25,627 >> loading configuration file models/basev2/config.json\n",
            "[INFO|configuration_utils.py:789] 2024-04-12 00:51:25,631 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"models/basev2\",\n",
            "  \"architectures\": [\n",
            "    \"BertModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"cell\": {},\n",
            "  \"classifier_dropout\": null,\n",
            "  \"emb_size\": 312,\n",
            "  \"finetuning_task\": \"mrpc\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 312,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 1200,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 4,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"pre_trained\": \"\",\n",
            "  \"structure\": [],\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.40.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2085] 2024-04-12 00:51:25,631 >> loading file vocab.txt\n",
            "[INFO|tokenization_utils_base.py:2085] 2024-04-12 00:51:25,631 >> loading file tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2085] 2024-04-12 00:51:25,632 >> loading file added_tokens.json\n",
            "[INFO|tokenization_utils_base.py:2085] 2024-04-12 00:51:25,632 >> loading file special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2085] 2024-04-12 00:51:25,632 >> loading file tokenizer_config.json\n",
            "[INFO|modeling_utils.py:3426] 2024-04-12 00:51:25,739 >> loading weights file models/basev2/model.safetensors\n",
            "[INFO|modeling_utils.py:4170] 2024-04-12 00:51:25,809 >> All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
            "\n",
            "[WARNING|modeling_utils.py:4172] 2024-04-12 00:51:25,810 >> Some weights of BertForSequenceClassification were not initialized from the model checkpoint at models/basev2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Running tokenizer on dataset:   0% 0/3668 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/nyu-mll___glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-0eadafee39d27610.arrow\n",
            "04/12/2024 00:51:26 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/nyu-mll___glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-0eadafee39d27610.arrow\n",
            "Running tokenizer on dataset: 100% 3668/3668 [00:01<00:00, 2763.09 examples/s]\n",
            "Running tokenizer on dataset:   0% 0/408 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/nyu-mll___glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-082ac213c8c49741.arrow\n",
            "04/12/2024 00:51:27 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/nyu-mll___glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-082ac213c8c49741.arrow\n",
            "Running tokenizer on dataset: 100% 408/408 [00:00<00:00, 1864.72 examples/s]\n",
            "Running tokenizer on dataset:   0% 0/1725 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/nyu-mll___glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-f64dc56f9427b7e9.arrow\n",
            "04/12/2024 00:51:27 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/nyu-mll___glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-f64dc56f9427b7e9.arrow\n",
            "Running tokenizer on dataset: 100% 1725/1725 [00:00<00:00, 1970.38 examples/s]\n",
            "04/12/2024 00:51:28 - INFO - __main__ - Sample 2619 of the training set: {'sentence1': 'The proceedings were taken up with prosecutors outlining their case against Amrozi , reading 33 pages of documents outlining allegations against him .', 'sentence2': 'Proceedings were taken up with prosecutors outlining their case against Amrozi , reading a 33-page accusation letter to the court .', 'label': 1, 'idx': 2916, 'input_ids': [101, 1996, 8931, 2020, 2579, 2039, 2007, 19608, 2041, 16992, 2037, 2553, 2114, 2572, 3217, 5831, 1010, 3752, 3943, 5530, 1997, 5491, 2041, 16992, 9989, 2114, 2032, 1012, 102, 8931, 2020, 2579, 2039, 2007, 19608, 2041, 16992, 2037, 2553, 2114, 2572, 3217, 5831, 1010, 3752, 1037, 3943, 1011, 3931, 19238, 3661, 2000, 1996, 2457, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "04/12/2024 00:51:28 - INFO - __main__ - Sample 456 of the training set: {'sentence1': \"Chechen officials working for the Moscow-backed government are a frequent target for rebels and tension is running high ahead of next Sunday 's presidential election in war-torn Chechnya .\", 'sentence2': \"Officials in Chechnya 's Moscow-backed government are a frequent target for rebels , and tension is running high ahead of Sunday 's presidential election in the war-ravaged region .\", 'label': 1, 'idx': 509, 'input_ids': [101, 18178, 8661, 4584, 2551, 2005, 1996, 4924, 1011, 6153, 2231, 2024, 1037, 6976, 4539, 2005, 8431, 1998, 6980, 2003, 2770, 2152, 3805, 1997, 2279, 4465, 1005, 1055, 4883, 2602, 1999, 2162, 1011, 7950, 18178, 2818, 17238, 1012, 102, 4584, 1999, 18178, 2818, 17238, 1005, 1055, 4924, 1011, 6153, 2231, 2024, 1037, 6976, 4539, 2005, 8431, 1010, 1998, 6980, 2003, 2770, 2152, 3805, 1997, 4465, 1005, 1055, 4883, 2602, 1999, 1996, 2162, 1011, 25537, 2555, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "04/12/2024 00:51:28 - INFO - __main__ - Sample 102 of the training set: {'sentence1': \"Standard & Poor 's 500 stock index futures declined 4.40 points to 983.50 , while Nasdaq futures fell 6.5 points to 1,206.50 .\", 'sentence2': \"The Standard & Poor 's 500 Index was up 1.75 points , or 0.18 percent , to 977.68 .\", 'label': 0, 'idx': 116, 'input_ids': [101, 3115, 1004, 3532, 1005, 1055, 3156, 4518, 5950, 17795, 6430, 1018, 1012, 2871, 2685, 2000, 5818, 2509, 1012, 2753, 1010, 2096, 17235, 2850, 4160, 17795, 3062, 1020, 1012, 1019, 2685, 2000, 1015, 1010, 18744, 1012, 2753, 1012, 102, 1996, 3115, 1004, 3532, 1005, 1055, 3156, 5950, 2001, 2039, 1015, 1012, 4293, 2685, 1010, 2030, 1014, 1012, 2324, 3867, 1010, 2000, 5989, 2581, 1012, 6273, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "Downloading builder script: 100% 5.75k/5.75k [00:00<00:00, 12.2MB/s]\n",
            "[INFO|trainer.py:785] 2024-04-12 00:51:29,665 >> The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2047] 2024-04-12 00:51:29,671 >> ***** Running training *****\n",
            "[INFO|trainer.py:2048] 2024-04-12 00:51:29,671 >>   Num examples = 3,668\n",
            "[INFO|trainer.py:2049] 2024-04-12 00:51:29,671 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:2050] 2024-04-12 00:51:29,671 >>   Instantaneous batch size per device = 32\n",
            "[INFO|trainer.py:2053] 2024-04-12 00:51:29,671 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "[INFO|trainer.py:2054] 2024-04-12 00:51:29,671 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:2055] 2024-04-12 00:51:29,671 >>   Total optimization steps = 345\n",
            "[INFO|trainer.py:2056] 2024-04-12 00:51:29,672 >>   Number of trainable parameters = 14,350,874\n",
            "100% 345/345 [17:56<00:00,  2.67s/it][INFO|trainer.py:2315] 2024-04-12 01:09:25,858 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 1076.1865, 'train_samples_per_second': 10.225, 'train_steps_per_second': 0.321, 'train_loss': 0.4922018742215806, 'epoch': 3.0}\n",
            "100% 345/345 [17:56<00:00,  3.12s/it]\n",
            "[INFO|trainer.py:3304] 2024-04-12 01:09:25,861 >> Saving model checkpoint to test_results_tinyBERT_MRPC\n",
            "[INFO|configuration_utils.py:471] 2024-04-12 01:09:25,863 >> Configuration saved in test_results_tinyBERT_MRPC/config.json\n",
            "[INFO|modeling_utils.py:2590] 2024-04-12 01:09:25,941 >> Model weights saved in test_results_tinyBERT_MRPC/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2488] 2024-04-12 01:09:25,942 >> tokenizer config file saved in test_results_tinyBERT_MRPC/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2497] 2024-04-12 01:09:25,943 >> Special tokens file saved in test_results_tinyBERT_MRPC/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        3.0\n",
            "  train_loss               =     0.4922\n",
            "  train_runtime            = 0:17:56.18\n",
            "  train_samples            =       3668\n",
            "  train_samples_per_second =     10.225\n",
            "  train_steps_per_second   =      0.321\n",
            "04/12/2024 01:09:25 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:785] 2024-04-12 01:09:25,971 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3613] 2024-04-12 01:09:25,974 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3615] 2024-04-12 01:09:25,974 >>   Num examples = 408\n",
            "[INFO|trainer.py:3618] 2024-04-12 01:09:25,974 >>   Batch size = 8\n",
            "100% 51/51 [00:12<00:00,  4.01it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        3.0\n",
            "  eval_accuracy           =     0.8235\n",
            "  eval_combined_score     =      0.851\n",
            "  eval_f1                 =     0.8784\n",
            "  eval_loss               =     0.4304\n",
            "  eval_runtime            = 0:00:12.96\n",
            "  eval_samples            =        408\n",
            "  eval_samples_per_second =     31.467\n",
            "  eval_steps_per_second   =      3.933\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Code for running benchmark\n",
        "!mkdir test_results_small_MRPC\n",
        "!python run_glue.py --model_name_or_path models/small --task_name MRPC --do_train --do_eval --max_seq_length 128 --per_device_train_batch_size 32 --learning_rate 2e-5 --num_train_epochs 3 --output_dir test_results_small_MRPC"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Btk0Y9T3An8i",
        "outputId": "ba1f48df-a8f6-408b-c816-9b4465ecc32f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-04-12 01:24:45.841169: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-04-12 01:24:45.841231: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-04-12 01:24:45.843762: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-04-12 01:24:47.401531: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "04/12/2024 01:24:50 - WARNING - __main__ - Process rank: 0, device: cpu, n_gpu: 0, distributed training: False, 16-bits training: False\n",
            "04/12/2024 01:24:50 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=0,\n",
            "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'gradient_accumulation_kwargs': None},\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_persistent_workers=False,\n",
            "dataloader_pin_memory=True,\n",
            "dataloader_prefetch_factor=None,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_num_input_tokens_seen=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=2e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=test_results_small_MRPC/runs/Apr12_01-24-50_4c4acca1938d,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "optim_target_modules=None,\n",
            "output_dir=test_results_small_MRPC,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=32,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=test_results_small_MRPC,\n",
            "save_on_each_node=False,\n",
            "save_only_model=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "split_batches=None,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "Overwrite dataset info from restored data version if exists.\n",
            "04/12/2024 01:24:53 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/nyu-mll___glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c\n",
            "04/12/2024 01:24:53 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/nyu-mll___glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c\n",
            "Found cached dataset glue (/root/.cache/huggingface/datasets/nyu-mll___glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)\n",
            "04/12/2024 01:24:53 - INFO - datasets.builder - Found cached dataset glue (/root/.cache/huggingface/datasets/nyu-mll___glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/nyu-mll___glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c\n",
            "04/12/2024 01:24:53 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/nyu-mll___glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c\n",
            "[INFO|configuration_utils.py:724] 2024-04-12 01:24:53,214 >> loading configuration file models/small/config.json\n",
            "[INFO|configuration_utils.py:789] 2024-04-12 01:24:53,220 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"models/small\",\n",
            "  \"architectures\": [\n",
            "    \"BertModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"cell\": {},\n",
            "  \"classifier_dropout\": null,\n",
            "  \"emb_size\": 312,\n",
            "  \"finetuning_task\": \"mrpc\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 312,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 1200,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 4,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"pre_trained\": \"\",\n",
            "  \"structure\": [],\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.40.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2085] 2024-04-12 01:24:53,222 >> loading file vocab.txt\n",
            "[INFO|tokenization_utils_base.py:2085] 2024-04-12 01:24:53,222 >> loading file tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2085] 2024-04-12 01:24:53,222 >> loading file added_tokens.json\n",
            "[INFO|tokenization_utils_base.py:2085] 2024-04-12 01:24:53,222 >> loading file special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2085] 2024-04-12 01:24:53,222 >> loading file tokenizer_config.json\n",
            "[INFO|modeling_utils.py:3426] 2024-04-12 01:24:53,410 >> loading weights file models/small/model.safetensors\n",
            "[INFO|modeling_utils.py:4170] 2024-04-12 01:24:53,974 >> All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
            "\n",
            "[WARNING|modeling_utils.py:4172] 2024-04-12 01:24:53,974 >> Some weights of BertForSequenceClassification were not initialized from the model checkpoint at models/small and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Running tokenizer on dataset:   0% 0/3668 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/nyu-mll___glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-1bae599d287976eb.arrow\n",
            "04/12/2024 01:24:54 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/nyu-mll___glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-1bae599d287976eb.arrow\n",
            "Running tokenizer on dataset: 100% 3668/3668 [00:01<00:00, 3358.20 examples/s]\n",
            "Running tokenizer on dataset:   0% 0/408 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/nyu-mll___glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-7cfce37071a45e83.arrow\n",
            "04/12/2024 01:24:55 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/nyu-mll___glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-7cfce37071a45e83.arrow\n",
            "Running tokenizer on dataset: 100% 408/408 [00:00<00:00, 3623.26 examples/s]\n",
            "Running tokenizer on dataset:   0% 0/1725 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/nyu-mll___glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-374b98a8169d7a43.arrow\n",
            "04/12/2024 01:24:55 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/nyu-mll___glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-374b98a8169d7a43.arrow\n",
            "Running tokenizer on dataset: 100% 1725/1725 [00:00<00:00, 3649.37 examples/s]\n",
            "04/12/2024 01:24:55 - INFO - __main__ - Sample 2619 of the training set: {'sentence1': 'The proceedings were taken up with prosecutors outlining their case against Amrozi , reading 33 pages of documents outlining allegations against him .', 'sentence2': 'Proceedings were taken up with prosecutors outlining their case against Amrozi , reading a 33-page accusation letter to the court .', 'label': 1, 'idx': 2916, 'input_ids': [101, 1996, 8931, 2020, 2579, 2039, 2007, 19608, 2041, 16992, 2037, 2553, 2114, 2572, 3217, 5831, 1010, 3752, 3943, 5530, 1997, 5491, 2041, 16992, 9989, 2114, 2032, 1012, 102, 8931, 2020, 2579, 2039, 2007, 19608, 2041, 16992, 2037, 2553, 2114, 2572, 3217, 5831, 1010, 3752, 1037, 3943, 1011, 3931, 19238, 3661, 2000, 1996, 2457, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "04/12/2024 01:24:55 - INFO - __main__ - Sample 456 of the training set: {'sentence1': \"Chechen officials working for the Moscow-backed government are a frequent target for rebels and tension is running high ahead of next Sunday 's presidential election in war-torn Chechnya .\", 'sentence2': \"Officials in Chechnya 's Moscow-backed government are a frequent target for rebels , and tension is running high ahead of Sunday 's presidential election in the war-ravaged region .\", 'label': 1, 'idx': 509, 'input_ids': [101, 18178, 8661, 4584, 2551, 2005, 1996, 4924, 1011, 6153, 2231, 2024, 1037, 6976, 4539, 2005, 8431, 1998, 6980, 2003, 2770, 2152, 3805, 1997, 2279, 4465, 1005, 1055, 4883, 2602, 1999, 2162, 1011, 7950, 18178, 2818, 17238, 1012, 102, 4584, 1999, 18178, 2818, 17238, 1005, 1055, 4924, 1011, 6153, 2231, 2024, 1037, 6976, 4539, 2005, 8431, 1010, 1998, 6980, 2003, 2770, 2152, 3805, 1997, 4465, 1005, 1055, 4883, 2602, 1999, 1996, 2162, 1011, 25537, 2555, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "04/12/2024 01:24:55 - INFO - __main__ - Sample 102 of the training set: {'sentence1': \"Standard & Poor 's 500 stock index futures declined 4.40 points to 983.50 , while Nasdaq futures fell 6.5 points to 1,206.50 .\", 'sentence2': \"The Standard & Poor 's 500 Index was up 1.75 points , or 0.18 percent , to 977.68 .\", 'label': 0, 'idx': 116, 'input_ids': [101, 3115, 1004, 3532, 1005, 1055, 3156, 4518, 5950, 17795, 6430, 1018, 1012, 2871, 2685, 2000, 5818, 2509, 1012, 2753, 1010, 2096, 17235, 2850, 4160, 17795, 3062, 1020, 1012, 1019, 2685, 2000, 1015, 1010, 18744, 1012, 2753, 1012, 102, 1996, 3115, 1004, 3532, 1005, 1055, 3156, 5950, 2001, 2039, 1015, 1012, 4293, 2685, 1010, 2030, 1014, 1012, 2324, 3867, 1010, 2000, 5989, 2581, 1012, 6273, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "[INFO|trainer.py:785] 2024-04-12 01:24:56,683 >> The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2047] 2024-04-12 01:24:56,690 >> ***** Running training *****\n",
            "[INFO|trainer.py:2048] 2024-04-12 01:24:56,690 >>   Num examples = 3,668\n",
            "[INFO|trainer.py:2049] 2024-04-12 01:24:56,690 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:2050] 2024-04-12 01:24:56,690 >>   Instantaneous batch size per device = 32\n",
            "[INFO|trainer.py:2053] 2024-04-12 01:24:56,690 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "[INFO|trainer.py:2054] 2024-04-12 01:24:56,690 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:2055] 2024-04-12 01:24:56,690 >>   Total optimization steps = 345\n",
            "[INFO|trainer.py:2056] 2024-04-12 01:24:56,691 >>   Number of trainable parameters = 14,350,874\n",
            "100% 345/345 [17:28<00:00,  2.61s/it][INFO|trainer.py:2315] 2024-04-12 01:42:25,481 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 1048.79, 'train_samples_per_second': 10.492, 'train_steps_per_second': 0.329, 'train_loss': 0.5675279257954031, 'epoch': 3.0}\n",
            "100% 345/345 [17:28<00:00,  3.04s/it]\n",
            "[INFO|trainer.py:3304] 2024-04-12 01:42:25,484 >> Saving model checkpoint to test_results_small_MRPC\n",
            "[INFO|configuration_utils.py:471] 2024-04-12 01:42:25,486 >> Configuration saved in test_results_small_MRPC/config.json\n",
            "[INFO|modeling_utils.py:2590] 2024-04-12 01:42:25,560 >> Model weights saved in test_results_small_MRPC/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2488] 2024-04-12 01:42:25,561 >> tokenizer config file saved in test_results_small_MRPC/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2497] 2024-04-12 01:42:25,562 >> Special tokens file saved in test_results_small_MRPC/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        3.0\n",
            "  train_loss               =     0.5675\n",
            "  train_runtime            = 0:17:28.78\n",
            "  train_samples            =       3668\n",
            "  train_samples_per_second =     10.492\n",
            "  train_steps_per_second   =      0.329\n",
            "04/12/2024 01:42:25 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:785] 2024-04-12 01:42:25,579 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3613] 2024-04-12 01:42:25,581 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3615] 2024-04-12 01:42:25,581 >>   Num examples = 408\n",
            "[INFO|trainer.py:3618] 2024-04-12 01:42:25,581 >>   Batch size = 8\n",
            "100% 51/51 [00:12<00:00,  4.03it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        3.0\n",
            "  eval_accuracy           =     0.7696\n",
            "  eval_combined_score     =      0.809\n",
            "  eval_f1                 =     0.8484\n",
            "  eval_loss               =     0.5107\n",
            "  eval_runtime            = 0:00:12.88\n",
            "  eval_samples            =        408\n",
            "  eval_samples_per_second =     31.668\n",
            "  eval_steps_per_second   =      3.958\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Code for running benchmark\n",
        "!mkdir test_results_medium_MRPC\n",
        "!python run_glue.py --model_name_or_path models/medium --task_name MRPC --do_train --do_eval --max_seq_length 128 --per_device_train_batch_size 32 --learning_rate 2e-5 --num_train_epochs 3 --output_dir test_results_medium_MRPC"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "utwp9pUTAsRt",
        "outputId": "095896e4-ba25-4312-9f1e-42ecdf9beedd"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-04-12 01:42:45.107300: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-04-12 01:42:45.107366: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-04-12 01:42:45.108991: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-04-12 01:42:46.535461: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "04/12/2024 01:42:49 - WARNING - __main__ - Process rank: 0, device: cpu, n_gpu: 0, distributed training: False, 16-bits training: False\n",
            "04/12/2024 01:42:49 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=0,\n",
            "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'gradient_accumulation_kwargs': None},\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_persistent_workers=False,\n",
            "dataloader_pin_memory=True,\n",
            "dataloader_prefetch_factor=None,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_num_input_tokens_seen=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=2e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=test_results_medium_MRPC/runs/Apr12_01-42-49_4c4acca1938d,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "optim_target_modules=None,\n",
            "output_dir=test_results_medium_MRPC,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=32,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=test_results_medium_MRPC,\n",
            "save_on_each_node=False,\n",
            "save_only_model=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "split_batches=None,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "Overwrite dataset info from restored data version if exists.\n",
            "04/12/2024 01:42:52 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/nyu-mll___glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c\n",
            "04/12/2024 01:42:52 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/nyu-mll___glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c\n",
            "Found cached dataset glue (/root/.cache/huggingface/datasets/nyu-mll___glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)\n",
            "04/12/2024 01:42:52 - INFO - datasets.builder - Found cached dataset glue (/root/.cache/huggingface/datasets/nyu-mll___glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/nyu-mll___glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c\n",
            "04/12/2024 01:42:52 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/nyu-mll___glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c\n",
            "[INFO|configuration_utils.py:724] 2024-04-12 01:42:52,243 >> loading configuration file models/medium/config.json\n",
            "[INFO|configuration_utils.py:789] 2024-04-12 01:42:52,262 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"models/medium\",\n",
            "  \"architectures\": [\n",
            "    \"BertModel\"\n",
            "  ],\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"cell\": {},\n",
            "  \"classifier_dropout\": null,\n",
            "  \"emb_size\": 312,\n",
            "  \"finetuning_task\": \"mrpc\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 312,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 1200,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 4,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"pre_trained\": \"\",\n",
            "  \"structure\": [],\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.40.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2085] 2024-04-12 01:42:52,265 >> loading file vocab.txt\n",
            "[INFO|tokenization_utils_base.py:2085] 2024-04-12 01:42:52,266 >> loading file tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2085] 2024-04-12 01:42:52,266 >> loading file added_tokens.json\n",
            "[INFO|tokenization_utils_base.py:2085] 2024-04-12 01:42:52,266 >> loading file special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2085] 2024-04-12 01:42:52,266 >> loading file tokenizer_config.json\n",
            "[INFO|modeling_utils.py:3426] 2024-04-12 01:42:52,631 >> loading weights file models/medium/model.safetensors\n",
            "[INFO|modeling_utils.py:4170] 2024-04-12 01:42:53,362 >> All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
            "\n",
            "[WARNING|modeling_utils.py:4172] 2024-04-12 01:42:53,362 >> Some weights of BertForSequenceClassification were not initialized from the model checkpoint at models/medium and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Running tokenizer on dataset:   0% 0/3668 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/nyu-mll___glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-c997e5c6fdb0efdd.arrow\n",
            "04/12/2024 01:42:54 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/nyu-mll___glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-c997e5c6fdb0efdd.arrow\n",
            "Running tokenizer on dataset: 100% 3668/3668 [00:02<00:00, 1334.77 examples/s]\n",
            "Running tokenizer on dataset:   0% 0/408 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/nyu-mll___glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-6542309ed907d6ad.arrow\n",
            "04/12/2024 01:42:56 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/nyu-mll___glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-6542309ed907d6ad.arrow\n",
            "Running tokenizer on dataset: 100% 408/408 [00:00<00:00, 2985.70 examples/s]\n",
            "Running tokenizer on dataset:   0% 0/1725 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/nyu-mll___glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-b76271c3ff7024fc.arrow\n",
            "04/12/2024 01:42:56 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/nyu-mll___glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-b76271c3ff7024fc.arrow\n",
            "Running tokenizer on dataset: 100% 1725/1725 [00:00<00:00, 3130.57 examples/s]\n",
            "04/12/2024 01:42:56 - INFO - __main__ - Sample 2619 of the training set: {'sentence1': 'The proceedings were taken up with prosecutors outlining their case against Amrozi , reading 33 pages of documents outlining allegations against him .', 'sentence2': 'Proceedings were taken up with prosecutors outlining their case against Amrozi , reading a 33-page accusation letter to the court .', 'label': 1, 'idx': 2916, 'input_ids': [101, 1996, 8931, 2020, 2579, 2039, 2007, 19608, 2041, 16992, 2037, 2553, 2114, 2572, 3217, 5831, 1010, 3752, 3943, 5530, 1997, 5491, 2041, 16992, 9989, 2114, 2032, 1012, 102, 8931, 2020, 2579, 2039, 2007, 19608, 2041, 16992, 2037, 2553, 2114, 2572, 3217, 5831, 1010, 3752, 1037, 3943, 1011, 3931, 19238, 3661, 2000, 1996, 2457, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "04/12/2024 01:42:56 - INFO - __main__ - Sample 456 of the training set: {'sentence1': \"Chechen officials working for the Moscow-backed government are a frequent target for rebels and tension is running high ahead of next Sunday 's presidential election in war-torn Chechnya .\", 'sentence2': \"Officials in Chechnya 's Moscow-backed government are a frequent target for rebels , and tension is running high ahead of Sunday 's presidential election in the war-ravaged region .\", 'label': 1, 'idx': 509, 'input_ids': [101, 18178, 8661, 4584, 2551, 2005, 1996, 4924, 1011, 6153, 2231, 2024, 1037, 6976, 4539, 2005, 8431, 1998, 6980, 2003, 2770, 2152, 3805, 1997, 2279, 4465, 1005, 1055, 4883, 2602, 1999, 2162, 1011, 7950, 18178, 2818, 17238, 1012, 102, 4584, 1999, 18178, 2818, 17238, 1005, 1055, 4924, 1011, 6153, 2231, 2024, 1037, 6976, 4539, 2005, 8431, 1010, 1998, 6980, 2003, 2770, 2152, 3805, 1997, 4465, 1005, 1055, 4883, 2602, 1999, 1996, 2162, 1011, 25537, 2555, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "04/12/2024 01:42:56 - INFO - __main__ - Sample 102 of the training set: {'sentence1': \"Standard & Poor 's 500 stock index futures declined 4.40 points to 983.50 , while Nasdaq futures fell 6.5 points to 1,206.50 .\", 'sentence2': \"The Standard & Poor 's 500 Index was up 1.75 points , or 0.18 percent , to 977.68 .\", 'label': 0, 'idx': 116, 'input_ids': [101, 3115, 1004, 3532, 1005, 1055, 3156, 4518, 5950, 17795, 6430, 1018, 1012, 2871, 2685, 2000, 5818, 2509, 1012, 2753, 1010, 2096, 17235, 2850, 4160, 17795, 3062, 1020, 1012, 1019, 2685, 2000, 1015, 1010, 18744, 1012, 2753, 1012, 102, 1996, 3115, 1004, 3532, 1005, 1055, 3156, 5950, 2001, 2039, 1015, 1012, 4293, 2685, 1010, 2030, 1014, 1012, 2324, 3867, 1010, 2000, 5989, 2581, 1012, 6273, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "[INFO|trainer.py:785] 2024-04-12 01:42:57,918 >> The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2047] 2024-04-12 01:42:57,927 >> ***** Running training *****\n",
            "[INFO|trainer.py:2048] 2024-04-12 01:42:57,927 >>   Num examples = 3,668\n",
            "[INFO|trainer.py:2049] 2024-04-12 01:42:57,927 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:2050] 2024-04-12 01:42:57,927 >>   Instantaneous batch size per device = 32\n",
            "[INFO|trainer.py:2053] 2024-04-12 01:42:57,927 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "[INFO|trainer.py:2054] 2024-04-12 01:42:57,927 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:2055] 2024-04-12 01:42:57,927 >>   Total optimization steps = 345\n",
            "[INFO|trainer.py:2056] 2024-04-12 01:42:57,927 >>   Number of trainable parameters = 14,350,874\n",
            "100% 345/345 [18:11<00:00,  2.93s/it][INFO|trainer.py:2315] 2024-04-12 02:01:09,296 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 1091.3687, 'train_samples_per_second': 10.083, 'train_steps_per_second': 0.316, 'train_loss': 0.5190469050752944, 'epoch': 3.0}\n",
            "100% 345/345 [18:11<00:00,  3.16s/it]\n",
            "[INFO|trainer.py:3304] 2024-04-12 02:01:09,299 >> Saving model checkpoint to test_results_medium_MRPC\n",
            "[INFO|configuration_utils.py:471] 2024-04-12 02:01:09,301 >> Configuration saved in test_results_medium_MRPC/config.json\n",
            "[INFO|modeling_utils.py:2590] 2024-04-12 02:01:09,402 >> Model weights saved in test_results_medium_MRPC/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2488] 2024-04-12 02:01:09,404 >> tokenizer config file saved in test_results_medium_MRPC/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2497] 2024-04-12 02:01:09,404 >> Special tokens file saved in test_results_medium_MRPC/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        3.0\n",
            "  train_loss               =      0.519\n",
            "  train_runtime            = 0:18:11.36\n",
            "  train_samples            =       3668\n",
            "  train_samples_per_second =     10.083\n",
            "  train_steps_per_second   =      0.316\n",
            "04/12/2024 02:01:09 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:785] 2024-04-12 02:01:09,431 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: idx, sentence1, sentence2. If idx, sentence1, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3613] 2024-04-12 02:01:09,437 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3615] 2024-04-12 02:01:09,439 >>   Num examples = 408\n",
            "[INFO|trainer.py:3618] 2024-04-12 02:01:09,439 >>   Batch size = 8\n",
            "100% 51/51 [00:13<00:00,  3.90it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        3.0\n",
            "  eval_accuracy           =     0.8137\n",
            "  eval_combined_score     =     0.8439\n",
            "  eval_f1                 =     0.8742\n",
            "  eval_loss               =     0.4611\n",
            "  eval_runtime            = 0:00:13.42\n",
            "  eval_samples            =        408\n",
            "  eval_samples_per_second =     30.385\n",
            "  eval_steps_per_second   =      3.798\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Code for running benchmark\n",
        "!mkdir test_results_int_expanded_MRPC\n",
        "!python run_glue.py --model_name_or_path models/int_expanded_v2 --task_name MRPC --do_train --do_eval --max_seq_length 128 --per_device_train_batch_size 32 --learning_rate 2e-5 --num_train_epochs 3 --output_dir test_results_int_expanded_MRPC"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tr3imCc9Axtb",
        "outputId": "0e232892-a3b1-466b-a82e-6fa5218e4901"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2024-04-12 02:01:29.422548: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-04-12 02:01:29.422617: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-04-12 02:01:29.424257: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-04-12 02:01:30.893441: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "04/12/2024 02:01:33 - WARNING - __main__ - Process rank: 0, device: cpu, n_gpu: 0, distributed training: False, 16-bits training: False\n",
            "04/12/2024 02:01:33 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
            "_n_gpu=0,\n",
            "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'gradient_accumulation_kwargs': None},\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_persistent_workers=False,\n",
            "dataloader_pin_memory=True,\n",
            "dataloader_prefetch_factor=None,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=None,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=False,\n",
            "dispatch_batches=None,\n",
            "do_eval=True,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=False,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "gradient_checkpointing_kwargs=None,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_num_input_tokens_seen=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=2e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=test_results_int_expanded_MRPC/runs/Apr12_02-01-33_4c4acca1938d,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=500,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_kwargs={},\n",
            "lr_scheduler_type=linear,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "neftune_noise_alpha=None,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "optim_target_modules=None,\n",
            "output_dir=test_results_int_expanded_MRPC,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=32,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=test_results_int_expanded_MRPC,\n",
            "save_on_each_node=False,\n",
            "save_only_model=False,\n",
            "save_safetensors=True,\n",
            "save_steps=500,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "skip_memory_metrics=True,\n",
            "split_batches=None,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "Overwrite dataset info from restored data version if exists.\n",
            "04/12/2024 02:01:36 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/nyu-mll___glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c\n",
            "04/12/2024 02:01:36 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/nyu-mll___glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c\n",
            "Found cached dataset glue (/root/.cache/huggingface/datasets/nyu-mll___glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)\n",
            "04/12/2024 02:01:36 - INFO - datasets.builder - Found cached dataset glue (/root/.cache/huggingface/datasets/nyu-mll___glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c)\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/nyu-mll___glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c\n",
            "04/12/2024 02:01:36 - INFO - datasets.info - Loading Dataset info from /root/.cache/huggingface/datasets/nyu-mll___glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c\n",
            "[INFO|configuration_utils.py:724] 2024-04-12 02:01:36,805 >> loading configuration file models/int_expanded_v2/config.json\n",
            "[INFO|configuration_utils.py:789] 2024-04-12 02:01:36,809 >> Model config BertConfig {\n",
            "  \"_name_or_path\": \"models/int_expanded_v2\",\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"cell\": {},\n",
            "  \"classifier_dropout\": null,\n",
            "  \"emb_size\": 312,\n",
            "  \"finetuning_task\": \"mrpc\",\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 312,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 1200,\n",
            "  \"layer_norm_eps\": 1e-12,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"model_type\": \"bert\",\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 4,\n",
            "  \"pad_token_id\": 0,\n",
            "  \"position_embedding_type\": \"absolute\",\n",
            "  \"pre_trained\": \"\",\n",
            "  \"structure\": [],\n",
            "  \"transformers_version\": \"4.40.0.dev0\",\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 30522\n",
            "}\n",
            "\n",
            "[INFO|tokenization_utils_base.py:2085] 2024-04-12 02:01:36,811 >> loading file vocab.txt\n",
            "[INFO|tokenization_utils_base.py:2085] 2024-04-12 02:01:36,811 >> loading file tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2085] 2024-04-12 02:01:36,811 >> loading file added_tokens.json\n",
            "[INFO|tokenization_utils_base.py:2085] 2024-04-12 02:01:36,811 >> loading file special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2085] 2024-04-12 02:01:36,811 >> loading file tokenizer_config.json\n",
            "[INFO|modeling_utils.py:3426] 2024-04-12 02:01:36,918 >> loading weights file models/int_expanded_v2/model.safetensors\n",
            "[INFO|modeling_utils.py:4170] 2024-04-12 02:01:37,436 >> All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
            "\n",
            "[WARNING|modeling_utils.py:4172] 2024-04-12 02:01:37,436 >> Some weights of BertForSequenceClassification were not initialized from the model checkpoint at models/int_expanded_v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Running tokenizer on dataset:   0% 0/3668 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/nyu-mll___glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-699054b5ba562e6c.arrow\n",
            "04/12/2024 02:01:37 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/nyu-mll___glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-699054b5ba562e6c.arrow\n",
            "Running tokenizer on dataset: 100% 3668/3668 [00:01<00:00, 3626.16 examples/s]\n",
            "Running tokenizer on dataset:   0% 0/408 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/nyu-mll___glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-04f6f63cc96619f2.arrow\n",
            "04/12/2024 02:01:38 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/nyu-mll___glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-04f6f63cc96619f2.arrow\n",
            "Running tokenizer on dataset: 100% 408/408 [00:00<00:00, 3458.97 examples/s]\n",
            "Running tokenizer on dataset:   0% 0/1725 [00:00<?, ? examples/s]Caching processed dataset at /root/.cache/huggingface/datasets/nyu-mll___glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-e96e16f5a2110db5.arrow\n",
            "04/12/2024 02:01:38 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/nyu-mll___glue/mrpc/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c/cache-e96e16f5a2110db5.arrow\n",
            "Running tokenizer on dataset: 100% 1725/1725 [00:00<00:00, 3683.41 examples/s]\n",
            "04/12/2024 02:01:39 - INFO - __main__ - Sample 2619 of the training set: {'sentence1': 'The proceedings were taken up with prosecutors outlining their case against Amrozi , reading 33 pages of documents outlining allegations against him .', 'sentence2': 'Proceedings were taken up with prosecutors outlining their case against Amrozi , reading a 33-page accusation letter to the court .', 'label': 1, 'idx': 2916, 'input_ids': [101, 1996, 8931, 2020, 2579, 2039, 2007, 19608, 2041, 16992, 2037, 2553, 2114, 2572, 3217, 5831, 1010, 3752, 3943, 5530, 1997, 5491, 2041, 16992, 9989, 2114, 2032, 1012, 102, 8931, 2020, 2579, 2039, 2007, 19608, 2041, 16992, 2037, 2553, 2114, 2572, 3217, 5831, 1010, 3752, 1037, 3943, 1011, 3931, 19238, 3661, 2000, 1996, 2457, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "04/12/2024 02:01:39 - INFO - __main__ - Sample 456 of the training set: {'sentence1': \"Chechen officials working for the Moscow-backed government are a frequent target for rebels and tension is running high ahead of next Sunday 's presidential election in war-torn Chechnya .\", 'sentence2': \"Officials in Chechnya 's Moscow-backed government are a frequent target for rebels , and tension is running high ahead of Sunday 's presidential election in the war-ravaged region .\", 'label': 1, 'idx': 509, 'input_ids': [101, 18178, 8661, 4584, 2551, 2005, 1996, 4924, 1011, 6153, 2231, 2024, 1037, 6976, 4539, 2005, 8431, 1998, 6980, 2003, 2770, 2152, 3805, 1997, 2279, 4465, 1005, 1055, 4883, 2602, 1999, 2162, 1011, 7950, 18178, 2818, 17238, 1012, 102, 4584, 1999, 18178, 2818, 17238, 1005, 1055, 4924, 1011, 6153, 2231, 2024, 1037, 6976, 4539, 2005, 8431, 1010, 1998, 6980, 2003, 2770, 2152, 3805, 1997, 4465, 1005, 1055, 4883, 2602, 1999, 1996, 2162, 1011, 25537, 2555, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "04/12/2024 02:01:39 - INFO - __main__ - Sample 102 of the training set: {'sentence1': \"Standard & Poor 's 500 stock index futures declined 4.40 points to 983.50 , while Nasdaq futures fell 6.5 points to 1,206.50 .\", 'sentence2': \"The Standard & Poor 's 500 Index was up 1.75 points , or 0.18 percent , to 977.68 .\", 'label': 0, 'idx': 116, 'input_ids': [101, 3115, 1004, 3532, 1005, 1055, 3156, 4518, 5950, 17795, 6430, 1018, 1012, 2871, 2685, 2000, 5818, 2509, 1012, 2753, 1010, 2096, 17235, 2850, 4160, 17795, 3062, 1020, 1012, 1019, 2685, 2000, 1015, 1010, 18744, 1012, 2753, 1012, 102, 1996, 3115, 1004, 3532, 1005, 1055, 3156, 5950, 2001, 2039, 1015, 1012, 4293, 2685, 1010, 2030, 1014, 1012, 2324, 3867, 1010, 2000, 5989, 2581, 1012, 6273, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}.\n",
            "[INFO|trainer.py:785] 2024-04-12 02:01:40,236 >> The following columns in the training set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:2047] 2024-04-12 02:01:40,243 >> ***** Running training *****\n",
            "[INFO|trainer.py:2048] 2024-04-12 02:01:40,244 >>   Num examples = 3,668\n",
            "[INFO|trainer.py:2049] 2024-04-12 02:01:40,244 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:2050] 2024-04-12 02:01:40,244 >>   Instantaneous batch size per device = 32\n",
            "[INFO|trainer.py:2053] 2024-04-12 02:01:40,244 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
            "[INFO|trainer.py:2054] 2024-04-12 02:01:40,244 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:2055] 2024-04-12 02:01:40,244 >>   Total optimization steps = 345\n",
            "[INFO|trainer.py:2056] 2024-04-12 02:01:40,244 >>   Number of trainable parameters = 14,350,874\n",
            "100% 345/345 [17:38<00:00,  2.71s/it][INFO|trainer.py:2315] 2024-04-12 02:19:18,514 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "{'train_runtime': 1058.2699, 'train_samples_per_second': 10.398, 'train_steps_per_second': 0.326, 'train_loss': 0.5401317651721015, 'epoch': 3.0}\n",
            "100% 345/345 [17:38<00:00,  3.07s/it]\n",
            "[INFO|trainer.py:3304] 2024-04-12 02:19:18,517 >> Saving model checkpoint to test_results_int_expanded_MRPC\n",
            "[INFO|configuration_utils.py:471] 2024-04-12 02:19:18,518 >> Configuration saved in test_results_int_expanded_MRPC/config.json\n",
            "[INFO|modeling_utils.py:2590] 2024-04-12 02:19:18,593 >> Model weights saved in test_results_int_expanded_MRPC/model.safetensors\n",
            "[INFO|tokenization_utils_base.py:2488] 2024-04-12 02:19:18,594 >> tokenizer config file saved in test_results_int_expanded_MRPC/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2497] 2024-04-12 02:19:18,595 >> Special tokens file saved in test_results_int_expanded_MRPC/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =        3.0\n",
            "  train_loss               =     0.5401\n",
            "  train_runtime            = 0:17:38.26\n",
            "  train_samples            =       3668\n",
            "  train_samples_per_second =     10.398\n",
            "  train_steps_per_second   =      0.326\n",
            "04/12/2024 02:19:18 - INFO - __main__ - *** Evaluate ***\n",
            "[INFO|trainer.py:785] 2024-04-12 02:19:18,612 >> The following columns in the evaluation set don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: sentence1, idx, sentence2. If sentence1, idx, sentence2 are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
            "[INFO|trainer.py:3613] 2024-04-12 02:19:18,615 >> ***** Running Evaluation *****\n",
            "[INFO|trainer.py:3615] 2024-04-12 02:19:18,615 >>   Num examples = 408\n",
            "[INFO|trainer.py:3618] 2024-04-12 02:19:18,615 >>   Batch size = 8\n",
            "100% 51/51 [00:12<00:00,  4.05it/s]\n",
            "***** eval metrics *****\n",
            "  epoch                   =        3.0\n",
            "  eval_accuracy           =      0.799\n",
            "  eval_combined_score     =     0.8303\n",
            "  eval_f1                 =     0.8615\n",
            "  eval_loss               =     0.4706\n",
            "  eval_runtime            = 0:00:12.83\n",
            "  eval_samples            =        408\n",
            "  eval_samples_per_second =     31.791\n",
            "  eval_steps_per_second   =      3.974\n"
          ]
        }
      ]
    }
  ]
}